{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Note We are current moving across documentation What is the Defra Azure Development Platform (ADP)? \u00b6 The AzureDevelopment Platform is a modern 'PaaS' service designed for delivery teams to design, build, run, and observe container-first Cloud Native business applications and intelligent apps. It supports microservices-based architectures and enforces approved Defra engineering standards. It can host and run any containerised Web App or API, and promotes a highly scalable, self-service, developer-centric approach to digital delivery. Why is it important? \u00b6 Alignment & Compliance \u00b6 Your delivery will be aligned and compliant with delivery standards on day 1. The ADP will help you align with Development & Delivery standards from day 1. The Platform provides everything you need to build business apps in a compliant manner, from architecture to development, security to quality assurance. Focus on business value \u00b6 You build the Apps \u2013 We build the Infra. The ADP is self-service and features highly automated processes which dev teams won\u2019t need to worry about. As all your infrastructure and team setup is automated, from backlogs to the apps you build, development teams can focus on delivering the business value features. Autonomy & Empowerment \u00b6 Your teams will be autonomous andempowered to deliver business value. Developers will have all the tools they need on day 1. Interactions with the Platform are centralised, and there is no requirement for ticketing tools for deployments or configuration. If you need to create a new business app, you simply access the ADP and self-service from the Portal catalogue. Team efficiency performance \u00b6 Your teams will have full observability & DORA metrics to continuously improve. A key goal of the ADP is to fully integrate the ecosystem of approved tools efficiently and coherently. With a fully automated and integrated Platform, development teams will be able to both measure and improve upon their change lead time, deployment frequencies and failure rates. What does ADP offer me? \u00b6 ADP is a set of self-service tools, services and processes that supports and accelerates your software development, while taking care of managing the underlying infrastructure. Our vision is: \"Build Apps, Not Infra\" . Multiple 'exemplar' & 'Hello Worl' development templates (APIs & User Interfaces) Fully preconfigured CI & CD Pipelines for Infrastructure & Applications, enforcing a modern Git-Ops approach & container scanning A full suite of deployed environments \u2013 from development to production A self-service developer portal that centralises information about your entire app estate \u2013 and enables self-service deployments of new services A full suite of observability and monitoring tools, including dashboards \u2013 from dev to production environments Multiple application container hosting options: Azure Kubernetes Service, Azure Container Apps and Azure Functions as well as Azure Open AI integration. Full support for NodeJS and C#/.NET Applications for frontends & backend Automated pipelines include a full testing suite, covering security tests, static code analysis, integration, accessibility, unit, acceptance, contract, and performance tests. Automated setup of Azure DevOps integrations, (where required) including Boards, Backlogs, Work Items, CI/CD and integration with GitHub for code repositories. Support for a wide range of infrastructure, including Storage, Load Balancing, Web App Firewalls & Bot protection, CDN, SQL & NoSQL Databases, Caching, API Management, private networking, flexible container hosting options. Enforces a secure by design approach with low/zero trust policies ADP Tools Landscape \u00b6","title":"Why ADP"},{"location":"#what-is-the-defra-azure-development-platform-adp","text":"The AzureDevelopment Platform is a modern 'PaaS' service designed for delivery teams to design, build, run, and observe container-first Cloud Native business applications and intelligent apps. It supports microservices-based architectures and enforces approved Defra engineering standards. It can host and run any containerised Web App or API, and promotes a highly scalable, self-service, developer-centric approach to digital delivery.","title":"What is the Defra Azure Development Platform (ADP)?"},{"location":"#why-is-it-important","text":"","title":"Why is it important?"},{"location":"#alignment-compliance","text":"Your delivery will be aligned and compliant with delivery standards on day 1. The ADP will help you align with Development & Delivery standards from day 1. The Platform provides everything you need to build business apps in a compliant manner, from architecture to development, security to quality assurance.","title":"Alignment &amp; Compliance"},{"location":"#focus-on-business-value","text":"You build the Apps \u2013 We build the Infra. The ADP is self-service and features highly automated processes which dev teams won\u2019t need to worry about. As all your infrastructure and team setup is automated, from backlogs to the apps you build, development teams can focus on delivering the business value features.","title":"Focus on business value"},{"location":"#autonomy-empowerment","text":"Your teams will be autonomous andempowered to deliver business value. Developers will have all the tools they need on day 1. Interactions with the Platform are centralised, and there is no requirement for ticketing tools for deployments or configuration. If you need to create a new business app, you simply access the ADP and self-service from the Portal catalogue.","title":"Autonomy &amp; Empowerment"},{"location":"#team-efficiency-performance","text":"Your teams will have full observability & DORA metrics to continuously improve. A key goal of the ADP is to fully integrate the ecosystem of approved tools efficiently and coherently. With a fully automated and integrated Platform, development teams will be able to both measure and improve upon their change lead time, deployment frequencies and failure rates.","title":"Team efficiency performance"},{"location":"#what-does-adp-offer-me","text":"ADP is a set of self-service tools, services and processes that supports and accelerates your software development, while taking care of managing the underlying infrastructure. Our vision is: \"Build Apps, Not Infra\" . Multiple 'exemplar' & 'Hello Worl' development templates (APIs & User Interfaces) Fully preconfigured CI & CD Pipelines for Infrastructure & Applications, enforcing a modern Git-Ops approach & container scanning A full suite of deployed environments \u2013 from development to production A self-service developer portal that centralises information about your entire app estate \u2013 and enables self-service deployments of new services A full suite of observability and monitoring tools, including dashboards \u2013 from dev to production environments Multiple application container hosting options: Azure Kubernetes Service, Azure Container Apps and Azure Functions as well as Azure Open AI integration. Full support for NodeJS and C#/.NET Applications for frontends & backend Automated pipelines include a full testing suite, covering security tests, static code analysis, integration, accessibility, unit, acceptance, contract, and performance tests. Automated setup of Azure DevOps integrations, (where required) including Boards, Backlogs, Work Items, CI/CD and integration with GitHub for code repositories. Support for a wide range of infrastructure, including Storage, Load Balancing, Web App Firewalls & Bot protection, CDN, SQL & NoSQL Databases, Caching, API Management, private networking, flexible container hosting options. Enforces a secure by design approach with low/zero trust policies","title":"What does ADP offer me?"},{"location":"#adp-tools-landscape","text":"","title":"ADP Tools Landscape"},{"location":"test/","text":"test \u00b6 C \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 using System ; namespace HelloWorld { class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello World!\" ); } } } JavaScript 1 2 3 function myFunction () { document . getElementById ( \"demo\" ). innerHTML = \"Paragraph changed.\" ; } 1 2 3 4 5 6 graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!]; 1 2 3 4 5 6 7 8 9 10 sequenceDiagram autonumber Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! Method Description GET Fetch resource PUT Update resource DELETE Delete resource HTML for content and structure JavaScript for interactivity CSS for text running out of boxes Internet Explorer ... huh? Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Image caption Subscribe to our newsletter Send Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Test"},{"location":"test/#test","text":"","title":"test"},{"location":"test/#c","text":"1 2 3 4 5 6 7 8 9 10 11 12 using System ; namespace HelloWorld { class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello World!\" ); } } } JavaScript 1 2 3 function myFunction () { document . getElementById ( \"demo\" ). innerHTML = \"Paragraph changed.\" ; } 1 2 3 4 5 6 graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!]; 1 2 3 4 5 6 7 8 9 10 sequenceDiagram autonumber Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! Method Description GET Fetch resource PUT Update resource DELETE Delete resource HTML for content and structure JavaScript for interactivity CSS for text running out of boxes Internet Explorer ... huh? Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Image caption Subscribe to our newsletter Send Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"C"},{"location":"Developer-Reference/Developer%20Reference/","text":"Whos is this for: \u00b6 Tech Leads Developers","title":"Developer Reference"},{"location":"Developer-Reference/Developer%20Reference/#whos-is-this-for","text":"Tech Leads Developers","title":"Whos is this for:"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/","text":"Azure Developer Portal \u00b6 Welcome to the Azure Developer Portal (ADP) repository. The portal is built using Backstage . Getting started \u00b6 Prerequisites \u00b6 Access to a UNIX based operating system. If on Windows it is recommended that you use WSL A GNU-like build environment available at the command line. For example, on Debian/Ubuntu you will want to have the make and build-essential packages installed curl or wget installed Node.js Active LTS release Yarn Docker Git See the Backstage Getting Started documentation for the full list of prerequisites. Integrations \u00b6 The portal is integrated with various 3 rd party services. Connections to these services are managed through the environment variables below: GitHub (via a GitHub app) Entra ID/Azure/ADO/Microsoft Graph (via an App Registration). ADO also uses a PAT token in some (very limited) scenarios. Azure Managed Grafana Azure Blob Storage (for TechDocs) AKS DevContainers \u00b6 Development can be done within a devcontainer if you wish. Once the devcontainer is set up, simply fill out the .env file at the root and rebuild the container. Once rebuilt, you will need to log into the az cli to the tenant you wish to connect to using az login --tenant <TenantId> If you are using VSCode, the steps are as follows: Install the DevContainers extension Open the command pallet and run the Dev Containers: Clone Repository in Container Volume command Either select the github option and locate the repo, or enter https://github.com/DEFRA/adp-portal.git Once the dev container is ready, open the .env file at the root, and fill it out with the variables described below Open the command pallet and run the Dev Containers: Rebuild Container command Once the dev container is rebuilt, run az login --tenant <YOUR_TENANT_ID> To start the application, run yarn dev To sign commits using GPG from within the devcontainer, please follow the steps here Environment Variables \u00b6 The application requires the following environment variables to be set. We recommend creating a .env file in the root of your repo (this is ignored by Git) and pasting the variables in to this file. This file will be used whenever you run a script through yarn such as yarn dev . All environment variables that are left blank can be found in the dev keyvault in azure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 APP_BASE_URL = http://localhost:3000 APP_BACKEND_BASE_URL = http://localhost:7007 ADP_PORTAL_API_ENDPOINT = \"http://localhost:5096/api\" #If developing in WSL and running the portal API on your host machine, use: http://$(hostname).local:5096/api ADP_PORTAL_TEMPLATE_VERSION = main GITHUB_APP_ID = \"\" GITHUB_CLIENT_ID = \"\" GITHUB_CLIENT_SECRET = \"\" GITHUB_PRIVATE_KEY = \"\" GITHUB_ORGANIZATION = \"\" AUTH_MICROSOFT_CLIENT_ID = \"\" AUTH_MICROSOFT_CLIENT_SECRET = \"\" AUTH_MICROSOFT_TENANT_ID = \"\" BACKSTAGE_BACKEND_SECRET = \"\" BACKEND_PLUGINS_SECRET = \"\" ADO_PAT = \"\" ADO_ORGANIZATION = \"\" GRAFANA_TOKEN = \"\" GRAFANA_ENDPOINT = \"\" TECHDOCS_AZURE_BLOB_STORAGE_ACCOUNT_NAME = \"\" TECHDOCS_AZURE_BLOB_STORAGE_ACCOUNT_KEY = \"\" ADP_PORTAL_PLATFORM_ADMINS_GROUP = \"\" ADP_PORTAL_PROGRAMME_ADMINS_GROUP = \"\" ADP_PORTAL_USERS_GROUP = \"\" ADP_PORTAL_USERS_GROUP_PREFIX = \"\" SND1_CLUSTER_NAME = \"\" SND1_CLUSTER_API_SERVER_ADDRESS = \"\" SND2_CLUSTER_NAME = \"\" SND2_CLUSTER_API_SERVER_ADDRESS = \"\" SND3_CLUSTER_NAME = \"\" SND3_CLUSTER_API_SERVER_ADDRESS = \"\" TZ = utc # Optional env variables, you might not need these. NODE_ENV = development NODE_OPTIONS = --no-node-snapshot NODE_TLS_REJECT_UNAUTHORIZED = 0 To run the end to end tests, you will need to additionally set the following variables in your .env file. These are not needed for normal development. 1 2 3 4 5 6 7 8 9 10 11 12 E2E_TEST_ACCOUNTS_ADPTESTUSER1_EMAIL = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER1_PASSWORD = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER2_EMAIL = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER2_PASSWORD = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER3_EMAIL = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER3_PASSWORD = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER4_EMAIL = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER4_PASSWORD = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER5_EMAIL = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER5_PASSWORD = \"\" TEST_ENVIRONMENT_ROOT_URL = https://portal.snd1.adp.defra.gov.uk/ To convert a GitHub private key into a format that can be used in the GITHUB_PRIVATE_KEY environment variable use one of the following scripts: Powershell 1 $rsaprivkey = ( Get-Content \"private-key.pem\" | Out-String ) -replace \" `r`n \" , \"\\n\" Shell 1 awk 'NF {sub(/\\r/, \"\"); printf \"%s\\\\n\",$0;}' private-key.pem > rsaprivkey.txt Techdocs \u00b6 A hybrid strategy is implemented for techdocs which means documentation can be generated on the fly by out of the box generator or using an external pipeline. All generated documentation are stored in Azure blob storage. For more info please refer : Ref Running locally \u00b6 Run the following commands from the root of the repository: 1 2 yarn install yarn dev Configuration \u00b6 If you want to override any settings in ./app-config.yaml , create a local configuration file named app-config.local.yaml and define your overrides here. Mac \u00b6 You need to have the azure cli installed and the azure development client installed Login into both az, and azd before running the server. 1 2 az login --tenant XXXXX.azure.com az auth login --tenant-id <your tenant id> You must run the application in the same browser session, that the authentication ran in. If you use a \"private window\", new session, it will not have access to the required cookies to complete authentication, and you will get a 'user not found' error message Feature Requests \u00b6 If you have an idea for a new feature or an improvement to an existing feature, please follow these steps: Check if the feature has already been requested by searching the project's issue tracker. If the feature hasn't been requested, create a new issue and provide a clear description of the proposed feature and why it would be beneficial. Pull Requests \u00b6 If you're ready to submit your code changes, please follow these steps specified in the pull_request_template Code Style Guidelines \u00b6 To maintain a consistent code style throughout the project, please adhere to the following guidelines: Use descriptive variable and function names. Follow the existing code formatting and indentation style. Write clear and concise comments to explain complex code logic. License \u00b6 Include information about the project's license and any relevant copyright notices.","title":"Portal Development"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#azure-developer-portal","text":"Welcome to the Azure Developer Portal (ADP) repository. The portal is built using Backstage .","title":"Azure Developer Portal"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#getting-started","text":"","title":"Getting started"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#prerequisites","text":"Access to a UNIX based operating system. If on Windows it is recommended that you use WSL A GNU-like build environment available at the command line. For example, on Debian/Ubuntu you will want to have the make and build-essential packages installed curl or wget installed Node.js Active LTS release Yarn Docker Git See the Backstage Getting Started documentation for the full list of prerequisites.","title":"Prerequisites"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#integrations","text":"The portal is integrated with various 3 rd party services. Connections to these services are managed through the environment variables below: GitHub (via a GitHub app) Entra ID/Azure/ADO/Microsoft Graph (via an App Registration). ADO also uses a PAT token in some (very limited) scenarios. Azure Managed Grafana Azure Blob Storage (for TechDocs) AKS","title":"Integrations"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#devcontainers","text":"Development can be done within a devcontainer if you wish. Once the devcontainer is set up, simply fill out the .env file at the root and rebuild the container. Once rebuilt, you will need to log into the az cli to the tenant you wish to connect to using az login --tenant <TenantId> If you are using VSCode, the steps are as follows: Install the DevContainers extension Open the command pallet and run the Dev Containers: Clone Repository in Container Volume command Either select the github option and locate the repo, or enter https://github.com/DEFRA/adp-portal.git Once the dev container is ready, open the .env file at the root, and fill it out with the variables described below Open the command pallet and run the Dev Containers: Rebuild Container command Once the dev container is rebuilt, run az login --tenant <YOUR_TENANT_ID> To start the application, run yarn dev To sign commits using GPG from within the devcontainer, please follow the steps here","title":"DevContainers"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#environment-variables","text":"The application requires the following environment variables to be set. We recommend creating a .env file in the root of your repo (this is ignored by Git) and pasting the variables in to this file. This file will be used whenever you run a script through yarn such as yarn dev . All environment variables that are left blank can be found in the dev keyvault in azure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 APP_BASE_URL = http://localhost:3000 APP_BACKEND_BASE_URL = http://localhost:7007 ADP_PORTAL_API_ENDPOINT = \"http://localhost:5096/api\" #If developing in WSL and running the portal API on your host machine, use: http://$(hostname).local:5096/api ADP_PORTAL_TEMPLATE_VERSION = main GITHUB_APP_ID = \"\" GITHUB_CLIENT_ID = \"\" GITHUB_CLIENT_SECRET = \"\" GITHUB_PRIVATE_KEY = \"\" GITHUB_ORGANIZATION = \"\" AUTH_MICROSOFT_CLIENT_ID = \"\" AUTH_MICROSOFT_CLIENT_SECRET = \"\" AUTH_MICROSOFT_TENANT_ID = \"\" BACKSTAGE_BACKEND_SECRET = \"\" BACKEND_PLUGINS_SECRET = \"\" ADO_PAT = \"\" ADO_ORGANIZATION = \"\" GRAFANA_TOKEN = \"\" GRAFANA_ENDPOINT = \"\" TECHDOCS_AZURE_BLOB_STORAGE_ACCOUNT_NAME = \"\" TECHDOCS_AZURE_BLOB_STORAGE_ACCOUNT_KEY = \"\" ADP_PORTAL_PLATFORM_ADMINS_GROUP = \"\" ADP_PORTAL_PROGRAMME_ADMINS_GROUP = \"\" ADP_PORTAL_USERS_GROUP = \"\" ADP_PORTAL_USERS_GROUP_PREFIX = \"\" SND1_CLUSTER_NAME = \"\" SND1_CLUSTER_API_SERVER_ADDRESS = \"\" SND2_CLUSTER_NAME = \"\" SND2_CLUSTER_API_SERVER_ADDRESS = \"\" SND3_CLUSTER_NAME = \"\" SND3_CLUSTER_API_SERVER_ADDRESS = \"\" TZ = utc # Optional env variables, you might not need these. NODE_ENV = development NODE_OPTIONS = --no-node-snapshot NODE_TLS_REJECT_UNAUTHORIZED = 0 To run the end to end tests, you will need to additionally set the following variables in your .env file. These are not needed for normal development. 1 2 3 4 5 6 7 8 9 10 11 12 E2E_TEST_ACCOUNTS_ADPTESTUSER1_EMAIL = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER1_PASSWORD = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER2_EMAIL = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER2_PASSWORD = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER3_EMAIL = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER3_PASSWORD = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER4_EMAIL = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER4_PASSWORD = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER5_EMAIL = \"\" E2E_TEST_ACCOUNTS_ADPTESTUSER5_PASSWORD = \"\" TEST_ENVIRONMENT_ROOT_URL = https://portal.snd1.adp.defra.gov.uk/ To convert a GitHub private key into a format that can be used in the GITHUB_PRIVATE_KEY environment variable use one of the following scripts: Powershell 1 $rsaprivkey = ( Get-Content \"private-key.pem\" | Out-String ) -replace \" `r`n \" , \"\\n\" Shell 1 awk 'NF {sub(/\\r/, \"\"); printf \"%s\\\\n\",$0;}' private-key.pem > rsaprivkey.txt","title":"Environment Variables"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#techdocs","text":"A hybrid strategy is implemented for techdocs which means documentation can be generated on the fly by out of the box generator or using an external pipeline. All generated documentation are stored in Azure blob storage. For more info please refer : Ref","title":"Techdocs"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#running-locally","text":"Run the following commands from the root of the repository: 1 2 yarn install yarn dev","title":"Running locally"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#configuration","text":"If you want to override any settings in ./app-config.yaml , create a local configuration file named app-config.local.yaml and define your overrides here.","title":"Configuration"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#mac","text":"You need to have the azure cli installed and the azure development client installed Login into both az, and azd before running the server. 1 2 az login --tenant XXXXX.azure.com az auth login --tenant-id <your tenant id> You must run the application in the same browser session, that the authentication ran in. If you use a \"private window\", new session, it will not have access to the required cookies to complete authentication, and you will get a 'user not found' error message","title":"Mac"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#feature-requests","text":"If you have an idea for a new feature or an improvement to an existing feature, please follow these steps: Check if the feature has already been requested by searching the project's issue tracker. If the feature hasn't been requested, create a new issue and provide a clear description of the proposed feature and why it would be beneficial.","title":"Feature Requests"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#pull-requests","text":"If you're ready to submit your code changes, please follow these steps specified in the pull_request_template","title":"Pull Requests"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#code-style-guidelines","text":"To maintain a consistent code style throughout the project, please adhere to the following guidelines: Use descriptive variable and function names. Follow the existing code formatting and indentation style. Write clear and concise comments to explain complex code logic.","title":"Code Style Guidelines"},{"location":"Developer-Reference/Infrastructure/ADP%20Portal/#license","text":"Include information about the project's license and any relevant copyright notices.","title":"License"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/","text":"ADP Platform Azure Service Operator Helm Library Chart \u00b6 A Helm chart library that captures general configuration for Azure Service Operator (ASO) resources. It can be used by any microservice Helm chart to import AzureServiceOperator K8s object templates configured to run on the ADP platform. Including the library chart \u00b6 In your microservice Helm chart: * Update Chart.yaml to apiVersion: v2 . * Add the library chart under dependencies and choose the version you want (example below). Version number can include ~ or ^ to pick up latest PATCH and MINOR versions respectively. * Issue the following commands to add the repo that contains the library chart, update the repo, then update dependencies in your Helm chart: 1 2 3 helm repo add adp https://raw.githubusercontent.com/defra/adp-helm-repository/main/adp-aso-helm-library helm repo update helm dependency update <helm_chart_location> An example Demo microservice Chart.yaml : 1 2 3 4 5 6 7 8 apiVersion: v2 description: A Helm chart to deploy a microservice to the ADP Kubernetes platform name: demo-microservice version: 1.0.0 dependencies: - name: adp-aso-helm-library version: ^1.0.0 repository: https://raw.githubusercontent.com/defra/adp-helm-repository/main/adp-aso-helm-library NOTE: We will use ACR where ASO Helm Library Chart can be published. So above dependencies will be changed to import library from ACR (In Progress). Using the K8s object templates \u00b6 First, follow the instructions for including the ASO Helm library chart. The ASO Helm library chart has been configured using the conventions described in the Helm library chart documentation . The K8s object templates provide settings shared by all objects of that type, which can be augmented with extra settings from the parent (Demo microservice) chart. The library object templates will merge the library and parent templates. In the case where settings are defined in both the library and parent chart, the parent chart settings will take precedence, so library chart settings can be overridden. The library object templates will expect values to be set in the parent .values.yaml . Any required values (defined for each template below) that are not provided will result in an error message when processing the template ( helm install , helm upgrade , helm template ). The general strategy for using one of the library templates in the parent microservice Helm chart is to create a template for the K8s object formateted as so: 1 {{- include \"adp-aso-helm-library.namespace-queue\" . -}} All template required values \u00b6 All the K8s object templates in the library require the following values to be set in the parent microservice Helm chart's values.yaml : 1 namespace: <string> Environment specific Default values \u00b6 The below values are used by the ASO templates internally, and their values are set using platform variables in adp-flux-services repository. for e.g. NameSpace Queues will get created inside serviceBusNamespaceName namespace and postgres database will get created inside postgresServerName server. Whilst the Platform orchestration will manage the 'platform' level variables, they can be optionally supplied in some circumstances. Examples include in sandpit/development when testing against team-specific infrastructure (that isn't Platform shared). So, if you have a dedicated Service Bus or Database Server instance, you can point to those to ensure you apps works as expected. Otherwise, don't supply the Platform level variables as these will be automatically managed and orchestrated throughout all the environments appropriately against core shared infrastructure. You (as a Platform Tenant) just supply your team-specific/instance specific infrastructure config' (i.e. Queues, Storage Accounts, Databases). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 namespace: <string> --namespace name environment: <string> --environment name fluxConfigNamespace: <string> --fluxConfig namespace name subscriptionId: <string> --subscription Id serviceBusResourceGroupName: <string> --Name of the service bus resource group serviceBusNamespaceName: <string> --Name of the environment specific service bus postgresResourceGroupName: <string> --Name of the Postgres server resource group postgresServerName: <string> --Name of the environment specific postgres server keyVaultResourceGroupName: <string> --Name of the keyvault resource group keyVaultName: <string> --Name of the environment specific keyVaultName teamMIPrefix: <string> --The prefix used for the ManageIdentity/UserAssignedIdentity resource name serviceName: <string> --Service name. Suffix used for ManageIdentity/UserAssignedIdentity resource name teamResourceGroupName: <string> --Team ResourceGroup Name where team resources are created virtualNetworkResourceGroupName: <string> --Virtual Network resource group virtualNetworkName: <string> --Virtual Network name storageAccountPrefix: <string> --The prefix used for the storage account resource name privateEndpointSubnetName: <string> --The name of the subnet for the service's private endpoint privateEndpointPrefix: <string> --The prefix used for the private endpoint resource name azrMSTPrivateLinkDNSUKSouthResourceGroupName: <string> --NOT USED. We need to discuss this further azrMSTPrivateLinkDNSUKWestResourceGroupName: <string> --NOT USED. We need to discuss this further azrMSTPrivateLinkDNSSubscriptionID: <string> --NOT USED. We need to discuss this further commonTags: environment: <string> serviceCode: <string> serviceName: <string> serviceType: <string> (Shared or Dedicated) kubernetes_cluster: <string> kubernetes_namespace: <string> kubernetes_label_serviceCode: <string> NameSpace Queue \u00b6 Template file: _namespace-queue.yaml Template name: adp-aso-helm-library.namespace-queue An ASO NamespacesQueue object to create a Microsoft.ServiceBus/namespaces/queues resource. A basic usage of this object template would involve the creation of templates/namespace-queue.yaml in the parent Helm chart (e.g. adp-microservice ) containing: 1 {{- include \"adp-aso-helm-library.namespace-queue\" . -}} Required values \u00b6 The following values need to be set in the parent chart's values.yaml in addition to the globally required values listed above . Note that namespaceQueues is an array of objects that can be used to create more than one queue. Please note that the queue name is prefixed with the namespace internally. For example, if the namespace name is \"adp-demo\" and you have provided the queue name as \"queue1\", then in the service bus, it creates a queue with the adp-demo-queue1 name. 1 2 3 namespaceQueues: - name: <string> - name: <string> Optional values \u00b6 The following values can optionally be set in the parent chart's values.yaml to set the other properties for servicebus queues: owner property is used to control the ownership of the queue. The default value is yes and you don't need to provide it if you are creating and owning the queue. If you are creating only role assignments for the queue you do not own, then you should explicitly set the owner flag to no so that it will only create the role assignments on the existing queue. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 namespaceQueues: - name: <string> owner: <string> --Default yes (Accepted values = yes, no) deadLetteringOnMessageExpiration: <bool> --Default false defaultMessageTimeToLive: <string> --Default P14D duplicateDetectionHistoryTimeWindow: <string> --Default PT10M enableBatchedOperations: <bool> --Default true enableExpress: <bool> --Default false enablePartitioning: <bool> --Default false lockDuration: <string> --Default PT1M maxDeliveryCount: <int> --Default 10 maxMessageSizeInKilobytes: <int> --Default 1024 maxSizeInMegabytes: <int> --Default 1024 requiresDuplicateDetection: <bool> --Default false requiresSession: <bool> --Default false roleAssignments: - roleName: <string> --<RoleName. for e.g QueueSender> NameSpace Queue: RoleAssignments \u00b6 This template also optionally allows you to create Role Assignments by providing roleAssignments properties in the namespaceQueues object. Below are the minimum values that are required to be set in the parent chart's values.yaml to create a roleAssignments . 1 2 3 4 5 namespaceQueues: - name: <string> roleAssignments: <Array of Object> - roleName: <string> <RoleName. for e.g QueueSender> (Allowed values QueueSender', 'QueueReceiver') - roleName: <string> If you are creating only role assignments for the queue you do not own, then you should explicitly set the owner flag to no so that it will only create the role assignments on the existing queue. Usage examples \u00b6 The following section provides usage examples for the Namespace Queues template. Example 1 : ServiceA in TeamA creates queue with 2 role assignments \u00b6 1 2 3 4 5 namespaceQueues: name: claim roleAssignments: - roleName: QueueSender - roleName: QueueReceiver Example 2 : ServiceB in TeamA needs to receive messages from existing claim queue. Note that owner is set to no . \u00b6 1 2 3 4 5 namespaceQueues: name: claim owner: 'no' roleAssignments: - roleName: QueueReceiver NameSpace Topic \u00b6 Template file: _namespace-topic.yaml Template name: adp-aso-helm-library.namespace-topic An ASO NamespacesTopic object to create a Microsoft.ServiceBus/namespaces/topics resource. A basic usage of this object template would involve the creation of templates/namespace-topic.yaml in the parent Helm chart (e.g. adp-microservice ) containing: 1 {{- include \"adp-aso-helm-library.namespace-topic\" . -}} Required values \u00b6 The following values need to be set in the parent chart's values.yaml in addition to the globally required values listed above . Note that namespaceTopics is an array of objects that can be used to create more than one topic. Please note that the topic name is prefixed with the namespace internally. For example, if the namespace name is \"adp-demo\" and you have provided the topic name as \"topic1,\" then in the service bus, it creates a topic with the \"adp-demo-topic1\" name. 1 2 3 namespaceTopics: <Array of Object> - name: <string> - name: <string> Optional values \u00b6 The following values can optionally be set in the parent chart's values.yaml to set the other properties for namespaceTopics : owner property is used to control the ownership of the topic. The default value is yes and you don't need to provide it if you are creating and owning the topic. If you are only creating role assignments for the topic you do not own, then you should explicitly set the owner flag to no so that it will only create the role assignments on the existing topic. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 namespaceTopics: - name: <string> owner: <string> --Default yes (Accepted values = yes, no) defaultMessageTimeToLive: <string> --Default P14D duplicateDetectionHistoryTimeWindow: <string> --Default PT10M enableBatchedOperations: <bool> --Default true enableExpress: <bool> --Default false enablePartitioning: <bool> --Default false maxMessageSizeInKilobytes: <int> --Default 1024 maxSizeInMegabytes: <int> --Default 1024 requiresDuplicateDetection: <bool> --Default false supportOrdering: <bool> --Default true roleAssignments: - roleName: <string> --<RoleName. for e.g TopicSender> NameSpace Topic: RoleAssignments \u00b6 This template also optionally allows you to create Role Assignments by providing roleAssignments properties in the namespaceTopics object. Below are the minimum values that are required to be set in the parent chart's values.yaml to create a roleAssignments . 1 2 3 4 5 namespaceTopics: - name: <string> roleAssignments: <Array of Object> - roleName: <string> <RoleName. for e.g TopicSender> (Allowed values TopicSender', 'TopicReceiver') - roleName: <string> If you are creating only role assignments for the Topic you do not own, then you should explicitly set the owner flag to no so that it will only create the role assignments on the existing Topic (See Example 2 in Usage examples section). NameSpace Topic: Subscriptions, SubscriptionRules \u00b6 This template also optionally allows you to create Topic Subscriptions and Topic Subscriptions Rules for a given topic by providing Subscriptions and SubscriptionRules properties in the topic object. Below are the minimum values that are required to be set in the parent chart's values.yaml to create a NamespacesTopic , NamespacesTopicsSubscription and NamespacesTopicsSubscriptionsRule 1 2 3 4 5 6 7 8 9 namespaceTopics: - name: <string> topicSubscriptions: <Array of Object> refer \"Optional values for `topicSubscriptions`\" section for topicSubscriptions optional properties - name: <string> topicSubscriptionRules: <Array of Object> refer \"Optional values for `topicSubscriptionRules`\" section for topicSubscriptionRules properties - name: <string> filterType: <string> Accepted values : 'CorrelationFilter' or 'SqlFilter' correlationFilter: <Object> refer \"Optional values for `topicSubscriptionRules`\" section for correlationFilter properties sqlFilter: <Object> refer \"Optional values for `topicSubscriptionRules`\" section for sqlFilter properties To create topicSubscriptions inside already existing topics, set the property owner to no . By default owner is set to yes which creates the topic name defined in values (See Example 4 in Usage examples section). Optional values for topicSubscriptions \u00b6 The following values can optionally be set in the parent chart's values.yaml to set the other properties for topicSubscriptions : 1 2 3 4 5 6 7 8 9 10 11 12 topicSubscriptions: - name: <string> deadLetteringOnFilterEvaluationExceptions: <bool> --Default false deadLetteringOnMessageExpiration: <bool> --Default false defaultMessageTimeToLive: <string> --Default P14D duplicateDetectionHistoryTimeWindow: <string> --Default P10M enableBatchedOperations: <bool> --Default true forwardTo: <string> isClientAffine: <bool> --Default false lockDuration: <string> --Default PT1M maxDeliveryCount: <int> --Default 10 requiresSession: <bool> --Default false Optional values for topicSubscriptionRules \u00b6 The following values can optionally be set in the parent chart's values.yaml to set the other properties for topicSubscriptionRules : 1 2 3 4 5 6 7 8 9 10 11 12 13 topicSubscriptionRules: - name: <string> correlationFilter: contentType: <string> correlationId: <string> label: <string> messageId: <string> replyTo: <string> replyToSessionId: <string> sessionId: <string> to: <string> sqlFilter: sqlExpression: <string> Usage examples \u00b6 The following section provides usage examples for the Namespace Topic template. Example 1 : ServiceA in TeamA creates Topic with 1 role assignment \u00b6 1 2 3 4 namespaceTopics: name: claim-notify roleAssignments: - roleName: TopicSender Example 2 : ServiceB in TeamA needs to receive messages from existing claim-notify Topic. Note that owner is set to no . \u00b6 1 2 3 4 5 namespaceTopics: name: claim-notify owner: 'no' roleAssignments: - roleName: TopicReceiver Example 3 : ServiceA in TeamA creates Topic with 1 role assignment, Topic Subscription and Topic Subscription Rule. \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 namespaceTopics: name: claim-notify roleAssignments: - roleName: TopicSender topicSubscriptions: - name: claim-notify-subscription-01 topicSubscriptionRules: - name: claim-notify-subscription-rule-01 filterType: SqlFilter sqlFilter: sqlExpression: \"3=3\" - name: claim-notify-subscription-rule-02 filterType: CorrelationFilter sqlFilter: contentType: \"testvalue\" Example 4: ServiceB in TeamA creates Topic Subscription in existing Topic. \u00b6 1 2 3 4 5 6 7 namespaceTopics: name: claim-notify owner: \"no\" roleAssignments: - roleName: TopicReceiver topicSubscriptions: - name: claim-notify-subscription-03 Database for Postgres Flexible server template \u00b6 Template file: _flexible-servers-db.yaml Template name: adp-aso-helm-library.flexible-servers-db An ASO FlexibleServersDatabase object. A basic usage of this object template would involve the creation of templates/flexible-servers-db.yaml in the parent Helm chart (e.g. adp-microservice ) containing: 1 2 3 4 {{- include \"adp-aso-helm-library.flexible-servers-db\" (list . \"adp-microservice.postgres-flexible-db\") -}} {{- define \"adp-microservice.postgres-flexible-db\" -}} # Microservice specific configuration in here {{- end -}} Required values \u00b6 The following values need to be set in the parent chart's values.yaml in addition to the globally required values listed above : 1 2 3 4 5 postgres: db: name: <string> charset: <string> collation: <string> Please note that the postgres DB name is prefixed with namespace internally. For example, if the namespace name is \"adp-microservice\" and you have provided the DB name as \"demo-db,\" then in the postgres server, it creates a database with the name \"adp-microservice-demo-db\". Usage examples \u00b6 The following section provides usage examples for the Flexible-Servers-Db template. Example 1 : ServiceA in TeamA creates payment database \u00b6 1 2 3 4 5 postgres: db: name: payment charset: UTF8 collation: en_US.utf8 UserAssignedIdentity \u00b6 Template file: _userassignedidentity.yaml Template name: adp-aso-helm-library.userassignedidentity An ASO UserAssignedIdentity object to create a Microsoft.ManagedIdentity/userAssignedIdentities resource. A basic usage of this object template would involve the creation of templates/userassignedidentity.yaml in the parent Helm chart (e.g. adp-microservice ) containing: 1 {{- include \"adp-aso-helm-library.userassignedidentity\" . -}} This template uses the below values, whose values are set using platform variables in the adp-flux-services repository as a part of the service's ASO helmrelease value configuration, and you don't need to set them explicitly in the values.yaml file. teamMIPrefix serviceName teamResourceGroupName clusterOIDCIssuerUrl UserAssignedIdentity Name is derived internally, and it is set to = {TEAM_MI_PREFIX}-{SERVICE_NAME} For e.g. In SND1 if the TEAM_MI_PREFIX value is set to \"sndadpinfmid1401\" and SERVICE_NAME value is set to \"adp-demo-service\", then UserAssignedIdentity value will be : \"sndadpinfmid1401-adp-demo-service\". Optional values \u00b6 The following values can optionally be set in the parent chart's values.yaml to set the other properties for servicebus queues: 1 2 userAssignedIdentity: location: <string> This template also optionally allows you to create Federated credentials for a given User Assigned Identity by providing federatedCreds properties in the userAssignedIdentity object. Below are the minimum values that are required to be set in the parent chart's values.yaml to create a userAssignedIdentity and federatedCreds . 1 2 3 4 userAssignedIdentity: federatedCreds: <Array of Object> - namespace: <string> serviceAccountName: <string> Usage examples \u00b6 The following section provides usage examples for the UserAssignedIdentity template. Example 1 : The below example will create userAssignedIdentity with one federated credential. \u00b6 1 2 3 4 userAssignedIdentity: federatedCreds: - namespace: ffc-demo serviceAccountName: ffc-demo Storage Account \u00b6 Template file: _storage-account.yaml Template name: adp-aso-helm-library.storage-account.yaml Version 2.0.0 and above Starting from version 2.0.1, the Storage Account has been enhanced with role assignments. These data role assignments are now scoped at the storage account level, introducing two new data roles: DataWriter and DataReader. The DataWriter role grants applications the ability to both read and write data in the blob container, tables, and files. Conversely, the DataReader role provides applications with read-only access to data in the blob container, tables, and files. An ASO StorageAccount object to create a Microsoft.Storage/storageAccounts resource and optionally sub resources Blob Containers and Tables. By default, private endpoints are always enabled on storage accounts and publicNetworkAccess is disabled. Optionally, you can also configure ipRules in scenarios where you want to limit access to your storage account to requests originating from specified IP addresses. Please be aware that this template only includes A records in the central DNS zone for the Dev, Tst, Pre, and Prd environments. For Sandpit environments snd1, snd2, and snd3, it currently only generates a private endpoint without adding an A record to the DNS zone. You will need to separately add this entry via PowerShell script. With this template, you can create the below resources. - Storage Accounts and RoleAssignments - Blob - Tables - FileShare A basic usage of this object template would involve the creation of templates/storage-account.yaml in the parent Helm chart (e.g. adp-microservice ) containing: 1 {{- include \"adp-aso-helm-library.storage-account\" . -}} Default values for Storage account \u00b6 Below are the default values used by the the storage account template internally, and they cannot be overridden by the user from the values.yaml file. 1 2 3 4 5 6 kind: \"StorageV2\" -- The type of storage account will always be \"StorageV2\" dnsEndpointType: \"Standard\" -- The type of endpoint minimumTlsVersion: \"TLS1_2\" allowBlobPublicAccess: \"false\" sku: \"Standard_LRS\" -- This is the sku for Sandpit environments (snd1, snd2, snd3) sku: \"Standard_RAGRS\" -- This is the sku for production environments (dev, test, pre, prd) Required values (Only Storage Account) \u00b6 The following values need to be set in the parent chart's values.yaml in addition to the globally required values listed above . Note that storageAccounts is an array of objects that can be used to create more than one Storage Accounts. Please note that the storage account name must be unique across Azure. storage account name is internally prefixed with the storageAccountPrefix . For instance, in the Dev environment, the storageAccountPrefix is configured as devadpinfst2401 . If you input \"claim\" as the storage account name, the final storage account name will be devadpinfst2401claim . 1 2 3 4 5 6 7 storageAccounts: <Array of Object> - name: <string> --Storage account name. Name should be Lowercase letters and numbers and Maximum character limit is `9` roleAssignments: roleName: <string> --RoleAssignment Name (Accepted values = \"DataWriter\", \"DataReader\") - name: <string> roleAssignments: roleName: <string> --RoleAssignment Name (Accepted values = \"DataWriter\", \"DataReader\") Required values (Storage Account with BlobContainers, Tables and FileShare) \u00b6 The following values need to be set in the parent chart's values.yaml in addition to the globally required values listed above . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Version 2.0.0 and above storageAccounts: <Array of Object> - name: <string> --Storage account name. Name should be lowercase letters and numbers and Maximum character limit is `9` roleAssignments: roleName: <string> --RoleAssignment Name (Accepted values = \"DataWriter\", \"DataReader\") - name: <string> blobContainers: - name: <string> --Blob container name. Name should be lowercase and can contain only letters, numbers, and the hyphen/minus (-) character. Character limit: 3-63 - name: <string> tables: - name: <string> --Table name. Name should be lowercase and may contain only alphanumeric characters. and Character limit: 3-63 - name: <string> fileShares: - name: <string> --File Share name. Name should be lowercase and may contain only alphanumeric characters. and Character limit: 3-63 - name: <string> --File Share name. Name should be lowercase and may contain only alphanumeric characters. and Character limit: 3-63 accessTier: <string> --Access Tier. Allowed values are TransactionOptimized, Hot, Cold. Default is TransactionOptimized shareQuota: <int> --Storage Quota. Share Quota is defined in GiB. Default is 10 Optional values \u00b6 The following values can optionally be set in the parent chart's values.yaml to set the other properties for storageAccounts : For detailed description of each property see here owner property is used to control the ownership of the storage account. The default value is yes and you don't need to provide it if you are creating and owning the storage account. If you are creating Blob containers or Tables on the existing storage account that you do not own, then you should explicitly set the owner flag to no so that it will only create Blob containers or Tables on the existing storage account. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 storageAccounts: - name: <string> owner: <string> --Default \"yes\" (Accepted values = \"yes\", \"no\") location: <string> --Default \"uksouth\" accessTier: <string> --Default \"Hot\" (Accepted values = \"Hot\", \"Cool\") allowCrossTenantReplication: <bool> --Default false allowSharedKeyAccess: <bool> --Default false defaultToOAuthAuthentication: <string> --Default true (Accepted values = \"true\", \"false\") networkAcls: ipRules: <array> --Storage Firewall: Sets the IP ACL rules storageAccountsBlobService: --Confugure properties for the blob service changeFeed: --The blob service properties for change feed events enabled: <bool> --Default false retentionInDays: <int> --Applies when changeFeed.enabled is set to true containerDeleteRetentionPolicy: --The blob service properties for container soft delete enabled: <bool> --Default true days: <int> --Applies when containerDeleteRetentionPolicy.enabled is set to true. Default is 7 days deleteRetentionPolicy: --The blob service properties for blob soft delete enabled: <bool> --Default true days: <int> --Applies when deleteRetentionPolicy.enabled is set to true. Default is 7 days allowPermanentDelete: <bool> --Default false isVersioningEnabled: <bool> --Default false. Versioning is enabled if set to true restorePolicy: --The blob service properties for blob restore policy enabled: <bool> --Default false days: <int> --Applies when restorePolicy.enabled is set to true storageAccountsFileService: --Confugure properties for the blob service deleteRetentionPolicy: --The blob service properties for blob soft delete enabled: <bool> --Default true days: <int> --Applies when deleteRetentionPolicy.enabled is set to true. Default is 7 days blobContainers: --List of Blob containers - name: <string> --Blob container name tables: --List of Tables - name: <string> --Table name Usage examples \u00b6 The following section provides usage examples for the storage account template. Example 1 : Create 2 storage accounts \u00b6 1 2 3 4 5 6 7 storageAccounts: - name: storage01 roleAssignments: - roleName: 'DataWriter' - name: storage02 roleAssignments: - roleName: 'DataReader' Example 2 : Create 1 storage account using large parameter set and storage firewall \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 storageAccounts: - name: storage01 roleAssignments: - roleName: 'DataWriter' accessTier: Hot location: uksouth allowCrossTenantReplication: false allowSharedKeyAccess: true defaultToOAuthAuthentication: \"false\" networkAcls: ipRules: - 82.13.86.001 - 82.13.86.002 Example 3 : Create 1 storage account and configure properties for the Storage Account BlobService \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 storageAccounts: - name: storage01 roleAssignments: - roleName: 'DataWriter' storageAccountsBlobService: changeFeed: enabled: true retentionInDays: 14 containerDeleteRetentionPolicy: days: 14 enabled: true deleteRetentionPolicy: allowPermanentDelete: true days: 20 enabled: false isVersioningEnabled: true restorePolicy: enabled: true days: 40 Example 4 : Create 1 storage account with 2 blob containers and 1 table \u00b6 1 2 3 4 5 6 7 8 9 storageAccounts: - name: storage01 roleAssignments: - roleName: 'DataWriter' blobContainers: - name: container-01 - name: container-02 tables: - name: table01 Example 5 : Create 1 blob containers and 2 tables for the existing storage account in your team. \u00b6 1 2 3 4 5 6 7 8 9 10 storageAccounts: - name: storage01 owner: \"No\" --Note owner is set to 'No' to indicate storage account already exists and is owned by a different service in the team roleAssignments: - roleName: 'DataWriter' blobContainers: - name: container-01 tables: - name: table01 - name: table02 Example 6 : Create 1 storage account and configure properties for the Storage Account FileService \u00b6 1 2 3 4 5 6 7 8 storageAccounts: - name: storage01 roleAssignments: - roleName: 'DataWriter' storageAccountsFileService: deleteRetentionPolicy: enabled: true days: 20 Example 7 : Create 2 file share, one with default properties other one with specific properties \u00b6 1 2 3 4 5 6 7 8 9 10 storageAccounts: - name: storage01 owner: \"No\" --Note owner is set to 'No' to indicate storage account already exists and is owned by a different service in the team roleAssignments: - roleName: 'DataWriter' fileShares: - name: share-01 - name: share-02 accessTier: Hot shareQuota: 50 Example 9 : Create roleassignments with reader access for existing storage account in the team \u00b6 1 2 3 4 5 storageAccounts: - name: storage01 owner: 'no' roleAssignments: - roleName: 'DataReader' Reference Table for Resource Names in Azure and Kubernetes \u00b6 The table below shows the Azure Service Operator (ASO) resource naming convention in Azure and Kubernetes: In the example below, the following platform values are used for demonstration purposes: - namespace = 'ffc-demo' - serviceName = 'ffc-demo-web' - teamMIPrefix = 'sndadpinfmi1401' - storageAccountPrefix = 'sndadpinfst1401' - privateEndpointPrefix = 'sndadpinfpe1401' - postgresServerName = 'sndadpdbsps1401' - userassignedidentityName = 'sndadpinfmi1401-ffc-demo-web' And the following user input values are used for demonstration purposes: QueueName = 'queue01' TopicName = 'topic01' TopicSubName = 'topicSub01' DatabaseName = 'claim' StorageAccountName = 'demo' Resource Type Resource Name Format in Azure Resource Name Example in Azure Resource Name Format in Kubernetes Resource Name Example in Kubernetes NamespacesQueue {namespace}-{QueueName} ffc-demo-queue01 {namespace}-{QueueName} ffc-demo-queue01 Queue RoleAssignment NA NA {userassignedidentityName}-{QueueName}-{RoleName}-rbac-{index} sndadpinfmi1401-ffc-demo-web-ffc-demo-queue01-queuereceiver-rbac-0 NamespacesTopic {namespace}-{TopicName} ffc-demo-topic01 {namespace}-{TopicName} ffc-demo-topic01 NamespacesTopicsSubscription {namespace}-{TopicSubName} ffc-demo-topicSub01 {namespace}-{TopicName}-{TopicSubName}-subscription ffc-demo-topic01-topicsub01-subscription Topic RoleAssignment NA NA {userassignedidentityName}-{TopicName}-{RoleName}-rbac-{index} sndadpinfmi1401-ffc-demo-web-ffc-demo-topic01-topicreceiver-rbac-0 Postgres Database {namespace}-{DatabaseName} ffc-demo-claim {postgresServerName}-{namespace}-{DatabaseName} sndadpdbsps1401-ffc-demo-claim Manage Idenitty {teamMIPrefix}-{serviceName} sndadpinfmi1401-ffc-demo-web {teamMIPrefix}-{serviceName} sndadpinfmi1401-ffc-demo-web StorageAccount {storageAccountPrefix}{StorageAccountName} sndadpinfst1401demo {serviceName}-{StorageAccountName} ffc-demo-web-sndadpinfst1401demo StorageAccountsBlobService default default {serviceName}-{StorageAccountName}-default ffc-demo-web-sndadpinfst1401demo-default StorageAccountsBlobServicesContainer {ContainerName} container-01 {serviceName}-{StorageAccountName}-default-{ContainerName} ffc-demo-web-sndadpinfst1401demo-default-container-01 StorageAccountsTableServicesTable {TableName} table01 {serviceName}-{StorageAccountName}-default-{TableName} ffc-demo-web-sndadpinfst1401demo-default-table01 PrivateEndpoint {privateEndpointPrefix}-{ResourceName}-{SubResource} sndadpinfpe1401-sndadpinfst1401demo-blob {privateEndpointPrefix}-{ResourceName}-{SubResource} sndadpinfpe1401-sndadpinfst1401demo-blob PrivateEndpointsPrivateDnsZoneGroup default default {PrivateEndpointName}-default sndadpinfpe1401-sndadpinfst1401demo-blob-default Helper templates \u00b6 In addition to the K8s object templates described above, a number of helper templates are defined in _helpers.tpl that are both used within the library chart and available to use within a consuming parent chart. Default check required message \u00b6 Template name: adp-aso-helm-library.default-check-required-msg Usage: {{- include \"adp-aso-helm-library.default-check-required-msg\" . }} A template defining the default message to print when checking for a required value within the library. This is not designed to be used outside of the library. Tags \u00b6 Template name: adp-aso-helm-library.commonTags Usage: {{- include \"adp-aso-helm-library.commonTags\" $ | nindent 4 }} ( $ is mapped to the root scope) Common tags to apply to tags of all ASO resource objects on the ADP K8s platform. This template relies on the globally required values listed above . Labels \u00b6 1 In Progress. Annotations \u00b6 For the Azure Service Operator to not delete the resources created in Azure on the deletion of the kubernetes resource manifest files, the below section can be added to Values.yaml in the parent helm chart. This specifies the reconcile policy to be used and can be set to manage , skip or detach-on-delete . More info over here . 1 2 asoAnnotations: serviceoperator.azure.com/reconcile-policy: detach-on-delete Licence \u00b6 THIS INFORMATION IS LICENSED UNDER THE CONDITIONS OF THE OPEN GOVERNMENT LICENCE found at: http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 The following attribution statement MUST be cited in your products and applications when using this information. Contains public sector information licensed under the Open Government license v3 About the licence \u00b6 The Open Government Licence (OGL) was developed by the Controller of Her Majesty's Stationery Office (HMSO) to enable information providers in the public sector to license the use and re-use of their information under a common open licence. It is designed to encourage use and re-use of information freely and flexibly, with only a few conditions.","title":"ASO Helm Library Chart"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#adp-platform-azure-service-operator-helm-library-chart","text":"A Helm chart library that captures general configuration for Azure Service Operator (ASO) resources. It can be used by any microservice Helm chart to import AzureServiceOperator K8s object templates configured to run on the ADP platform.","title":"ADP Platform Azure Service Operator Helm Library Chart"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#including-the-library-chart","text":"In your microservice Helm chart: * Update Chart.yaml to apiVersion: v2 . * Add the library chart under dependencies and choose the version you want (example below). Version number can include ~ or ^ to pick up latest PATCH and MINOR versions respectively. * Issue the following commands to add the repo that contains the library chart, update the repo, then update dependencies in your Helm chart: 1 2 3 helm repo add adp https://raw.githubusercontent.com/defra/adp-helm-repository/main/adp-aso-helm-library helm repo update helm dependency update <helm_chart_location> An example Demo microservice Chart.yaml : 1 2 3 4 5 6 7 8 apiVersion: v2 description: A Helm chart to deploy a microservice to the ADP Kubernetes platform name: demo-microservice version: 1.0.0 dependencies: - name: adp-aso-helm-library version: ^1.0.0 repository: https://raw.githubusercontent.com/defra/adp-helm-repository/main/adp-aso-helm-library NOTE: We will use ACR where ASO Helm Library Chart can be published. So above dependencies will be changed to import library from ACR (In Progress).","title":"Including the library chart"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#using-the-k8s-object-templates","text":"First, follow the instructions for including the ASO Helm library chart. The ASO Helm library chart has been configured using the conventions described in the Helm library chart documentation . The K8s object templates provide settings shared by all objects of that type, which can be augmented with extra settings from the parent (Demo microservice) chart. The library object templates will merge the library and parent templates. In the case where settings are defined in both the library and parent chart, the parent chart settings will take precedence, so library chart settings can be overridden. The library object templates will expect values to be set in the parent .values.yaml . Any required values (defined for each template below) that are not provided will result in an error message when processing the template ( helm install , helm upgrade , helm template ). The general strategy for using one of the library templates in the parent microservice Helm chart is to create a template for the K8s object formateted as so: 1 {{- include \"adp-aso-helm-library.namespace-queue\" . -}}","title":"Using the K8s object templates"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#all-template-required-values","text":"All the K8s object templates in the library require the following values to be set in the parent microservice Helm chart's values.yaml : 1 namespace: <string>","title":"All template required values"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#environment-specific-default-values","text":"The below values are used by the ASO templates internally, and their values are set using platform variables in adp-flux-services repository. for e.g. NameSpace Queues will get created inside serviceBusNamespaceName namespace and postgres database will get created inside postgresServerName server. Whilst the Platform orchestration will manage the 'platform' level variables, they can be optionally supplied in some circumstances. Examples include in sandpit/development when testing against team-specific infrastructure (that isn't Platform shared). So, if you have a dedicated Service Bus or Database Server instance, you can point to those to ensure you apps works as expected. Otherwise, don't supply the Platform level variables as these will be automatically managed and orchestrated throughout all the environments appropriately against core shared infrastructure. You (as a Platform Tenant) just supply your team-specific/instance specific infrastructure config' (i.e. Queues, Storage Accounts, Databases). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 namespace: <string> --namespace name environment: <string> --environment name fluxConfigNamespace: <string> --fluxConfig namespace name subscriptionId: <string> --subscription Id serviceBusResourceGroupName: <string> --Name of the service bus resource group serviceBusNamespaceName: <string> --Name of the environment specific service bus postgresResourceGroupName: <string> --Name of the Postgres server resource group postgresServerName: <string> --Name of the environment specific postgres server keyVaultResourceGroupName: <string> --Name of the keyvault resource group keyVaultName: <string> --Name of the environment specific keyVaultName teamMIPrefix: <string> --The prefix used for the ManageIdentity/UserAssignedIdentity resource name serviceName: <string> --Service name. Suffix used for ManageIdentity/UserAssignedIdentity resource name teamResourceGroupName: <string> --Team ResourceGroup Name where team resources are created virtualNetworkResourceGroupName: <string> --Virtual Network resource group virtualNetworkName: <string> --Virtual Network name storageAccountPrefix: <string> --The prefix used for the storage account resource name privateEndpointSubnetName: <string> --The name of the subnet for the service's private endpoint privateEndpointPrefix: <string> --The prefix used for the private endpoint resource name azrMSTPrivateLinkDNSUKSouthResourceGroupName: <string> --NOT USED. We need to discuss this further azrMSTPrivateLinkDNSUKWestResourceGroupName: <string> --NOT USED. We need to discuss this further azrMSTPrivateLinkDNSSubscriptionID: <string> --NOT USED. We need to discuss this further commonTags: environment: <string> serviceCode: <string> serviceName: <string> serviceType: <string> (Shared or Dedicated) kubernetes_cluster: <string> kubernetes_namespace: <string> kubernetes_label_serviceCode: <string>","title":"Environment specific Default values"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#namespace-queue","text":"Template file: _namespace-queue.yaml Template name: adp-aso-helm-library.namespace-queue An ASO NamespacesQueue object to create a Microsoft.ServiceBus/namespaces/queues resource. A basic usage of this object template would involve the creation of templates/namespace-queue.yaml in the parent Helm chart (e.g. adp-microservice ) containing: 1 {{- include \"adp-aso-helm-library.namespace-queue\" . -}}","title":"NameSpace Queue"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#required-values","text":"The following values need to be set in the parent chart's values.yaml in addition to the globally required values listed above . Note that namespaceQueues is an array of objects that can be used to create more than one queue. Please note that the queue name is prefixed with the namespace internally. For example, if the namespace name is \"adp-demo\" and you have provided the queue name as \"queue1\", then in the service bus, it creates a queue with the adp-demo-queue1 name. 1 2 3 namespaceQueues: - name: <string> - name: <string>","title":"Required values"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#optional-values","text":"The following values can optionally be set in the parent chart's values.yaml to set the other properties for servicebus queues: owner property is used to control the ownership of the queue. The default value is yes and you don't need to provide it if you are creating and owning the queue. If you are creating only role assignments for the queue you do not own, then you should explicitly set the owner flag to no so that it will only create the role assignments on the existing queue. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 namespaceQueues: - name: <string> owner: <string> --Default yes (Accepted values = yes, no) deadLetteringOnMessageExpiration: <bool> --Default false defaultMessageTimeToLive: <string> --Default P14D duplicateDetectionHistoryTimeWindow: <string> --Default PT10M enableBatchedOperations: <bool> --Default true enableExpress: <bool> --Default false enablePartitioning: <bool> --Default false lockDuration: <string> --Default PT1M maxDeliveryCount: <int> --Default 10 maxMessageSizeInKilobytes: <int> --Default 1024 maxSizeInMegabytes: <int> --Default 1024 requiresDuplicateDetection: <bool> --Default false requiresSession: <bool> --Default false roleAssignments: - roleName: <string> --<RoleName. for e.g QueueSender>","title":"Optional values"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#namespace-queue-roleassignments","text":"This template also optionally allows you to create Role Assignments by providing roleAssignments properties in the namespaceQueues object. Below are the minimum values that are required to be set in the parent chart's values.yaml to create a roleAssignments . 1 2 3 4 5 namespaceQueues: - name: <string> roleAssignments: <Array of Object> - roleName: <string> <RoleName. for e.g QueueSender> (Allowed values QueueSender', 'QueueReceiver') - roleName: <string> If you are creating only role assignments for the queue you do not own, then you should explicitly set the owner flag to no so that it will only create the role assignments on the existing queue.","title":"NameSpace Queue: RoleAssignments"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#usage-examples","text":"The following section provides usage examples for the Namespace Queues template.","title":"Usage examples"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-1-servicea-in-teama-creates-queue-with-2-role-assignments","text":"1 2 3 4 5 namespaceQueues: name: claim roleAssignments: - roleName: QueueSender - roleName: QueueReceiver","title":"Example 1 : ServiceA in TeamA creates queue with 2 role assignments"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-2-serviceb-in-teama-needs-to-receive-messages-from-existing-claim-queue-note-that-owner-is-set-to-no","text":"1 2 3 4 5 namespaceQueues: name: claim owner: 'no' roleAssignments: - roleName: QueueReceiver","title":"Example 2 : ServiceB in TeamA needs to receive messages from existing claim queue. Note that owner is set to no."},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#namespace-topic","text":"Template file: _namespace-topic.yaml Template name: adp-aso-helm-library.namespace-topic An ASO NamespacesTopic object to create a Microsoft.ServiceBus/namespaces/topics resource. A basic usage of this object template would involve the creation of templates/namespace-topic.yaml in the parent Helm chart (e.g. adp-microservice ) containing: 1 {{- include \"adp-aso-helm-library.namespace-topic\" . -}}","title":"NameSpace Topic"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#required-values_1","text":"The following values need to be set in the parent chart's values.yaml in addition to the globally required values listed above . Note that namespaceTopics is an array of objects that can be used to create more than one topic. Please note that the topic name is prefixed with the namespace internally. For example, if the namespace name is \"adp-demo\" and you have provided the topic name as \"topic1,\" then in the service bus, it creates a topic with the \"adp-demo-topic1\" name. 1 2 3 namespaceTopics: <Array of Object> - name: <string> - name: <string>","title":"Required values"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#optional-values_1","text":"The following values can optionally be set in the parent chart's values.yaml to set the other properties for namespaceTopics : owner property is used to control the ownership of the topic. The default value is yes and you don't need to provide it if you are creating and owning the topic. If you are only creating role assignments for the topic you do not own, then you should explicitly set the owner flag to no so that it will only create the role assignments on the existing topic. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 namespaceTopics: - name: <string> owner: <string> --Default yes (Accepted values = yes, no) defaultMessageTimeToLive: <string> --Default P14D duplicateDetectionHistoryTimeWindow: <string> --Default PT10M enableBatchedOperations: <bool> --Default true enableExpress: <bool> --Default false enablePartitioning: <bool> --Default false maxMessageSizeInKilobytes: <int> --Default 1024 maxSizeInMegabytes: <int> --Default 1024 requiresDuplicateDetection: <bool> --Default false supportOrdering: <bool> --Default true roleAssignments: - roleName: <string> --<RoleName. for e.g TopicSender>","title":"Optional values"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#namespace-topic-roleassignments","text":"This template also optionally allows you to create Role Assignments by providing roleAssignments properties in the namespaceTopics object. Below are the minimum values that are required to be set in the parent chart's values.yaml to create a roleAssignments . 1 2 3 4 5 namespaceTopics: - name: <string> roleAssignments: <Array of Object> - roleName: <string> <RoleName. for e.g TopicSender> (Allowed values TopicSender', 'TopicReceiver') - roleName: <string> If you are creating only role assignments for the Topic you do not own, then you should explicitly set the owner flag to no so that it will only create the role assignments on the existing Topic (See Example 2 in Usage examples section).","title":"NameSpace Topic: RoleAssignments"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#namespace-topic-subscriptions-subscriptionrules","text":"This template also optionally allows you to create Topic Subscriptions and Topic Subscriptions Rules for a given topic by providing Subscriptions and SubscriptionRules properties in the topic object. Below are the minimum values that are required to be set in the parent chart's values.yaml to create a NamespacesTopic , NamespacesTopicsSubscription and NamespacesTopicsSubscriptionsRule 1 2 3 4 5 6 7 8 9 namespaceTopics: - name: <string> topicSubscriptions: <Array of Object> refer \"Optional values for `topicSubscriptions`\" section for topicSubscriptions optional properties - name: <string> topicSubscriptionRules: <Array of Object> refer \"Optional values for `topicSubscriptionRules`\" section for topicSubscriptionRules properties - name: <string> filterType: <string> Accepted values : 'CorrelationFilter' or 'SqlFilter' correlationFilter: <Object> refer \"Optional values for `topicSubscriptionRules`\" section for correlationFilter properties sqlFilter: <Object> refer \"Optional values for `topicSubscriptionRules`\" section for sqlFilter properties To create topicSubscriptions inside already existing topics, set the property owner to no . By default owner is set to yes which creates the topic name defined in values (See Example 4 in Usage examples section).","title":"NameSpace Topic: Subscriptions, SubscriptionRules"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#optional-values-for-topicsubscriptions","text":"The following values can optionally be set in the parent chart's values.yaml to set the other properties for topicSubscriptions : 1 2 3 4 5 6 7 8 9 10 11 12 topicSubscriptions: - name: <string> deadLetteringOnFilterEvaluationExceptions: <bool> --Default false deadLetteringOnMessageExpiration: <bool> --Default false defaultMessageTimeToLive: <string> --Default P14D duplicateDetectionHistoryTimeWindow: <string> --Default P10M enableBatchedOperations: <bool> --Default true forwardTo: <string> isClientAffine: <bool> --Default false lockDuration: <string> --Default PT1M maxDeliveryCount: <int> --Default 10 requiresSession: <bool> --Default false","title":"Optional values for topicSubscriptions"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#optional-values-for-topicsubscriptionrules","text":"The following values can optionally be set in the parent chart's values.yaml to set the other properties for topicSubscriptionRules : 1 2 3 4 5 6 7 8 9 10 11 12 13 topicSubscriptionRules: - name: <string> correlationFilter: contentType: <string> correlationId: <string> label: <string> messageId: <string> replyTo: <string> replyToSessionId: <string> sessionId: <string> to: <string> sqlFilter: sqlExpression: <string>","title":"Optional values for topicSubscriptionRules"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#usage-examples_1","text":"The following section provides usage examples for the Namespace Topic template.","title":"Usage examples"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-1-servicea-in-teama-creates-topic-with-1-role-assignment","text":"1 2 3 4 namespaceTopics: name: claim-notify roleAssignments: - roleName: TopicSender","title":"Example 1 : ServiceA in TeamA creates Topic with 1 role assignment"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-2-serviceb-in-teama-needs-to-receive-messages-from-existing-claim-notify-topic-note-that-owner-is-set-to-no","text":"1 2 3 4 5 namespaceTopics: name: claim-notify owner: 'no' roleAssignments: - roleName: TopicReceiver","title":"Example 2 : ServiceB in TeamA needs to receive messages from existing claim-notify Topic. Note that owner is set to no."},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-3-servicea-in-teama-creates-topic-with-1-role-assignment-topic-subscription-and-topic-subscription-rule","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 namespaceTopics: name: claim-notify roleAssignments: - roleName: TopicSender topicSubscriptions: - name: claim-notify-subscription-01 topicSubscriptionRules: - name: claim-notify-subscription-rule-01 filterType: SqlFilter sqlFilter: sqlExpression: \"3=3\" - name: claim-notify-subscription-rule-02 filterType: CorrelationFilter sqlFilter: contentType: \"testvalue\"","title":"Example 3 : ServiceA in TeamA creates Topic with 1 role assignment, Topic Subscription and Topic Subscription Rule."},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-4-serviceb-in-teama-creates-topic-subscription-in-existing-topic","text":"1 2 3 4 5 6 7 namespaceTopics: name: claim-notify owner: \"no\" roleAssignments: - roleName: TopicReceiver topicSubscriptions: - name: claim-notify-subscription-03","title":"Example 4: ServiceB in TeamA creates Topic Subscription in existing Topic."},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#database-for-postgres-flexible-server-template","text":"Template file: _flexible-servers-db.yaml Template name: adp-aso-helm-library.flexible-servers-db An ASO FlexibleServersDatabase object. A basic usage of this object template would involve the creation of templates/flexible-servers-db.yaml in the parent Helm chart (e.g. adp-microservice ) containing: 1 2 3 4 {{- include \"adp-aso-helm-library.flexible-servers-db\" (list . \"adp-microservice.postgres-flexible-db\") -}} {{- define \"adp-microservice.postgres-flexible-db\" -}} # Microservice specific configuration in here {{- end -}}","title":"Database for Postgres Flexible server template"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#required-values_2","text":"The following values need to be set in the parent chart's values.yaml in addition to the globally required values listed above : 1 2 3 4 5 postgres: db: name: <string> charset: <string> collation: <string> Please note that the postgres DB name is prefixed with namespace internally. For example, if the namespace name is \"adp-microservice\" and you have provided the DB name as \"demo-db,\" then in the postgres server, it creates a database with the name \"adp-microservice-demo-db\".","title":"Required values"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#usage-examples_2","text":"The following section provides usage examples for the Flexible-Servers-Db template.","title":"Usage examples"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-1-servicea-in-teama-creates-payment-database","text":"1 2 3 4 5 postgres: db: name: payment charset: UTF8 collation: en_US.utf8","title":"Example 1 : ServiceA in TeamA creates payment database"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#userassignedidentity","text":"Template file: _userassignedidentity.yaml Template name: adp-aso-helm-library.userassignedidentity An ASO UserAssignedIdentity object to create a Microsoft.ManagedIdentity/userAssignedIdentities resource. A basic usage of this object template would involve the creation of templates/userassignedidentity.yaml in the parent Helm chart (e.g. adp-microservice ) containing: 1 {{- include \"adp-aso-helm-library.userassignedidentity\" . -}} This template uses the below values, whose values are set using platform variables in the adp-flux-services repository as a part of the service's ASO helmrelease value configuration, and you don't need to set them explicitly in the values.yaml file. teamMIPrefix serviceName teamResourceGroupName clusterOIDCIssuerUrl UserAssignedIdentity Name is derived internally, and it is set to = {TEAM_MI_PREFIX}-{SERVICE_NAME} For e.g. In SND1 if the TEAM_MI_PREFIX value is set to \"sndadpinfmid1401\" and SERVICE_NAME value is set to \"adp-demo-service\", then UserAssignedIdentity value will be : \"sndadpinfmid1401-adp-demo-service\".","title":"UserAssignedIdentity"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#optional-values_2","text":"The following values can optionally be set in the parent chart's values.yaml to set the other properties for servicebus queues: 1 2 userAssignedIdentity: location: <string> This template also optionally allows you to create Federated credentials for a given User Assigned Identity by providing federatedCreds properties in the userAssignedIdentity object. Below are the minimum values that are required to be set in the parent chart's values.yaml to create a userAssignedIdentity and federatedCreds . 1 2 3 4 userAssignedIdentity: federatedCreds: <Array of Object> - namespace: <string> serviceAccountName: <string>","title":"Optional values"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#usage-examples_3","text":"The following section provides usage examples for the UserAssignedIdentity template.","title":"Usage examples"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-1-the-below-example-will-create-userassignedidentity-with-one-federated-credential","text":"1 2 3 4 userAssignedIdentity: federatedCreds: - namespace: ffc-demo serviceAccountName: ffc-demo","title":"Example 1 : The below example will create userAssignedIdentity with one federated credential."},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#storage-account","text":"Template file: _storage-account.yaml Template name: adp-aso-helm-library.storage-account.yaml Version 2.0.0 and above Starting from version 2.0.1, the Storage Account has been enhanced with role assignments. These data role assignments are now scoped at the storage account level, introducing two new data roles: DataWriter and DataReader. The DataWriter role grants applications the ability to both read and write data in the blob container, tables, and files. Conversely, the DataReader role provides applications with read-only access to data in the blob container, tables, and files. An ASO StorageAccount object to create a Microsoft.Storage/storageAccounts resource and optionally sub resources Blob Containers and Tables. By default, private endpoints are always enabled on storage accounts and publicNetworkAccess is disabled. Optionally, you can also configure ipRules in scenarios where you want to limit access to your storage account to requests originating from specified IP addresses. Please be aware that this template only includes A records in the central DNS zone for the Dev, Tst, Pre, and Prd environments. For Sandpit environments snd1, snd2, and snd3, it currently only generates a private endpoint without adding an A record to the DNS zone. You will need to separately add this entry via PowerShell script. With this template, you can create the below resources. - Storage Accounts and RoleAssignments - Blob - Tables - FileShare A basic usage of this object template would involve the creation of templates/storage-account.yaml in the parent Helm chart (e.g. adp-microservice ) containing: 1 {{- include \"adp-aso-helm-library.storage-account\" . -}}","title":"Storage Account"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#default-values-for-storage-account","text":"Below are the default values used by the the storage account template internally, and they cannot be overridden by the user from the values.yaml file. 1 2 3 4 5 6 kind: \"StorageV2\" -- The type of storage account will always be \"StorageV2\" dnsEndpointType: \"Standard\" -- The type of endpoint minimumTlsVersion: \"TLS1_2\" allowBlobPublicAccess: \"false\" sku: \"Standard_LRS\" -- This is the sku for Sandpit environments (snd1, snd2, snd3) sku: \"Standard_RAGRS\" -- This is the sku for production environments (dev, test, pre, prd)","title":"Default values for Storage account"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#required-values-only-storage-account","text":"The following values need to be set in the parent chart's values.yaml in addition to the globally required values listed above . Note that storageAccounts is an array of objects that can be used to create more than one Storage Accounts. Please note that the storage account name must be unique across Azure. storage account name is internally prefixed with the storageAccountPrefix . For instance, in the Dev environment, the storageAccountPrefix is configured as devadpinfst2401 . If you input \"claim\" as the storage account name, the final storage account name will be devadpinfst2401claim . 1 2 3 4 5 6 7 storageAccounts: <Array of Object> - name: <string> --Storage account name. Name should be Lowercase letters and numbers and Maximum character limit is `9` roleAssignments: roleName: <string> --RoleAssignment Name (Accepted values = \"DataWriter\", \"DataReader\") - name: <string> roleAssignments: roleName: <string> --RoleAssignment Name (Accepted values = \"DataWriter\", \"DataReader\")","title":"Required values (Only Storage Account)"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#required-values-storage-account-with-blobcontainers-tables-and-fileshare","text":"The following values need to be set in the parent chart's values.yaml in addition to the globally required values listed above . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Version 2.0.0 and above storageAccounts: <Array of Object> - name: <string> --Storage account name. Name should be lowercase letters and numbers and Maximum character limit is `9` roleAssignments: roleName: <string> --RoleAssignment Name (Accepted values = \"DataWriter\", \"DataReader\") - name: <string> blobContainers: - name: <string> --Blob container name. Name should be lowercase and can contain only letters, numbers, and the hyphen/minus (-) character. Character limit: 3-63 - name: <string> tables: - name: <string> --Table name. Name should be lowercase and may contain only alphanumeric characters. and Character limit: 3-63 - name: <string> fileShares: - name: <string> --File Share name. Name should be lowercase and may contain only alphanumeric characters. and Character limit: 3-63 - name: <string> --File Share name. Name should be lowercase and may contain only alphanumeric characters. and Character limit: 3-63 accessTier: <string> --Access Tier. Allowed values are TransactionOptimized, Hot, Cold. Default is TransactionOptimized shareQuota: <int> --Storage Quota. Share Quota is defined in GiB. Default is 10","title":"Required values (Storage Account with BlobContainers, Tables and FileShare)"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#optional-values_3","text":"The following values can optionally be set in the parent chart's values.yaml to set the other properties for storageAccounts : For detailed description of each property see here owner property is used to control the ownership of the storage account. The default value is yes and you don't need to provide it if you are creating and owning the storage account. If you are creating Blob containers or Tables on the existing storage account that you do not own, then you should explicitly set the owner flag to no so that it will only create Blob containers or Tables on the existing storage account. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 storageAccounts: - name: <string> owner: <string> --Default \"yes\" (Accepted values = \"yes\", \"no\") location: <string> --Default \"uksouth\" accessTier: <string> --Default \"Hot\" (Accepted values = \"Hot\", \"Cool\") allowCrossTenantReplication: <bool> --Default false allowSharedKeyAccess: <bool> --Default false defaultToOAuthAuthentication: <string> --Default true (Accepted values = \"true\", \"false\") networkAcls: ipRules: <array> --Storage Firewall: Sets the IP ACL rules storageAccountsBlobService: --Confugure properties for the blob service changeFeed: --The blob service properties for change feed events enabled: <bool> --Default false retentionInDays: <int> --Applies when changeFeed.enabled is set to true containerDeleteRetentionPolicy: --The blob service properties for container soft delete enabled: <bool> --Default true days: <int> --Applies when containerDeleteRetentionPolicy.enabled is set to true. Default is 7 days deleteRetentionPolicy: --The blob service properties for blob soft delete enabled: <bool> --Default true days: <int> --Applies when deleteRetentionPolicy.enabled is set to true. Default is 7 days allowPermanentDelete: <bool> --Default false isVersioningEnabled: <bool> --Default false. Versioning is enabled if set to true restorePolicy: --The blob service properties for blob restore policy enabled: <bool> --Default false days: <int> --Applies when restorePolicy.enabled is set to true storageAccountsFileService: --Confugure properties for the blob service deleteRetentionPolicy: --The blob service properties for blob soft delete enabled: <bool> --Default true days: <int> --Applies when deleteRetentionPolicy.enabled is set to true. Default is 7 days blobContainers: --List of Blob containers - name: <string> --Blob container name tables: --List of Tables - name: <string> --Table name","title":"Optional values"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#usage-examples_4","text":"The following section provides usage examples for the storage account template.","title":"Usage examples"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-1-create-2-storage-accounts","text":"1 2 3 4 5 6 7 storageAccounts: - name: storage01 roleAssignments: - roleName: 'DataWriter' - name: storage02 roleAssignments: - roleName: 'DataReader'","title":"Example 1 : Create 2 storage accounts"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-2-create-1-storage-account-using-large-parameter-set-and-storage-firewall","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 storageAccounts: - name: storage01 roleAssignments: - roleName: 'DataWriter' accessTier: Hot location: uksouth allowCrossTenantReplication: false allowSharedKeyAccess: true defaultToOAuthAuthentication: \"false\" networkAcls: ipRules: - 82.13.86.001 - 82.13.86.002","title":"Example 2 : Create 1 storage account using large parameter set and storage firewall"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-3-create-1-storage-account-and-configure-properties-for-the-storage-account-blobservice","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 storageAccounts: - name: storage01 roleAssignments: - roleName: 'DataWriter' storageAccountsBlobService: changeFeed: enabled: true retentionInDays: 14 containerDeleteRetentionPolicy: days: 14 enabled: true deleteRetentionPolicy: allowPermanentDelete: true days: 20 enabled: false isVersioningEnabled: true restorePolicy: enabled: true days: 40","title":"Example 3 : Create 1 storage account and configure properties for the Storage Account BlobService"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-4-create-1-storage-account-with-2-blob-containers-and-1-table","text":"1 2 3 4 5 6 7 8 9 storageAccounts: - name: storage01 roleAssignments: - roleName: 'DataWriter' blobContainers: - name: container-01 - name: container-02 tables: - name: table01","title":"Example 4 : Create 1 storage account with 2 blob containers and 1 table"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-5-create-1-blob-containers-and-2-tables-for-the-existing-storage-account-in-your-team","text":"1 2 3 4 5 6 7 8 9 10 storageAccounts: - name: storage01 owner: \"No\" --Note owner is set to 'No' to indicate storage account already exists and is owned by a different service in the team roleAssignments: - roleName: 'DataWriter' blobContainers: - name: container-01 tables: - name: table01 - name: table02","title":"Example 5 : Create 1 blob containers and 2 tables for the existing storage account in your team."},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-6-create-1-storage-account-and-configure-properties-for-the-storage-account-fileservice","text":"1 2 3 4 5 6 7 8 storageAccounts: - name: storage01 roleAssignments: - roleName: 'DataWriter' storageAccountsFileService: deleteRetentionPolicy: enabled: true days: 20","title":"Example 6 : Create 1 storage account and configure properties for the Storage Account FileService"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-7-create-2-file-share-one-with-default-properties-other-one-with-specific-properties","text":"1 2 3 4 5 6 7 8 9 10 storageAccounts: - name: storage01 owner: \"No\" --Note owner is set to 'No' to indicate storage account already exists and is owned by a different service in the team roleAssignments: - roleName: 'DataWriter' fileShares: - name: share-01 - name: share-02 accessTier: Hot shareQuota: 50","title":"Example 7 : Create 2 file share, one with default properties other one with specific properties"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#example-9-create-roleassignments-with-reader-access-for-existing-storage-account-in-the-team","text":"1 2 3 4 5 storageAccounts: - name: storage01 owner: 'no' roleAssignments: - roleName: 'DataReader'","title":"Example 9 : Create roleassignments with reader access for existing storage account in the team"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#reference-table-for-resource-names-in-azure-and-kubernetes","text":"The table below shows the Azure Service Operator (ASO) resource naming convention in Azure and Kubernetes: In the example below, the following platform values are used for demonstration purposes: - namespace = 'ffc-demo' - serviceName = 'ffc-demo-web' - teamMIPrefix = 'sndadpinfmi1401' - storageAccountPrefix = 'sndadpinfst1401' - privateEndpointPrefix = 'sndadpinfpe1401' - postgresServerName = 'sndadpdbsps1401' - userassignedidentityName = 'sndadpinfmi1401-ffc-demo-web' And the following user input values are used for demonstration purposes: QueueName = 'queue01' TopicName = 'topic01' TopicSubName = 'topicSub01' DatabaseName = 'claim' StorageAccountName = 'demo' Resource Type Resource Name Format in Azure Resource Name Example in Azure Resource Name Format in Kubernetes Resource Name Example in Kubernetes NamespacesQueue {namespace}-{QueueName} ffc-demo-queue01 {namespace}-{QueueName} ffc-demo-queue01 Queue RoleAssignment NA NA {userassignedidentityName}-{QueueName}-{RoleName}-rbac-{index} sndadpinfmi1401-ffc-demo-web-ffc-demo-queue01-queuereceiver-rbac-0 NamespacesTopic {namespace}-{TopicName} ffc-demo-topic01 {namespace}-{TopicName} ffc-demo-topic01 NamespacesTopicsSubscription {namespace}-{TopicSubName} ffc-demo-topicSub01 {namespace}-{TopicName}-{TopicSubName}-subscription ffc-demo-topic01-topicsub01-subscription Topic RoleAssignment NA NA {userassignedidentityName}-{TopicName}-{RoleName}-rbac-{index} sndadpinfmi1401-ffc-demo-web-ffc-demo-topic01-topicreceiver-rbac-0 Postgres Database {namespace}-{DatabaseName} ffc-demo-claim {postgresServerName}-{namespace}-{DatabaseName} sndadpdbsps1401-ffc-demo-claim Manage Idenitty {teamMIPrefix}-{serviceName} sndadpinfmi1401-ffc-demo-web {teamMIPrefix}-{serviceName} sndadpinfmi1401-ffc-demo-web StorageAccount {storageAccountPrefix}{StorageAccountName} sndadpinfst1401demo {serviceName}-{StorageAccountName} ffc-demo-web-sndadpinfst1401demo StorageAccountsBlobService default default {serviceName}-{StorageAccountName}-default ffc-demo-web-sndadpinfst1401demo-default StorageAccountsBlobServicesContainer {ContainerName} container-01 {serviceName}-{StorageAccountName}-default-{ContainerName} ffc-demo-web-sndadpinfst1401demo-default-container-01 StorageAccountsTableServicesTable {TableName} table01 {serviceName}-{StorageAccountName}-default-{TableName} ffc-demo-web-sndadpinfst1401demo-default-table01 PrivateEndpoint {privateEndpointPrefix}-{ResourceName}-{SubResource} sndadpinfpe1401-sndadpinfst1401demo-blob {privateEndpointPrefix}-{ResourceName}-{SubResource} sndadpinfpe1401-sndadpinfst1401demo-blob PrivateEndpointsPrivateDnsZoneGroup default default {PrivateEndpointName}-default sndadpinfpe1401-sndadpinfst1401demo-blob-default","title":"Reference Table for Resource Names in Azure and Kubernetes"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#helper-templates","text":"In addition to the K8s object templates described above, a number of helper templates are defined in _helpers.tpl that are both used within the library chart and available to use within a consuming parent chart.","title":"Helper templates"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#default-check-required-message","text":"Template name: adp-aso-helm-library.default-check-required-msg Usage: {{- include \"adp-aso-helm-library.default-check-required-msg\" . }} A template defining the default message to print when checking for a required value within the library. This is not designed to be used outside of the library.","title":"Default check required message"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#tags","text":"Template name: adp-aso-helm-library.commonTags Usage: {{- include \"adp-aso-helm-library.commonTags\" $ | nindent 4 }} ( $ is mapped to the root scope) Common tags to apply to tags of all ASO resource objects on the ADP K8s platform. This template relies on the globally required values listed above .","title":"Tags"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#labels","text":"1 In Progress.","title":"Labels"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#annotations","text":"For the Azure Service Operator to not delete the resources created in Azure on the deletion of the kubernetes resource manifest files, the below section can be added to Values.yaml in the parent helm chart. This specifies the reconcile policy to be used and can be set to manage , skip or detach-on-delete . More info over here . 1 2 asoAnnotations: serviceoperator.azure.com/reconcile-policy: detach-on-delete","title":"Annotations"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#licence","text":"THIS INFORMATION IS LICENSED UNDER THE CONDITIONS OF THE OPEN GOVERNMENT LICENCE found at: http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 The following attribution statement MUST be cited in your products and applications when using this information. Contains public sector information licensed under the Open Government license v3","title":"Licence"},{"location":"Developer-Reference/Infrastructure/ASO%20Helm%20Library%20Chart/#about-the-licence","text":"The Open Government Licence (OGL) was developed by the Controller of Her Majesty's Stationery Office (HMSO) to enable information providers in the public sector to license the use and re-use of their information under a common open licence. It is designed to encourage use and re-use of information freely and flexibly, with only a few conditions.","title":"About the licence"},{"location":"Developer-Reference/Infrastructure/helm-library-memory-and-cpu-tiers/","text":"Helm Library - Memory and CPU Tiers \u00b6 We have now implemented an abstraction layer within the adp-helm-library that allows the dynamic allocation of memory and CPU resources based on the memory and cpu tier provided in the values.yaml file. The new memory and cpu tier values are in the below table: TIER CPU-REQUEST CPU-LIMIT MEMORY-REQUEST MEMORY-LIMIT S 50m 50m 50Mi 50Mi M 100m 100m 100Mi 100Mi L 150m 150m 150Mi 150Mi XL 200m 200m 200Mi 200Mi XXL 300m 600m 300Mi 600Mi CUSTOM <?> <?> <?> <?> Instructions The following values can optionally be set in a values.yaml to select the required CPU and Memory for a container: 1 2 3 4 5 6 container: memCpuTier: <string S|M|L|XL|XXL|CUSTOM> requestMemory: <string - REQUIRED if memCpuTier is CUSTOM> requestCpu: <string - REQUIRED if memCpuTier is CUSTOM> limitMemory: <string - REQUIRED if memCpuTier is CUSTOM> limitCpu: <string - REQUIRED if memCpuTier is CUSTOM> example 1 - select an Extra Large (XL) tier: 1 2 container: memCpuTier: XL example 2 - select an Small (S) tier: 1 2 container: memCpuTier: S example 3 - select custom values and provide your own values if the TIER sizes don't fit your requirements. 1 2 3 4 5 6 container: memCpuTier: CUSTOM requestMemory: 10Mi requestCpu: 10m limitMemory: 200Mi limitCpu: 200m example 4 - The default is Medium (M). If this works for you then you don't need to pass a memCpuTier. 1 container: {} NOTE: If you do not add a 'memCpuTier' then the Tier will default to 'M' NOTE: You can also choose CUSTOM and provide your own values if the TIER sizes don't fit your requirements. If you choose CUSTOM, requestMemory, requestCpu, limitMemory and limitCpu are required. IMPORTANT: Your team namespace will be given a fixed amount of resources via ResourceQuotas. Once your cumulative resource request total passes the assigned quota on your namespace, all further deployments will be unsuccessful. If you require an increase to your ResourceQuota, you will need to raise a request via the ADP team. It's important you monitor the performance of your application and adjust pod requests and limits accordingly. Please choose the appropriate cpu and memory tier for your application or provide custom values for your CPU and Memory requests and limits. References https://learn.microsoft.com/en-us/azure/aks/developer-best-practices-resource-management https://learn.microsoft.com/en-us/azure/aks/operator-best-practices-scheduler#enforce-resource-quotas","title":"Helm Library Memory & CPU Tiers"},{"location":"Developer-Reference/Infrastructure/helm-library-memory-and-cpu-tiers/#helm-library-memory-and-cpu-tiers","text":"We have now implemented an abstraction layer within the adp-helm-library that allows the dynamic allocation of memory and CPU resources based on the memory and cpu tier provided in the values.yaml file. The new memory and cpu tier values are in the below table: TIER CPU-REQUEST CPU-LIMIT MEMORY-REQUEST MEMORY-LIMIT S 50m 50m 50Mi 50Mi M 100m 100m 100Mi 100Mi L 150m 150m 150Mi 150Mi XL 200m 200m 200Mi 200Mi XXL 300m 600m 300Mi 600Mi CUSTOM <?> <?> <?> <?> Instructions The following values can optionally be set in a values.yaml to select the required CPU and Memory for a container: 1 2 3 4 5 6 container: memCpuTier: <string S|M|L|XL|XXL|CUSTOM> requestMemory: <string - REQUIRED if memCpuTier is CUSTOM> requestCpu: <string - REQUIRED if memCpuTier is CUSTOM> limitMemory: <string - REQUIRED if memCpuTier is CUSTOM> limitCpu: <string - REQUIRED if memCpuTier is CUSTOM> example 1 - select an Extra Large (XL) tier: 1 2 container: memCpuTier: XL example 2 - select an Small (S) tier: 1 2 container: memCpuTier: S example 3 - select custom values and provide your own values if the TIER sizes don't fit your requirements. 1 2 3 4 5 6 container: memCpuTier: CUSTOM requestMemory: 10Mi requestCpu: 10m limitMemory: 200Mi limitCpu: 200m example 4 - The default is Medium (M). If this works for you then you don't need to pass a memCpuTier. 1 container: {} NOTE: If you do not add a 'memCpuTier' then the Tier will default to 'M' NOTE: You can also choose CUSTOM and provide your own values if the TIER sizes don't fit your requirements. If you choose CUSTOM, requestMemory, requestCpu, limitMemory and limitCpu are required. IMPORTANT: Your team namespace will be given a fixed amount of resources via ResourceQuotas. Once your cumulative resource request total passes the assigned quota on your namespace, all further deployments will be unsuccessful. If you require an increase to your ResourceQuota, you will need to raise a request via the ADP team. It's important you monitor the performance of your application and adjust pod requests and limits accordingly. Please choose the appropriate cpu and memory tier for your application or provide custom values for your CPU and Memory requests and limits. References https://learn.microsoft.com/en-us/azure/aks/developer-best-practices-resource-management https://learn.microsoft.com/en-us/azure/aks/operator-best-practices-scheduler#enforce-resource-quotas","title":"Helm Library - Memory and CPU Tiers"},{"location":"Developer-Reference/Quality-Assurance/Quality-Assurance-Overview/","text":"ADP Quality Assurance Approach \u00b6 This document outlines the QA approach for the Azure Developer Platform (APD). The objective of the quality assurance is to ensure that all business applications developed and hosted on the ADP meet DEFRA's standards of quality, reliability and performance. The Quality Assurance approach follows the traditional QA Pyramid that modes how software testing is categorised and layered. Guidelines \u00b6 All Testing Tooling that is used has to have been approved by the DEFRA Tools Authority Test Results must confirm to the agreed DEFRA standards e.g. 90% code coverage for unit tests. Selected Tools \u00b6 Below are the tools that are currently supported on the ADP Type of Test Tooling Unit Testing \"C# NUnit/ xUnit, Nsubstitute: NodeJS: Jest Functional/Acceptance WebDriver.IO Security Testing \"OWASP ZAP (Zed Attack Proxy) API Testing (Contract Testing) PACT Broker Accessibility Testing AXE Lighthouse? Performance Testing JMeter, BrowserStack. Azure Load Testing is under consideration Exploratory Testing (Manual) ADO Test Plans How to create Tests in the ADP \u00b6 Development teams use the ADP Portal to scaffold a new service using one of the exemplar software templates (refer to How to create a platform service ). Based on the template type (frontend or backend), basic tests will be included that the teams can build on as they add more functionality to the service. The ADP Platform provides the ability to execute the above tests. These tests are executed as post deployment tests. The pipeline will check for the existence of specific docker-compose test files to determine if it can run tests. Refer to the how-to-guides for the different types of tests. However, it is the responsibility of the delivery projects to ensure that the business services they are delivering have written sufficient tests for the different types of tests that meet DEFRA's standards. Unit Tests \u00b6 The supported programming frameworks are .NET/C# and NodeJS/Javascript. The unit tests are executed in the CI Build Pipeline. SonarQube analysis has been integrated in the ADP Core Pipeline Teplate to ensure the code conforms to the DEFRA quality standards. Links to the SonarCloud analysis, Synk Analysis will available in the component page of the service in the ADP Portal. Functional/Acceptance Testing \u00b6 These end-to-end tests for internal (via Open VPN) or public endpoints for frontends and APIs. Refer to the Guide on how to create an Acceptance Test Performance Testing \u00b6 These tests should be executed against internal (via Open VPN) or public endpoints for frontends and APIs. Docker is used with BrowserStack to execute the peformance tests. As a pre-requisite, Non Functional Requirements should be defined by the delivery project to set the baseline for the expected behavior e.g. expected average API response time, page load duration. There are various types of performance tests. Load tests access the peformance of the service under a typical and peak load Stress Load tests are intended to test the limits of the service. Spike tests are similiar to stress load tests, however, they test the service with sudden surges in traffic. Soak tests verify the reliability of the system over a long period time. Refer to the Guide on how to create a Performance Test Accessibility Testing \u00b6 These tests verify that the all DEFRA public websites/business services are in compliance with WCAG 2.2 AA accessibility standard Refer to the guidiance on Understanding accessibility requirements for public sector bodies Security Testing \u00b6 SonarQube Security Testing has been incorporated into the CI Build Pipeline. In addition to that, OWASP ZAP is executed as per of the post deployment tests.","title":"Overview of Quality Assurnace in ADP"},{"location":"Developer-Reference/Quality-Assurance/Quality-Assurance-Overview/#adp-quality-assurance-approach","text":"This document outlines the QA approach for the Azure Developer Platform (APD). The objective of the quality assurance is to ensure that all business applications developed and hosted on the ADP meet DEFRA's standards of quality, reliability and performance. The Quality Assurance approach follows the traditional QA Pyramid that modes how software testing is categorised and layered.","title":"ADP Quality Assurance Approach"},{"location":"Developer-Reference/Quality-Assurance/Quality-Assurance-Overview/#guidelines","text":"All Testing Tooling that is used has to have been approved by the DEFRA Tools Authority Test Results must confirm to the agreed DEFRA standards e.g. 90% code coverage for unit tests.","title":"Guidelines"},{"location":"Developer-Reference/Quality-Assurance/Quality-Assurance-Overview/#selected-tools","text":"Below are the tools that are currently supported on the ADP Type of Test Tooling Unit Testing \"C# NUnit/ xUnit, Nsubstitute: NodeJS: Jest Functional/Acceptance WebDriver.IO Security Testing \"OWASP ZAP (Zed Attack Proxy) API Testing (Contract Testing) PACT Broker Accessibility Testing AXE Lighthouse? Performance Testing JMeter, BrowserStack. Azure Load Testing is under consideration Exploratory Testing (Manual) ADO Test Plans","title":"Selected Tools"},{"location":"Developer-Reference/Quality-Assurance/Quality-Assurance-Overview/#how-to-create-tests-in-the-adp","text":"Development teams use the ADP Portal to scaffold a new service using one of the exemplar software templates (refer to How to create a platform service ). Based on the template type (frontend or backend), basic tests will be included that the teams can build on as they add more functionality to the service. The ADP Platform provides the ability to execute the above tests. These tests are executed as post deployment tests. The pipeline will check for the existence of specific docker-compose test files to determine if it can run tests. Refer to the how-to-guides for the different types of tests. However, it is the responsibility of the delivery projects to ensure that the business services they are delivering have written sufficient tests for the different types of tests that meet DEFRA's standards.","title":"How to create Tests in the ADP"},{"location":"Developer-Reference/Quality-Assurance/Quality-Assurance-Overview/#unit-tests","text":"The supported programming frameworks are .NET/C# and NodeJS/Javascript. The unit tests are executed in the CI Build Pipeline. SonarQube analysis has been integrated in the ADP Core Pipeline Teplate to ensure the code conforms to the DEFRA quality standards. Links to the SonarCloud analysis, Synk Analysis will available in the component page of the service in the ADP Portal.","title":"Unit Tests"},{"location":"Developer-Reference/Quality-Assurance/Quality-Assurance-Overview/#functionalacceptance-testing","text":"These end-to-end tests for internal (via Open VPN) or public endpoints for frontends and APIs. Refer to the Guide on how to create an Acceptance Test","title":"Functional/Acceptance Testing"},{"location":"Developer-Reference/Quality-Assurance/Quality-Assurance-Overview/#performance-testing","text":"These tests should be executed against internal (via Open VPN) or public endpoints for frontends and APIs. Docker is used with BrowserStack to execute the peformance tests. As a pre-requisite, Non Functional Requirements should be defined by the delivery project to set the baseline for the expected behavior e.g. expected average API response time, page load duration. There are various types of performance tests. Load tests access the peformance of the service under a typical and peak load Stress Load tests are intended to test the limits of the service. Spike tests are similiar to stress load tests, however, they test the service with sudden surges in traffic. Soak tests verify the reliability of the system over a long period time. Refer to the Guide on how to create a Performance Test","title":"Performance Testing"},{"location":"Developer-Reference/Quality-Assurance/Quality-Assurance-Overview/#accessibility-testing","text":"These tests verify that the all DEFRA public websites/business services are in compliance with WCAG 2.2 AA accessibility standard Refer to the guidiance on Understanding accessibility requirements for public sector bodies","title":"Accessibility Testing"},{"location":"Developer-Reference/Quality-Assurance/Quality-Assurance-Overview/#security-testing","text":"SonarQube Security Testing has been incorporated into the CI Build Pipeline. In addition to that, OWASP ZAP is executed as per of the post deployment tests.","title":"Security Testing"},{"location":"Developer-Reference/Secret%20Management/secret-management/","text":"ADP Services Secret Management The secrets in ADP services are managed using Azure Key Vault. The secretes for each individual services are imported to the Azure Key Vault through ADO Pipeline tasks and are accessed by the services by using the application configuration YAML files. The importing of secretes to the Key Vault and referencing them by individual services are automated using ADO Pipelines. The detailed workflow for secret management includes the following steps: 1. Configure ADO Library Create variable groups for each environments of the service in ADO Library. Naming Convection: It follows the following convention for creating the variable groups for a service. 1 {service name}-{env} Example: The variable groups for different environment of a service are shown below. Add the variables and the values for the secretes in each of the variable groups. Variable Naming Convection: It follows the following convention for creating the variables in variable groups. 1 {service name}-{variable name} Example: Secrete variables for a service are shown below. 2. ADO Pipeline - Import secrets to Key Vault The variables and values from the variable groups are automated to import to the Azure Key Vault using Pipeline task and Power Shell scripts. Repo: ADO-PIPELINE-COMMON Example: The code snippet involved in importing the secrets to the Azure Key Vault is shown below. 3. Azure Key Vault - Imported secretes After the secretes are added to the ADO Library variable groups and the service CI pipeline run successfully would import the secrets to the Key Vault as shown below. Example: Secretes imported to the Key Vault for a service are shown below 4. App Config Access the secrets from the Key Vault through appConfig YAML files included in each of the services. There are two different kinds of appConfig files. Environment specific appConfig file: Each service has it own environment specific appConfig file to access it respective secrets from the Key Vault. File Naming Convection: 1 appConfig.{environment}.yaml Common appConfig file: There is a common appConfig.yaml file included in each of the service that defines the environment variables that commonly used by all of the service environments. Example: The appConfig files for different environments for a service are shown below. The type of the variable (key) that reference the secretes form the Key Vault should be defined as type: \"keyvault\" in the config YAML file. 4. ADO Pipeline - Import App Config The Pipeline tasks shown below use the environment specific appConfig YAML files to import the secrets from Azure Key Vault to the service. Repo: ADO-PIPELINE-COMMON 5. Run Pipeline - appConfig only The secretes can be added to the Key Vault and also referenced by the service using the appConfig files. This can be achieved by running the pipeline on selecting the Deploy App Config check box. This helps in running only the secrete management tasks instated of running all the tasks in the pipeline. This is useful when updating the secretes of a service.","title":"Secret Management"},{"location":"Developer-Reference/adp-portal/ADP%20Portal/","text":"Azure Developer Platform Portal (ADP) \u00b6 The Azure Development Platform Portal built using Backstage . The Portal enables users to self service by providing the functionality below. Onboard delivery programmes, projects and users onto the ADP Platform Enable Developers to create microservices using software templates based on GDS standards Getting Started \u00b6 Include instructions for getting the project set up locally. This may include steps to install dependencies, configure the development environment, and run the project. 1 2 yarn install yarn dev","title":"Azure Developer Platform Portal (ADP)"},{"location":"Developer-Reference/adp-portal/ADP%20Portal/#azure-developer-platform-portal-adp","text":"The Azure Development Platform Portal built using Backstage . The Portal enables users to self service by providing the functionality below. Onboard delivery programmes, projects and users onto the ADP Platform Enable Developers to create microservices using software templates based on GDS standards","title":"Azure Developer Platform Portal (ADP)"},{"location":"Developer-Reference/adp-portal/ADP%20Portal/#getting-started","text":"Include instructions for getting the project set up locally. This may include steps to install dependencies, configure the development environment, and run the project. 1 2 yarn install yarn dev","title":"Getting Started"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/","text":"ADP Data Plugin \u00b6 Overview \u00b6 ADP enables authorised users to self-service through the platform, allowing them to create and manage required arms-length bodies, delivery programmes, and delivery teams. The data added can subsequently be edited by those authorised users and viewed by all. The diagram below illustrates the high-level process flow of user journeys, distinguishing between four types of users: ADP Admins, Programme Managers, Project Managers, and Project Developers. ADP Admins have the authority to create new ALBs (Arms-Length Bodies) and initially seed Programme Managers. Programme Managers are able to onboard additional Programme and Project Managers, as well as to create Delivery Programmes and Projects. Project Managers have the capability to create new Delivery Projects and onboard Delivery Project Members. Finally, Project Developers are tasked with creating and managing platform services. Portal Permissions \u00b6 In the table below you can see the permissions per ADP Persona. Please note that users are not restricted to one role/persona. A single person may be a Programme Manager, a Team Manager for a team in their Programme and a developer within that team. Backend APIs \u00b6 ALB \u00b6 Endpoint: /armsLengthBody \u00b6 Method Parameters Example Request Body Example Response GET N/A [{ \"creator\":\"user:default/johnDoe\", \"owner\":\"owner value\", \"title\":\"ALB 1\", \"alias\":\"ALB\", \"description\": \"ALB description\", \"url\": null, \"name\":\"alb-1\", \"id\": \"123\", \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} , { \"creator\":\"user:default/johnDoe\", \"owner\":\"owner value\", \"title\":\"ALB 2\", \"alias\":\"ALB\", \"description\": \"ALB description\", \"url\": null, \"name\":\"alb-2\", \"id\": \"1234\", \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"}] GET id N/A { \"creator\":\"user:default/johnDoe\", \"owner\":\"owner value\", \"title\":\"ALB 1\", \"alias\":\"ALB\", \"description\": \"ALB description\", \"url\": null, \"name\":\"alb-1\", \"id\": \"123\", \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} POST { \"title\": \"ALB\", \"description\": \"ALB Description\" } { \"title\": \"ALB\", \"description\": \"ALB Description\" , \"url\": null, \"alias\": null, \"name\": \"alb\", \"creator\":\"user:default/johnDoe\", \"owner\":\"owner value\", \"id\": \"12345\",\"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} PATCH id { \"id\": \"12345\", \"title\": \"Updated ALB Title\" } { \"title\": \"Updated ALB Title\", \"description\": \"ALB Description\" , \"url\": null, \"alias\": null, \"name\": \"alb\", \"creator\":\"user:default/johnDoe\", \"owner\":\"owner value\", \"id\": \"12345\",\"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} Endpoint: /armsLengthBodyNames \u00b6 Method Example Response GET {\"123\": \"ALB 1\",\"1234\": \"ALB 2\",\"12345\": \"ALB 3\",\"123456\": \"ALB 4\"} Delivery Programmes \u00b6 Endpoint: /deliveryProgramme \u00b6 Method Parameters Example Request Body Example Response GET N/A [{ \"id\": \"123\", \"programme_managers\":[], \"arms_length_body_id\": \"12345\", \"title\": \"Delivery Programme 1\", \"name\": \"delivery-programme-1\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" , \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"}, { \"id\": \"1234\", \"programme_managers\":[], \"arms_length_body_id\": \"12345\", \"title\": \"Delivery Programme 2\", \"name\": \"delivery-programme-2\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" , \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"}] GET id N/A { \"id\": \"123\", \"programme_managers\":[{\"aad_entity_ref_id\": \"123\", \"id\": \"1\", \"delivery_programme_id\" :\"123\", \"email\": \"email@example.com\", \"name\": \"John Doe\"}], \"arms_length_body_id\": \"12345\", \"title\": \"Delivery Programme 1\", \"name\": \"delivery-programme-1\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" , \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} POST { \"programme_managers\":[{\"aad_entity_ref_id\": \"123\"}, {\"aad_entity_ref_id\": \"1234\"}], \"arms_length_body_id\": \"12345\", \"title\": \"Delivery Programme\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" } { \"id\": \"1234\", \"programme_managers\":[], \"arms_length_body_id\": \"12345\", \"title\": \"Delivery Programme\", \"name\": \"delivery-programme\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" , \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} PATCH { \"id\": \"1234\", \"title\": \"Updated Delivery Programme Title\" } { \"id\": \"1234\", \"programme_managers\":[], \"arms_length_body_id\": \"12345\", \"title\": \"Updated Delivery Programme Title, \"name\": \"delivery-programme\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" , \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} Endpoint: /programmeManager \u00b6 Method Example Response GET [{\"id\": \"5464de88-bc76-4a0b-a491-77284c392dab\",\"delivery_programme_id\": \"0bd0cb6b-569a-4c0f-bc6d-5b8708f45c4a\",\"aad_entity_ref_id\": \"aad entity ref id 1\" \"email\": \"example@defra.onmicrosoft.com\",\"name\": \"name 1\"},{\"id\": \"f0bca259-d0a2-4d30-8166-4569f8e7b6f2\",\"delivery_programme_id\": \"0bd0cb6b-569a-4c0f-bc6d-5b8708f45c4a\",\"aad_entity_ref_id\": \"aad entity ref id 2\",\"email\": \"example@defra.onmicrosoft.com\",\"name\": \"name 2\"}] Endpoint: /catalogEntities \u00b6 Method Example Response GET {\"items\": [ {\"metadata\": { \"name\": \"example.onmicrosoft.com\", \"annotations\": {\"graph.microsoft.com/user-id\": \"aad entity ref id 1\",\"microsoft.com/email\": \"example@defra.onmicrosoft.com\"}},\"spec\": {\"profile\": {\"displayName\": \"name 1\"}}},{\"metadata\": {\"name\": \"example.onmicrosoft.com\",\"annotations\": {\"graph.microsoft.com/user-id\": \"aad entity ref id 2\",\"microsoft.com/email\": \"example@defra.onmicrosoft.com\"}},\"spec\": {\"profile\": {\"displayName\": \"name 2\"}}}]} Delivery Projects \u00b6 Enpoint: /deliveryProject \u00b6 Method Parameters Example Request Body Example Response GET N/A [{\"id\": \"123\",\"name\": \"delivery-project-1\",\"title\": \"Delivery Project 1\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\",\"finance_code\": \"\",\"delivery_programme_id\": \"1\",\"delivery_project_code\": \"1\",\"url\": \"\",\"ado_project\": \"\",\"created_at\": \"2024-04-03T06:41:56.257Z\",\"updated_at\": \"2024-04-03T08:42:48.242Z\",\"updated_by\": \"user:default/johnDoe.com\"}, {\"id\": \"1234\",\"name\": \"delivery-project-2\",\"title\": \"Delivery Project 2\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\", \"finance_code\": \"\", \"delivery_programme_id\": \"2\", \"delivery_project_code\": \"2\", \"url\": \"\", \"ado_project\": \"\", \"created_at\": \"2024-04-03T05:42:31.914Z\", \"updated_at\": \"2024-04-03T08:43:03.622Z\",\"updated_by\": \"user:default/johnDoe\"}] GET id N/A {\"id\": \"1234\",\"name\": \"delivery-project-2\",\"title\": \"Delivery Project 2\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\", \"finance_code\": \"\", \"delivery_programme_id\": \"2\", \"delivery_project_code\": \"2\", \"url\": \"\", \"ado_project\": \"\", \"created_at\": \"2024-04-03T05:42:31.914Z\", \"updated_at\": \"2024-04-03T08:43:03.622Z\",\"updated_by\": \"user:default/johnDoe\"} POST \"title\": \"Delivery Project 3\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\", \"finance_code\": \"\", \"delivery_programme_id\": \"3\", \"delivery_project_code\": \"3\", \"url\": \"\", \"ado_project\": \"\"} {\"id\": \"12345\",\"name\": \"delivery-project-3\",\"title\": \"Delivery Project 3\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\", \"finance_code\": \"\", \"delivery_programme_id\": \"3\", \"delivery_project_code\": \"3\", \"url\": \"\", \"ado_project\": \"\", \"created_at\": \"2024-04-03T05:42:31.914Z\", \"updated_at\": \"2024-04-03T08:43:03.622Z\",\"updated_by\": \"user:default/johnDoe\"} PATCH { \"id\": \"12345\", \"title\": \"Updated Delivery Project Title\" } {\"id\": \"12345\",\"name\": \"delivery-project-3\",\"title\": \"Updated Delivery Project Title\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\", \"finance_code\": \"\", \"delivery_programme_id\": \"3\", \"delivery_project_code\": \"3\", \"url\": \"\", \"ado_project\": \"\", \"created_at\": \"2024-04-03T05:42:31.914Z\", \"updated_at\": \"2024-04-03T08:43:03.622Z\",\"updated_by\": \"user:default/johnDoe\"}","title":"ADP Data Plugin"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#adp-data-plugin","text":"","title":"ADP Data Plugin"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#overview","text":"ADP enables authorised users to self-service through the platform, allowing them to create and manage required arms-length bodies, delivery programmes, and delivery teams. The data added can subsequently be edited by those authorised users and viewed by all. The diagram below illustrates the high-level process flow of user journeys, distinguishing between four types of users: ADP Admins, Programme Managers, Project Managers, and Project Developers. ADP Admins have the authority to create new ALBs (Arms-Length Bodies) and initially seed Programme Managers. Programme Managers are able to onboard additional Programme and Project Managers, as well as to create Delivery Programmes and Projects. Project Managers have the capability to create new Delivery Projects and onboard Delivery Project Members. Finally, Project Developers are tasked with creating and managing platform services.","title":"Overview"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#portal-permissions","text":"In the table below you can see the permissions per ADP Persona. Please note that users are not restricted to one role/persona. A single person may be a Programme Manager, a Team Manager for a team in their Programme and a developer within that team.","title":"Portal Permissions"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#backend-apis","text":"","title":"Backend APIs"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#alb","text":"","title":"ALB"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#endpoint-armslengthbody","text":"Method Parameters Example Request Body Example Response GET N/A [{ \"creator\":\"user:default/johnDoe\", \"owner\":\"owner value\", \"title\":\"ALB 1\", \"alias\":\"ALB\", \"description\": \"ALB description\", \"url\": null, \"name\":\"alb-1\", \"id\": \"123\", \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} , { \"creator\":\"user:default/johnDoe\", \"owner\":\"owner value\", \"title\":\"ALB 2\", \"alias\":\"ALB\", \"description\": \"ALB description\", \"url\": null, \"name\":\"alb-2\", \"id\": \"1234\", \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"}] GET id N/A { \"creator\":\"user:default/johnDoe\", \"owner\":\"owner value\", \"title\":\"ALB 1\", \"alias\":\"ALB\", \"description\": \"ALB description\", \"url\": null, \"name\":\"alb-1\", \"id\": \"123\", \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} POST { \"title\": \"ALB\", \"description\": \"ALB Description\" } { \"title\": \"ALB\", \"description\": \"ALB Description\" , \"url\": null, \"alias\": null, \"name\": \"alb\", \"creator\":\"user:default/johnDoe\", \"owner\":\"owner value\", \"id\": \"12345\",\"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} PATCH id { \"id\": \"12345\", \"title\": \"Updated ALB Title\" } { \"title\": \"Updated ALB Title\", \"description\": \"ALB Description\" , \"url\": null, \"alias\": null, \"name\": \"alb\", \"creator\":\"user:default/johnDoe\", \"owner\":\"owner value\", \"id\": \"12345\",\"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"}","title":"Endpoint: /armsLengthBody"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#endpoint-armslengthbodynames","text":"Method Example Response GET {\"123\": \"ALB 1\",\"1234\": \"ALB 2\",\"12345\": \"ALB 3\",\"123456\": \"ALB 4\"}","title":"Endpoint: /armsLengthBodyNames"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#delivery-programmes","text":"","title":"Delivery Programmes"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#endpoint-deliveryprogramme","text":"Method Parameters Example Request Body Example Response GET N/A [{ \"id\": \"123\", \"programme_managers\":[], \"arms_length_body_id\": \"12345\", \"title\": \"Delivery Programme 1\", \"name\": \"delivery-programme-1\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" , \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"}, { \"id\": \"1234\", \"programme_managers\":[], \"arms_length_body_id\": \"12345\", \"title\": \"Delivery Programme 2\", \"name\": \"delivery-programme-2\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" , \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"}] GET id N/A { \"id\": \"123\", \"programme_managers\":[{\"aad_entity_ref_id\": \"123\", \"id\": \"1\", \"delivery_programme_id\" :\"123\", \"email\": \"email@example.com\", \"name\": \"John Doe\"}], \"arms_length_body_id\": \"12345\", \"title\": \"Delivery Programme 1\", \"name\": \"delivery-programme-1\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" , \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} POST { \"programme_managers\":[{\"aad_entity_ref_id\": \"123\"}, {\"aad_entity_ref_id\": \"1234\"}], \"arms_length_body_id\": \"12345\", \"title\": \"Delivery Programme\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" } { \"id\": \"1234\", \"programme_managers\":[], \"arms_length_body_id\": \"12345\", \"title\": \"Delivery Programme\", \"name\": \"delivery-programme\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" , \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"} PATCH { \"id\": \"1234\", \"title\": \"Updated Delivery Programme Title\" } { \"id\": \"1234\", \"programme_managers\":[], \"arms_length_body_id\": \"12345\", \"title\": \"Updated Delivery Programme Title, \"name\": \"delivery-programme\", \"alias\": \"Delivery Programme\", \"description\": \"Delivery Programme description\", \"finance_code\": \"1\", \"delivery_programme_code\": \"123\", \"url\": \"exampleurl.com\" , \"created_at\": \"2024-02-26T15:58:40.337Z\", \"updated_at\": \"2024-02-26T15:58:40.337Z\",\"updated_by\": \"user:default/johnDoe\"}","title":"Endpoint: /deliveryProgramme"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#endpoint-programmemanager","text":"Method Example Response GET [{\"id\": \"5464de88-bc76-4a0b-a491-77284c392dab\",\"delivery_programme_id\": \"0bd0cb6b-569a-4c0f-bc6d-5b8708f45c4a\",\"aad_entity_ref_id\": \"aad entity ref id 1\" \"email\": \"example@defra.onmicrosoft.com\",\"name\": \"name 1\"},{\"id\": \"f0bca259-d0a2-4d30-8166-4569f8e7b6f2\",\"delivery_programme_id\": \"0bd0cb6b-569a-4c0f-bc6d-5b8708f45c4a\",\"aad_entity_ref_id\": \"aad entity ref id 2\",\"email\": \"example@defra.onmicrosoft.com\",\"name\": \"name 2\"}]","title":"Endpoint: /programmeManager"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#endpoint-catalogentities","text":"Method Example Response GET {\"items\": [ {\"metadata\": { \"name\": \"example.onmicrosoft.com\", \"annotations\": {\"graph.microsoft.com/user-id\": \"aad entity ref id 1\",\"microsoft.com/email\": \"example@defra.onmicrosoft.com\"}},\"spec\": {\"profile\": {\"displayName\": \"name 1\"}}},{\"metadata\": {\"name\": \"example.onmicrosoft.com\",\"annotations\": {\"graph.microsoft.com/user-id\": \"aad entity ref id 2\",\"microsoft.com/email\": \"example@defra.onmicrosoft.com\"}},\"spec\": {\"profile\": {\"displayName\": \"name 2\"}}}]}","title":"Endpoint: /catalogEntities"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#delivery-projects","text":"","title":"Delivery Projects"},{"location":"Developer-Reference/adp-portal/ongoing-development/adp-data-plugin/#enpoint-deliveryproject","text":"Method Parameters Example Request Body Example Response GET N/A [{\"id\": \"123\",\"name\": \"delivery-project-1\",\"title\": \"Delivery Project 1\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\",\"finance_code\": \"\",\"delivery_programme_id\": \"1\",\"delivery_project_code\": \"1\",\"url\": \"\",\"ado_project\": \"\",\"created_at\": \"2024-04-03T06:41:56.257Z\",\"updated_at\": \"2024-04-03T08:42:48.242Z\",\"updated_by\": \"user:default/johnDoe.com\"}, {\"id\": \"1234\",\"name\": \"delivery-project-2\",\"title\": \"Delivery Project 2\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\", \"finance_code\": \"\", \"delivery_programme_id\": \"2\", \"delivery_project_code\": \"2\", \"url\": \"\", \"ado_project\": \"\", \"created_at\": \"2024-04-03T05:42:31.914Z\", \"updated_at\": \"2024-04-03T08:43:03.622Z\",\"updated_by\": \"user:default/johnDoe\"}] GET id N/A {\"id\": \"1234\",\"name\": \"delivery-project-2\",\"title\": \"Delivery Project 2\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\", \"finance_code\": \"\", \"delivery_programme_id\": \"2\", \"delivery_project_code\": \"2\", \"url\": \"\", \"ado_project\": \"\", \"created_at\": \"2024-04-03T05:42:31.914Z\", \"updated_at\": \"2024-04-03T08:43:03.622Z\",\"updated_by\": \"user:default/johnDoe\"} POST \"title\": \"Delivery Project 3\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\", \"finance_code\": \"\", \"delivery_programme_id\": \"3\", \"delivery_project_code\": \"3\", \"url\": \"\", \"ado_project\": \"\"} {\"id\": \"12345\",\"name\": \"delivery-project-3\",\"title\": \"Delivery Project 3\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\", \"finance_code\": \"\", \"delivery_programme_id\": \"3\", \"delivery_project_code\": \"3\", \"url\": \"\", \"ado_project\": \"\", \"created_at\": \"2024-04-03T05:42:31.914Z\", \"updated_at\": \"2024-04-03T08:43:03.622Z\",\"updated_by\": \"user:default/johnDoe\"} PATCH { \"id\": \"12345\", \"title\": \"Updated Delivery Project Title\" } {\"id\": \"12345\",\"name\": \"delivery-project-3\",\"title\": \"Updated Delivery Project Title\",\"alias\": \"Delivery Project\",\"description\": \"Delivery Project Description\", \"finance_code\": \"\", \"delivery_programme_id\": \"3\", \"delivery_project_code\": \"3\", \"url\": \"\", \"ado_project\": \"\", \"created_at\": \"2024-04-03T05:42:31.914Z\", \"updated_at\": \"2024-04-03T08:43:03.622Z\",\"updated_by\": \"user:default/johnDoe\"}","title":"Enpoint: /deliveryProject"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-plugin-index/","text":"Backstage Plugin Index \u00b6 We can extend Backstage's functionality by creating and installing plugins. Plugins can either be created by us (1 st party) or we can install 3 rd party plugins. The majority of 3 rd party plugins are free and open source, however there are some exceptions. This page tracks the plugins we have installed and the plugins we would like to evaluate. Considerations for 3 rd party plugins \u00b6 Does it bring value to the portal? Will it help the user complete a task more easily than if the plugin had not been installed? Will it offer the user more context about a software component? Is it actively maintained? Are bugs regularly addressed? Are the maintainers responsive to new issues, pull requests, etc. Is it compatible with the version of Backstage we're using? Backstage is updated regularly and plugins need to keep up. As we maintain and update Backstage we will need to keep any plugins we're using up to date - and the more plugins we have installed the more of a headache this will be! What are the licencing requirements? Most 3 rd party plugins are licensed under a FOSS licence, however there are a few commercial plugins out there for which we need a business case. How does it integrate with other systems? If the plugin needs to integrate with an external service through OAUTH, a service principal, etc what needs to be configured and what permissions does the plugin need? Plugins index \u00b6 Plugin Category Status Author Description azure-devops Catalog Implemented Backstage Displays Pipeline runs on component entity pages. We're not using the repos or README features. Requires components to have two annotations - dev.azure.com/project contains the ADO project name and dev.azure.com/build-definition contains the pipeline name. GitHub pull requests Catalog Implemented Roadie Adds a dashboard displaying GitHub pull requests on component entity pages. Requires components to have the github.com/project-slug in their catalog-info file. Grafana dashboard Catalog Implemented K-Phoen Displays Grafana alerts and dashboards for a component. Note that we cannot use the Dashboard embed - Managed Grafana does not allow us to configure embedding. Azure DevOps scaffolder actions Scaffolder Implemented ADP Custom scaffolder actions to get service connections, create and run pipelines, and permit access to ADO resources. Loosely based on the 3 rd party package by Parfumerie Douglas . GitHub scaffolder actions Scaffolder Implemented ADP Custom scaffolder actions to create GitHub teams and assign to repositories. Lighthouse Catalog Agreed Spotify Generates on-demand Lighthouse audits and tracks trends directly in Backstage. Helps to improve accessibility, performance and adhere to best practices. Requires PostgreSQL database and a running Lighthouse instance of the light-house-audit-service API which executes the tests before sending results back to the plugin. SonarQube Catalog Agreed SDA-SE Adds frontend visualisation of code statistics from SonarCloud or SonarQube. Requires SonarCloud subscription Prometheus Catalog Agreed Roadie Adds Embedded Prometheus Graphs and Alerts into backstage. Requires setting up a new proxy endpoint for the Prometheus API in the app-config.yaml Flux Catalog Agreed Weaveworks The Flux plugin for Backstage provides views of Flux resources available in Kubernetes clusters. Kubernetes Catalog Agreed Spotify Kubernetes in Backstage is a tool that's designed around the needs of service owners, not cluster admins. Now developers can easily check the health of their services no matter how or where those services are deployed \u2014 whether it's on a local host for testing or in production on dozens of clusters around the world. Snyk Catalog Assess Synk Snyk Backstage plugin leverages the Snyk API to enable Snyk data to be visualized directly in Backstage. KubeCost Catalog Assess SuXess-IT Kubecost is a plugin to help engineers get information about cost usage/prediction of their deployment. Some development work needed around namespaces. It doesn\u2019t look regularly maintained or updated regularly Key \u00b6 Status Description Assess Suggestions that we need to evaluate before accepting them into the backlog. Agreed Discussed and agreed to accept it, but more work needed to flesh out details. Accepted The plugin is suitable for our portal and a story for installing it as been added to the backlog. Implemented The plugin has been implemented. Rejected The plugin is unsuitable for the portal and we won't be installing it. Category Description Catalog The plugin extends the software catalog, e.g. through a card, or full page dashboard. Scaffolder The plugin adds custom actions to the component scaffolder.","title":"Backstage Plugin Index"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-plugin-index/#backstage-plugin-index","text":"We can extend Backstage's functionality by creating and installing plugins. Plugins can either be created by us (1 st party) or we can install 3 rd party plugins. The majority of 3 rd party plugins are free and open source, however there are some exceptions. This page tracks the plugins we have installed and the plugins we would like to evaluate.","title":"Backstage Plugin Index"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-plugin-index/#considerations-for-3rd-party-plugins","text":"Does it bring value to the portal? Will it help the user complete a task more easily than if the plugin had not been installed? Will it offer the user more context about a software component? Is it actively maintained? Are bugs regularly addressed? Are the maintainers responsive to new issues, pull requests, etc. Is it compatible with the version of Backstage we're using? Backstage is updated regularly and plugins need to keep up. As we maintain and update Backstage we will need to keep any plugins we're using up to date - and the more plugins we have installed the more of a headache this will be! What are the licencing requirements? Most 3 rd party plugins are licensed under a FOSS licence, however there are a few commercial plugins out there for which we need a business case. How does it integrate with other systems? If the plugin needs to integrate with an external service through OAUTH, a service principal, etc what needs to be configured and what permissions does the plugin need?","title":"Considerations for 3rd party plugins"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-plugin-index/#plugins-index","text":"Plugin Category Status Author Description azure-devops Catalog Implemented Backstage Displays Pipeline runs on component entity pages. We're not using the repos or README features. Requires components to have two annotations - dev.azure.com/project contains the ADO project name and dev.azure.com/build-definition contains the pipeline name. GitHub pull requests Catalog Implemented Roadie Adds a dashboard displaying GitHub pull requests on component entity pages. Requires components to have the github.com/project-slug in their catalog-info file. Grafana dashboard Catalog Implemented K-Phoen Displays Grafana alerts and dashboards for a component. Note that we cannot use the Dashboard embed - Managed Grafana does not allow us to configure embedding. Azure DevOps scaffolder actions Scaffolder Implemented ADP Custom scaffolder actions to get service connections, create and run pipelines, and permit access to ADO resources. Loosely based on the 3 rd party package by Parfumerie Douglas . GitHub scaffolder actions Scaffolder Implemented ADP Custom scaffolder actions to create GitHub teams and assign to repositories. Lighthouse Catalog Agreed Spotify Generates on-demand Lighthouse audits and tracks trends directly in Backstage. Helps to improve accessibility, performance and adhere to best practices. Requires PostgreSQL database and a running Lighthouse instance of the light-house-audit-service API which executes the tests before sending results back to the plugin. SonarQube Catalog Agreed SDA-SE Adds frontend visualisation of code statistics from SonarCloud or SonarQube. Requires SonarCloud subscription Prometheus Catalog Agreed Roadie Adds Embedded Prometheus Graphs and Alerts into backstage. Requires setting up a new proxy endpoint for the Prometheus API in the app-config.yaml Flux Catalog Agreed Weaveworks The Flux plugin for Backstage provides views of Flux resources available in Kubernetes clusters. Kubernetes Catalog Agreed Spotify Kubernetes in Backstage is a tool that's designed around the needs of service owners, not cluster admins. Now developers can easily check the health of their services no matter how or where those services are deployed \u2014 whether it's on a local host for testing or in production on dozens of clusters around the world. Snyk Catalog Assess Synk Snyk Backstage plugin leverages the Snyk API to enable Snyk data to be visualized directly in Backstage. KubeCost Catalog Assess SuXess-IT Kubecost is a plugin to help engineers get information about cost usage/prediction of their deployment. Some development work needed around namespaces. It doesn\u2019t look regularly maintained or updated regularly","title":"Plugins index"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-plugin-index/#key","text":"Status Description Assess Suggestions that we need to evaluate before accepting them into the backlog. Agreed Discussed and agreed to accept it, but more work needed to flesh out details. Accepted The plugin is suitable for our portal and a story for installing it as been added to the backlog. Implemented The plugin has been implemented. Rejected The plugin is unsuitable for the portal and we won't be installing it. Category Description Catalog The plugin extends the software catalog, e.g. through a card, or full page dashboard. Scaffolder The plugin adds custom actions to the component scaffolder.","title":"Key"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/","text":"Backstage Setup \u00b6 The ADP Portal is built on Backstage , an open-source platform for building developer portals. Backstage is a Node application which contains a backend API and React based front end. This page outlines the steps you need to follow to set up and run Backstage locally. [[ TOC ]] \ud83c\udfd7\ufe0f Setup \u00b6 Install pre-requisites \u00b6 Backstage has a few requirements to be able to run. These are detailed in the Backstage documentation , with some requirements detailed below. \ud83d\udc27 WSL \u00b6 Backstage requires a UNIX environment. If you're using Linux or a Mac you can skip this section, but if you're on a Windows machine you will need to install WSL . WSL can either be installed via the command line (follow Microsoft's instructions ) or from the Microsoft Store . You will then need to install a Linux distribution. Ubuntu is recommended; either download from the Microsoft Store or run wsl --install -d Ubuntu-22.04 in your terminal. Familiarise yourself with: Best practices for setting up your development environment WSL and VS Code WSL and Git Linux for Beginners if you're unfamiliar with Linux and BASH. \u26a0\ufe0f Everything you do with Backstage from this point forwards must be done in your WSL environment. Don't attempt to run Backstage from your Windows environment - it won't work! Node.js \u00b6 You will need either Node 16 or 18 to run Backstage. It will not run on Node 20. The recommended way to use the correct Node version is to use nvm . \u26a0\ufe0f If on a PC make sure you install and configure nvm in your WSL environment. You will then need to install Yarn globally. Run npm install --global yarn in your WSL environment. Git \u00b6 Make sure you've got Git configured. If on WSL follow the steps to make sure you've got Git configured correctly - your settings from Windows will not carry over. Ensure you have a GPG key set up so that you can sign your commits. See the guide on verifying Git signatures . If you have already set up a GPG key on Windows this will need to be exported and then imported in to your WSL environment. To export on Windows using Kleopatra, see here . To import using gpg on WSL, see here . \ud83d\udd28 Build tools \u00b6 If installing WSL for the first time you will likely need to install the build-essential package. Run sudo apt install build-essential . \u2601\ufe0f Azure CLI \u00b6 Check if you have the Azure CLI installed in your WSL environment. Run az --version . If this returns an error you need to install the Azure CLI: curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash . See Install the Azure CLI on Linux . We have integrated Backstage with Azure AD for authentication. For this to work you will need to sign in to the O365_DEFRADEV tenant via the Azure CLI. Project Setup \u00b6 After installing and configuring pre-requisites we can clone the adp-portal repo, configure Backstage, and run the application. \u26a0\ufe0f Remember, if on Windows these steps must be followed in your WSL environment. \ud83d\udcc3 Clone the repo \u00b6 If you haven't already, create a folder in your Home directory where you will can clone your repos. 1 ~$ mkdir projects && cd projects Clone the adp-portal repo into your projects folder. Set environment variables \u00b6 Client IDs, secrets, etc for integrations with 3 rd parties are read from environment variables. In the root of a repo there is a file named env.example.sh. Duplicate this file and rename it to env.sh. A senior developer will be able to provide you with the values for this file. A private key is also required for the GitHub app. Again, a senior developer will be able to provide you with this key. \u2139\ufe0f Later on down the line we are hoping to move these environment variables to Key Vault To load the environment variables in to your terminal session run . ./env.sh . Make sure you include both periods - the first ensures that the environment variables are loaded into the correct context. \u25b6\ufe0f Run the application \u00b6 The application needs to be run from the /app folder - run cd app if you're in the root of the project. Run the following two commands to install dependencies, and build and run the application: 1 2 yarn install yarn dev To stop the application, press <kbd> Ctrl </kbd> + <kbd> C </kbd> twice. \ud83c\udd98 Troubleshooting \u00b6 If you have issues starting Backstage, check the output in your terminal. Common errors are below: \" Backend failed to start up Error: Invalid Azure integration config for dev.azure.com: credential at position 1 is not a valid credential \" - Have you loaded your environment variables? Run . ./env.sh from the root of the repo, then try running the application again. \"MicrosoftGraphOrgEntityProvider:default refresh failed, AggregateAuthenticationError: ChainedTokenCredential authentication failed\" - have you logged in to the Azure CLI? Run az login and make sure you sign in to the O365_DEFRADEV tenant. Try running the application again.","title":"Backstage Setup"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#backstage-setup","text":"The ADP Portal is built on Backstage , an open-source platform for building developer portals. Backstage is a Node application which contains a backend API and React based front end. This page outlines the steps you need to follow to set up and run Backstage locally. [[ TOC ]]","title":"Backstage Setup"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#setup","text":"","title":"\ud83c\udfd7\ufe0f Setup"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#install-pre-requisites","text":"Backstage has a few requirements to be able to run. These are detailed in the Backstage documentation , with some requirements detailed below.","title":"Install pre-requisites"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#wsl","text":"Backstage requires a UNIX environment. If you're using Linux or a Mac you can skip this section, but if you're on a Windows machine you will need to install WSL . WSL can either be installed via the command line (follow Microsoft's instructions ) or from the Microsoft Store . You will then need to install a Linux distribution. Ubuntu is recommended; either download from the Microsoft Store or run wsl --install -d Ubuntu-22.04 in your terminal. Familiarise yourself with: Best practices for setting up your development environment WSL and VS Code WSL and Git Linux for Beginners if you're unfamiliar with Linux and BASH. \u26a0\ufe0f Everything you do with Backstage from this point forwards must be done in your WSL environment. Don't attempt to run Backstage from your Windows environment - it won't work!","title":"\ud83d\udc27 WSL"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#nodejs","text":"You will need either Node 16 or 18 to run Backstage. It will not run on Node 20. The recommended way to use the correct Node version is to use nvm . \u26a0\ufe0f If on a PC make sure you install and configure nvm in your WSL environment. You will then need to install Yarn globally. Run npm install --global yarn in your WSL environment.","title":"Node.js"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#git","text":"Make sure you've got Git configured. If on WSL follow the steps to make sure you've got Git configured correctly - your settings from Windows will not carry over. Ensure you have a GPG key set up so that you can sign your commits. See the guide on verifying Git signatures . If you have already set up a GPG key on Windows this will need to be exported and then imported in to your WSL environment. To export on Windows using Kleopatra, see here . To import using gpg on WSL, see here .","title":"Git"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#build-tools","text":"If installing WSL for the first time you will likely need to install the build-essential package. Run sudo apt install build-essential .","title":"\ud83d\udd28 Build tools"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#azure-cli","text":"Check if you have the Azure CLI installed in your WSL environment. Run az --version . If this returns an error you need to install the Azure CLI: curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash . See Install the Azure CLI on Linux . We have integrated Backstage with Azure AD for authentication. For this to work you will need to sign in to the O365_DEFRADEV tenant via the Azure CLI.","title":"\u2601\ufe0f Azure CLI"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#project-setup","text":"After installing and configuring pre-requisites we can clone the adp-portal repo, configure Backstage, and run the application. \u26a0\ufe0f Remember, if on Windows these steps must be followed in your WSL environment.","title":"Project Setup"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#clone-the-repo","text":"If you haven't already, create a folder in your Home directory where you will can clone your repos. 1 ~$ mkdir projects && cd projects Clone the adp-portal repo into your projects folder.","title":"\ud83d\udcc3 Clone the repo"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#set-environment-variables","text":"Client IDs, secrets, etc for integrations with 3 rd parties are read from environment variables. In the root of a repo there is a file named env.example.sh. Duplicate this file and rename it to env.sh. A senior developer will be able to provide you with the values for this file. A private key is also required for the GitHub app. Again, a senior developer will be able to provide you with this key. \u2139\ufe0f Later on down the line we are hoping to move these environment variables to Key Vault To load the environment variables in to your terminal session run . ./env.sh . Make sure you include both periods - the first ensures that the environment variables are loaded into the correct context.","title":"Set environment variables"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#run-the-application","text":"The application needs to be run from the /app folder - run cd app if you're in the root of the project. Run the following two commands to install dependencies, and build and run the application: 1 2 yarn install yarn dev To stop the application, press <kbd> Ctrl </kbd> + <kbd> C </kbd> twice.","title":"\u25b6\ufe0f Run the application"},{"location":"Developer-Reference/adp-portal/ongoing-development/backstage-setup/#troubleshooting","text":"If you have issues starting Backstage, check the output in your terminal. Common errors are below: \" Backend failed to start up Error: Invalid Azure integration config for dev.azure.com: credential at position 1 is not a valid credential \" - Have you loaded your environment variables? Run . ./env.sh from the root of the repo, then try running the application again. \"MicrosoftGraphOrgEntityProvider:default refresh failed, AggregateAuthenticationError: ChainedTokenCredential authentication failed\" - have you logged in to the Azure CLI? Run az login and make sure you sign in to the O365_DEFRADEV tenant. Try running the application again.","title":"\ud83c\udd98 Troubleshooting"},{"location":"Developer-Reference/adp-portal/ongoing-development/catalog-data-sources/","text":"Catalog Data Sources \u00b6 Overview \u00b6 Catalog data is pulled in from multiple sources which are configured in the app-config.yaml file. Individual entities are defined by a YAML file. Catalog Sources \u00b6 Components \u00b6 Backstage regularly scans the DEFRA GitHub organisation for repos containing a catalog-info.yaml file in the root of the master branch. The FFC demo services contain examples of this file (see ffc-demo-web ). New components scaffolded through Backstage will be contain this file (but it may need further customisation), existing components will need to have the file added in manually. A catalog-info.yaml for a component file might look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : backstage.io/v1alpha1 kind : Component metadata : name : \"ffc-demo-web\" description : | Digital service mock to claim public money in the event property subsides into mine shaft. This is the web front end for the application. It contains a simple claim submission journey where user input data is cached in Redis. On submission the data is pulled from Redis and passed to the message service. annotations : github.com/project-slug : DEFRA/ffc-demo-web dev.azure.com/project : DEFRA-FFC dev.azure.com/build-definition : DEFRA.ffc-demo-web sonarqube.org/project-key : adp-ffc-demo-web tags : - node - service-bus - redis - external - front-end spec : type : frontend lifecycle : beta owner : \"group:default/fcp-demo\" system : fcp-demo-service dependsOn : - \"resource:default/fcp-demo-claim-queue\" - \"resource:default/ADPINFSB01\" The Backstage documentation describes the format of this file - it is similar to a Kubernetes object config file. The key properties we need to set are: metadata.name - The name of the component. Must be unique, and should match the repository name. metadata.annotations - Annotations are used by integrations with 3 rd party systems. In the example above, github.com/project-slug is used to pull data from the specified project into the Pull Requests dashboard; the dev.azure.com annotations pull pipeline runs into the CI/CD dashboard; sonarqube.org/project-key pulls in Sonarcloud metrics for the specified project. spec.type - The type of component. In ADP we currently have two types - frontend (for a web application) and backend (for an API or backend service). spec.lifecycle - The state of the component. In ADP we have aligned the lifecycle with GDS project phases - discovery, alpha, beta, live, retirement. spec.owner - the group/team that owns the component. Groups are defined under shared entities below. spec.system - a reference to the system that the component belongs to. Systems are defined under shared entities below. spec.dependsOn - dependencies on other components and resources, e.g. if a service publishes to a message queue then a reference to that queue would be defined here. If a component consumes infrastructure such as a database or service bus queue then that must also be defined alongside the component. Multiple entities can be defined in a single file by using a triple dash --- to separate them. Shared entities \u00b6 Users \u00b6 Related Links \u00b6 Backstage System Model Backstage YAML format","title":"Catalog Data Sources"},{"location":"Developer-Reference/adp-portal/ongoing-development/catalog-data-sources/#catalog-data-sources","text":"","title":"Catalog Data Sources"},{"location":"Developer-Reference/adp-portal/ongoing-development/catalog-data-sources/#overview","text":"Catalog data is pulled in from multiple sources which are configured in the app-config.yaml file. Individual entities are defined by a YAML file.","title":"Overview"},{"location":"Developer-Reference/adp-portal/ongoing-development/catalog-data-sources/#catalog-sources","text":"","title":"Catalog Sources"},{"location":"Developer-Reference/adp-portal/ongoing-development/catalog-data-sources/#components","text":"Backstage regularly scans the DEFRA GitHub organisation for repos containing a catalog-info.yaml file in the root of the master branch. The FFC demo services contain examples of this file (see ffc-demo-web ). New components scaffolded through Backstage will be contain this file (but it may need further customisation), existing components will need to have the file added in manually. A catalog-info.yaml for a component file might look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : backstage.io/v1alpha1 kind : Component metadata : name : \"ffc-demo-web\" description : | Digital service mock to claim public money in the event property subsides into mine shaft. This is the web front end for the application. It contains a simple claim submission journey where user input data is cached in Redis. On submission the data is pulled from Redis and passed to the message service. annotations : github.com/project-slug : DEFRA/ffc-demo-web dev.azure.com/project : DEFRA-FFC dev.azure.com/build-definition : DEFRA.ffc-demo-web sonarqube.org/project-key : adp-ffc-demo-web tags : - node - service-bus - redis - external - front-end spec : type : frontend lifecycle : beta owner : \"group:default/fcp-demo\" system : fcp-demo-service dependsOn : - \"resource:default/fcp-demo-claim-queue\" - \"resource:default/ADPINFSB01\" The Backstage documentation describes the format of this file - it is similar to a Kubernetes object config file. The key properties we need to set are: metadata.name - The name of the component. Must be unique, and should match the repository name. metadata.annotations - Annotations are used by integrations with 3 rd party systems. In the example above, github.com/project-slug is used to pull data from the specified project into the Pull Requests dashboard; the dev.azure.com annotations pull pipeline runs into the CI/CD dashboard; sonarqube.org/project-key pulls in Sonarcloud metrics for the specified project. spec.type - The type of component. In ADP we currently have two types - frontend (for a web application) and backend (for an API or backend service). spec.lifecycle - The state of the component. In ADP we have aligned the lifecycle with GDS project phases - discovery, alpha, beta, live, retirement. spec.owner - the group/team that owns the component. Groups are defined under shared entities below. spec.system - a reference to the system that the component belongs to. Systems are defined under shared entities below. spec.dependsOn - dependencies on other components and resources, e.g. if a service publishes to a message queue then a reference to that queue would be defined here. If a component consumes infrastructure such as a database or service bus queue then that must also be defined alongside the component. Multiple entities can be defined in a single file by using a triple dash --- to separate them.","title":"Components"},{"location":"Developer-Reference/adp-portal/ongoing-development/catalog-data-sources/#shared-entities","text":"","title":"Shared entities"},{"location":"Developer-Reference/adp-portal/ongoing-development/catalog-data-sources/#users","text":"","title":"Users"},{"location":"Developer-Reference/adp-portal/ongoing-development/catalog-data-sources/#related-links","text":"Backstage System Model Backstage YAML format","title":"Related Links"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/","text":"GitHub App Permissions \u00b6 The minimum permissions that we requires for our ADP GitHub App are: For each repository in Defra GitHub, we require read access to metadata For each repository in Defra GitHub, we require read and write access to administration, code, and pull requests Please note: Any other permission assume that is set to \"no access\" These set of permissions are required for our short to medium term ADP goal and objectives and we will most likely need to expand the scope as time goes on. Repository Permissions \u00b6 Repository permissions permit access to repositories and related resources. Repository Administration \u00b6 Repository creation, deletion, settings, teams, and collaborators. Why we need this permission \u00b6 When Scaffolding new templated services within Backstage/ ADO, we need to creating Repos and setting default permissions. Repository Code (Contents) \u00b6 Repository contents, commits, branches, downloads, releases, and merges. Why we need this permission \u00b6 When Scaffolding new templated services within Backstage/ ADO, we need to add code to the repos. Create code, commit, and branch in adp-flux-core to allow us to automate flux. Potential Risks \u00b6 Repository Metadata (mandatory) \u00b6 Search repositories, list collaborators and access repository metadata. Why we need this permission \u00b6 Mandatory when creating an GitHub application. Potential Risks \u00b6 Pull requests \u00b6 Pull requests and related comments, assignees, labels, milestones and merges. Why we need this permission \u00b6 Within Backstage we need to be able to view pull request of matching services. Create pull requests in adp-flux-core to allow us to automate flux. Potential Risks \u00b6 Organisation permissions \u00b6 Organisation permissions permit access to organisation related resources. None required at this time Account permissions \u00b6 These permissions are granted on an individual user basis as part of the User authorization flow. None required at this time Potential Issues \u00b6 Risky API Permissions \u00b6 GitHub Apps can request almost any permission from the list of API actions supported by GitHub Apps. Possible Remediations: Review GitHub App permissions, selecting only permissions that are required in the short to medium term. Testing the GitHub App in sandbox organisation. Understanding context of backstage plugins or code implemented. Limit GitHub App permissions to specific repositories that are used by the ADP platform. This will have the side effect of decreasing the effectiveness of the platform for the onboarding of view services. Adding a manual step adding in repos (assumption). We would like to keep it the same scope as CDP which is to have access to all repositories in the Defra Org. Comprised App credentials \u00b6 Leaking or misplaced GitHub App credentials. Possible Remediation Following best practices ensuring credentials are secure and stored in a key vault in each of are staging and production ADP Portals ( documentation ) Following best practices ensuring credentials are secure when developing locally. Ensuring that the credentials are not checked in and stored out of source control a local env.sh file. Raised Concerns \u00b6 As far as I can tell, your Backstage installation will have a client secret to use the app identity to perform administrative functions? \u00b6 Answer: Yes, please see permission break down above with the reasons why we require them. So presumably we could only ever have the one instance of Backstage? \u00b6 \u201cIt's not possible to have multiple Backstage GitHub Apps installed in the same GitHub organisation, to be handled by Backstage. We currently don't check through all the registered GitHub Apps to see which ones are installed for a particular repository. We only respect global organisation installs right now.\u201d Answer: Should be be able to have multiple instances of backstage within the same GitHub organisation. There may be possible conflicts that may occur with certain backstage plugins. For example, GitHub Discovery search for a catalog-info.yaml are repositories to allow for automatic registering of entities. If backstage 1 and backstage 2 use the defaults GitHub Discovery provide configuration will be pick up the same files as each other. To resolve this it will would be as simple as changing config to find yaml(s) file of different names or paths in backstage 1 or 2. There is a second option which would be to restrict what repositories the GitHub Application has access to. To aid in remediating this concern, will be change the config where we can to add an \"adp\" suffix. For example, \"adp-catalog-info.yaml\". My other question is why there\u2019s an installed web hook \u2013 I don\u2019t really know what this is doing? \u00b6 We are not using the web hook at the moment but we may look to support GitHub events in future ( Documentation ). Key References \u00b6 GitHub App Documentation GitHub Apps Best Practices Github OAuth Apps Security: How to protect yourself against GitHub/OAuth Apps Supply Chain Attacks","title":"GitHub App Permissions"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#github-app-permissions","text":"The minimum permissions that we requires for our ADP GitHub App are: For each repository in Defra GitHub, we require read access to metadata For each repository in Defra GitHub, we require read and write access to administration, code, and pull requests Please note: Any other permission assume that is set to \"no access\" These set of permissions are required for our short to medium term ADP goal and objectives and we will most likely need to expand the scope as time goes on.","title":"GitHub App Permissions"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#repository-permissions","text":"Repository permissions permit access to repositories and related resources.","title":"Repository Permissions"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#repository-administration","text":"Repository creation, deletion, settings, teams, and collaborators.","title":"Repository Administration"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#why-we-need-this-permission","text":"When Scaffolding new templated services within Backstage/ ADO, we need to creating Repos and setting default permissions.","title":"Why we need this permission"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#repository-code-contents","text":"Repository contents, commits, branches, downloads, releases, and merges.","title":"Repository Code (Contents)"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#why-we-need-this-permission_1","text":"When Scaffolding new templated services within Backstage/ ADO, we need to add code to the repos. Create code, commit, and branch in adp-flux-core to allow us to automate flux.","title":"Why we need this permission"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#potential-risks","text":"","title":"Potential Risks"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#repository-metadata-mandatory","text":"Search repositories, list collaborators and access repository metadata.","title":"Repository Metadata  (mandatory)"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#why-we-need-this-permission_2","text":"Mandatory when creating an GitHub application.","title":"Why we need this permission"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#potential-risks_1","text":"","title":"Potential Risks"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#pull-requests","text":"Pull requests and related comments, assignees, labels, milestones and merges.","title":"Pull requests"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#why-we-need-this-permission_3","text":"Within Backstage we need to be able to view pull request of matching services. Create pull requests in adp-flux-core to allow us to automate flux.","title":"Why we need this permission"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#potential-risks_2","text":"","title":"Potential Risks"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#organisation-permissions","text":"Organisation permissions permit access to organisation related resources. None required at this time","title":"Organisation permissions"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#account-permissions","text":"These permissions are granted on an individual user basis as part of the User authorization flow. None required at this time","title":"Account permissions"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#potential-issues","text":"","title":"Potential Issues"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#risky-api-permissions","text":"GitHub Apps can request almost any permission from the list of API actions supported by GitHub Apps. Possible Remediations: Review GitHub App permissions, selecting only permissions that are required in the short to medium term. Testing the GitHub App in sandbox organisation. Understanding context of backstage plugins or code implemented. Limit GitHub App permissions to specific repositories that are used by the ADP platform. This will have the side effect of decreasing the effectiveness of the platform for the onboarding of view services. Adding a manual step adding in repos (assumption). We would like to keep it the same scope as CDP which is to have access to all repositories in the Defra Org.","title":"Risky API Permissions"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#comprised-app-credentials","text":"Leaking or misplaced GitHub App credentials. Possible Remediation Following best practices ensuring credentials are secure and stored in a key vault in each of are staging and production ADP Portals ( documentation ) Following best practices ensuring credentials are secure when developing locally. Ensuring that the credentials are not checked in and stored out of source control a local env.sh file.","title":"Comprised App credentials"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#raised-concerns","text":"","title":"Raised Concerns"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#as-far-as-i-can-tell-your-backstage-installation-will-have-a-client-secret-to-use-the-app-identity-to-perform-administrative-functions","text":"Answer: Yes, please see permission break down above with the reasons why we require them.","title":"As far as I can tell, your Backstage installation will have a client secret to use the app identity to perform administrative functions?"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#so-presumably-we-could-only-ever-have-the-one-instance-of-backstage","text":"\u201cIt's not possible to have multiple Backstage GitHub Apps installed in the same GitHub organisation, to be handled by Backstage. We currently don't check through all the registered GitHub Apps to see which ones are installed for a particular repository. We only respect global organisation installs right now.\u201d Answer: Should be be able to have multiple instances of backstage within the same GitHub organisation. There may be possible conflicts that may occur with certain backstage plugins. For example, GitHub Discovery search for a catalog-info.yaml are repositories to allow for automatic registering of entities. If backstage 1 and backstage 2 use the defaults GitHub Discovery provide configuration will be pick up the same files as each other. To resolve this it will would be as simple as changing config to find yaml(s) file of different names or paths in backstage 1 or 2. There is a second option which would be to restrict what repositories the GitHub Application has access to. To aid in remediating this concern, will be change the config where we can to add an \"adp\" suffix. For example, \"adp-catalog-info.yaml\".","title":"So presumably we could only ever have the one instance of Backstage?"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#my-other-question-is-why-theres-an-installed-web-hook-i-dont-really-know-what-this-is-doing","text":"We are not using the web hook at the moment but we may look to support GitHub events in future ( Documentation ).","title":"My other question is why there\u2019s an installed web hook \u2013 I don\u2019t really know what this is doing?"},{"location":"Developer-Reference/adp-portal/ongoing-development/github-app-permissions/#key-references","text":"GitHub App Documentation GitHub Apps Best Practices Github OAuth Apps Security: How to protect yourself against GitHub/OAuth Apps Supply Chain Attacks","title":"Key References"},{"location":"Developer-Reference/adp-portal/ongoing-development/govuk-branding/","text":"GovUK Branding for the ADP Portal \u00b6 The ADP portal is built on Backstage, using React and Typescript on the frontend. This page outlines, the steps taken to incorporate the GOV.UK branding into the ADP Portal. Key References: \u00b6 Backstage Theme Customization GOV.UK Frontend GOV.UK Styles Customization of Backstage \u00b6 Backstage allows the customization of the themes to a certain extent, for example the font family, colours, logos and icons can be changed following the tutorials. All of the theme changes have been made within the App.tsx file. GOV.UK Branding \u00b6 \ud83c\udfd7\ufe0f Setup \u00b6 In order to install GOV.UK Frontend you need to meet the some requirements: Have Node.js installed on your local environment Install Dart Sass Once those are successfully installed you can run the following in your terminal within the adp-portal/app/packages/app folder: yarn install govuk-frontend --save In order to import the GOV.UK styles, two lines of code have been added within the style.module.scss file: $govuk-assets-path: \"~govuk-frontend/govuk/assets/\"; // this creates a path to the fonts and images of GOV.UK assets. @import \"~govuk-frontend/govuk/all\"; // this imports all of the styles which enables the use of colours and typography. \ud83c\udfa8 Colour Scheme \u00b6 The colour scheme is applied through exporting the GOV.UK colours as variables within the style.module.scss file into the Backstage Themes created. Currently there are a few colours that are being used however more variables can be added within the scss file and can be imported within other files. To import the scss file with the styles variables this statement is used in the App.tsx file: import styles from 'style-loader!css-loader?{\"modules\": {\"auto\": true}}!sass-loader?{\"sassOptions\": {\"quietDeps\": true}}!./style.module.scss'; This import statement enables the scss file to load and process. The style variables then were used within the custom Backstage themes: 1 2 3 4 5 6 const lightTheme = createUnifiedTheme ({ ... primary : { main : styles . primaryColour , }, }); \ud83d\udddb Typography \u00b6 The font used within the ADP Portal is GDS Transport as the portal will be on the gov.uk domain. To get this working within the style.module.scss file the fonts were imported through assigning it to a scss variable called govuk-assets-path-font-woff2 and govuk-assets-path-font-woff : 1 2 $govuk-assets-path-font-woff2 : \"~govuk-frontend/govuk/assets/fonts/light-94a07e06a1-v2.woff2\" ; $govuk-assets-path-font-woff : \"~govuk-frontend/govuk/assets/fonts/light-f591b13f7d-v2.woff\" ; As recommended we are serving the assets from the GOV.UK assets folder so that the style stays up to date when there is an update to the GOV.UK frontend. Then this variable was parsed into the url of the font-face element: 1 2 3 4 5 6 @font-face { font-family : \"GDS Transport\" ; src : url ( ' #{ $govuk-assets-path-font-woff2 } ' ) format ( \"woff2\" ), url ( ' #{ $govuk-assets-path-font-woff } ' ) format ( \"woff\" ) } To customize the font of the backstage theme, the scss was imported (check the colour scheme section) and used within the fontFamily element of the createUnifiedTheme function: 1 2 3 4 const lightTheme = createUnifiedTheme ({ ... fontFamily : \"'GDS Transport',arial, sans-serif\" }); \ud83d\uddbc\ufe0f Logo \u00b6 The Logo of the ADP Portal was changed by updating the two files within the src/components/Roots folder. LogoFull.tsx - A larger logo used when the Sidebar navigation is opened. LogoIcon.tsx - A smaller logo used when the sidebar navigation is closed. Both DEFRA logos are imported as png and saved within the Root folder.","title":"Gov.uk Branding"},{"location":"Developer-Reference/adp-portal/ongoing-development/govuk-branding/#govuk-branding-for-the-adp-portal","text":"The ADP portal is built on Backstage, using React and Typescript on the frontend. This page outlines, the steps taken to incorporate the GOV.UK branding into the ADP Portal.","title":"GovUK Branding for the ADP Portal"},{"location":"Developer-Reference/adp-portal/ongoing-development/govuk-branding/#key-references","text":"Backstage Theme Customization GOV.UK Frontend GOV.UK Styles","title":"Key References:"},{"location":"Developer-Reference/adp-portal/ongoing-development/govuk-branding/#customization-of-backstage","text":"Backstage allows the customization of the themes to a certain extent, for example the font family, colours, logos and icons can be changed following the tutorials. All of the theme changes have been made within the App.tsx file.","title":"Customization of Backstage"},{"location":"Developer-Reference/adp-portal/ongoing-development/govuk-branding/#govuk-branding","text":"","title":"GOV.UK Branding"},{"location":"Developer-Reference/adp-portal/ongoing-development/govuk-branding/#setup","text":"In order to install GOV.UK Frontend you need to meet the some requirements: Have Node.js installed on your local environment Install Dart Sass Once those are successfully installed you can run the following in your terminal within the adp-portal/app/packages/app folder: yarn install govuk-frontend --save In order to import the GOV.UK styles, two lines of code have been added within the style.module.scss file: $govuk-assets-path: \"~govuk-frontend/govuk/assets/\"; // this creates a path to the fonts and images of GOV.UK assets. @import \"~govuk-frontend/govuk/all\"; // this imports all of the styles which enables the use of colours and typography.","title":"\ud83c\udfd7\ufe0f Setup"},{"location":"Developer-Reference/adp-portal/ongoing-development/govuk-branding/#colour-scheme","text":"The colour scheme is applied through exporting the GOV.UK colours as variables within the style.module.scss file into the Backstage Themes created. Currently there are a few colours that are being used however more variables can be added within the scss file and can be imported within other files. To import the scss file with the styles variables this statement is used in the App.tsx file: import styles from 'style-loader!css-loader?{\"modules\": {\"auto\": true}}!sass-loader?{\"sassOptions\": {\"quietDeps\": true}}!./style.module.scss'; This import statement enables the scss file to load and process. The style variables then were used within the custom Backstage themes: 1 2 3 4 5 6 const lightTheme = createUnifiedTheme ({ ... primary : { main : styles . primaryColour , }, });","title":"\ud83c\udfa8 Colour Scheme"},{"location":"Developer-Reference/adp-portal/ongoing-development/govuk-branding/#typography","text":"The font used within the ADP Portal is GDS Transport as the portal will be on the gov.uk domain. To get this working within the style.module.scss file the fonts were imported through assigning it to a scss variable called govuk-assets-path-font-woff2 and govuk-assets-path-font-woff : 1 2 $govuk-assets-path-font-woff2 : \"~govuk-frontend/govuk/assets/fonts/light-94a07e06a1-v2.woff2\" ; $govuk-assets-path-font-woff : \"~govuk-frontend/govuk/assets/fonts/light-f591b13f7d-v2.woff\" ; As recommended we are serving the assets from the GOV.UK assets folder so that the style stays up to date when there is an update to the GOV.UK frontend. Then this variable was parsed into the url of the font-face element: 1 2 3 4 5 6 @font-face { font-family : \"GDS Transport\" ; src : url ( ' #{ $govuk-assets-path-font-woff2 } ' ) format ( \"woff2\" ), url ( ' #{ $govuk-assets-path-font-woff } ' ) format ( \"woff\" ) } To customize the font of the backstage theme, the scss was imported (check the colour scheme section) and used within the fontFamily element of the createUnifiedTheme function: 1 2 3 4 const lightTheme = createUnifiedTheme ({ ... fontFamily : \"'GDS Transport',arial, sans-serif\" });","title":"\ud83d\udddb Typography"},{"location":"Developer-Reference/adp-portal/ongoing-development/govuk-branding/#logo","text":"The Logo of the ADP Portal was changed by updating the two files within the src/components/Roots folder. LogoFull.tsx - A larger logo used when the Sidebar navigation is opened. LogoIcon.tsx - A smaller logo used when the sidebar navigation is closed. Both DEFRA logos are imported as png and saved within the Root folder.","title":"\ud83d\uddbc\ufe0f Logo"},{"location":"Developer-Reference/github/pull_request_template/","text":"Pull Request Details \u00b6 What this PR does / why we need it: \u00b6 A brief description of changes being made Link to the relevant work items: e.g: Relates to ADO Work Item AB#213700 and builds on #3376 (link to ADO Build ID URL) Special notes for your reviewer \u00b6 Any specific actions or notes on review? Testing \u00b6 Any relevant testing information and pipeline runs. Checklist (please delete before completing or setting auto-complete) \u00b6 Story Work items associated (not Tasks) Successful testing run(s) link provided Title pattern should be {work item number}: {title} Description covers all the changes in the PR This PR contains documentation This PR contains tests How does this PR make you feel: \u00b6","title":"Pull request template"},{"location":"Developer-Reference/github/pull_request_template/#pull-request-details","text":"","title":"Pull Request Details"},{"location":"Developer-Reference/github/pull_request_template/#what-this-pr-does-why-we-need-it","text":"A brief description of changes being made Link to the relevant work items: e.g: Relates to ADO Work Item AB#213700 and builds on #3376 (link to ADO Build ID URL)","title":"What this PR does / why we need it:"},{"location":"Developer-Reference/github/pull_request_template/#special-notes-for-your-reviewer","text":"Any specific actions or notes on review?","title":"Special notes for your reviewer"},{"location":"Developer-Reference/github/pull_request_template/#testing","text":"Any relevant testing information and pipeline runs.","title":"Testing"},{"location":"Developer-Reference/github/pull_request_template/#checklist-please-delete-before-completing-or-setting-auto-complete","text":"Story Work items associated (not Tasks) Successful testing run(s) link provided Title pattern should be {work item number}: {title} Description covers all the changes in the PR This PR contains documentation This PR contains tests","title":"Checklist (please delete before completing or setting auto-complete)"},{"location":"Developer-Reference/github/pull_request_template/#how-does-this-pr-make-you-feel","text":"","title":"How does this PR make you feel:"},{"location":"Developer-Reference/github/verify-gitHub-commit-signatures/","text":"Verify GitHub commit signatures \u00b6 The project's branch policy is configured to necessitate the use of Git signed commits for any merging activities. This policy serves a twofold purpose: firstly, it validates the authenticity of changes and acts as a barrier against unauthorised or malevolent alterations to the codebase. Secondly, it provides assurance of code integrity by demonstrating that changes have remained unaltered throughout transit and subsequent commits. During the evaluation of pull requests or merge requests, the presence of signed commits also offers a reliable means to confirm that the proposed changes have been authored by authorised contributors, thereby reducing the likelihood of unintentionally accepting unauthorised code. To use signed commits, developers must generate a GPG (GNU Privacy Guard) key pair, which includes a private key kept secret and a public key that is shared. Commits are then signed using the private key, and others can verify the commits using the corresponding public key. Generate New GPG Key \u00b6 Please refer the following link. Please make sure the email you enter in step 8 is your github email account https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key Adding a GPG Key on Github \u00b6 In the upper-right corner, click your profile photo, then click Settings. In the \"Access\" section of the sidebar, click SSH and GPG keys . Next to the \"GPG keys\" header, click New GPG key. In the \"Title\" field, type a name for your GPG key. In the \"Key\" field, paste the GPG key you copied when you generated your GPG key. Click Add GPG key . To confirm the action, authenticate to your GitHub account. Further support and information \u00b6 https://docs.github.com/en/authentication/managing-commit-signature-verification Signing commits using WSL \u00b6 Generate new gpg key Adding a pgp key to your github account telling git about your signing key Still having problems committing try this? Signing old commits","title":"Verify GitHub Comment Signature"},{"location":"Developer-Reference/github/verify-gitHub-commit-signatures/#verify-github-commit-signatures","text":"The project's branch policy is configured to necessitate the use of Git signed commits for any merging activities. This policy serves a twofold purpose: firstly, it validates the authenticity of changes and acts as a barrier against unauthorised or malevolent alterations to the codebase. Secondly, it provides assurance of code integrity by demonstrating that changes have remained unaltered throughout transit and subsequent commits. During the evaluation of pull requests or merge requests, the presence of signed commits also offers a reliable means to confirm that the proposed changes have been authored by authorised contributors, thereby reducing the likelihood of unintentionally accepting unauthorised code. To use signed commits, developers must generate a GPG (GNU Privacy Guard) key pair, which includes a private key kept secret and a public key that is shared. Commits are then signed using the private key, and others can verify the commits using the corresponding public key.","title":"Verify GitHub commit signatures"},{"location":"Developer-Reference/github/verify-gitHub-commit-signatures/#generate-new-gpg-key","text":"Please refer the following link. Please make sure the email you enter in step 8 is your github email account https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key","title":"Generate New GPG Key"},{"location":"Developer-Reference/github/verify-gitHub-commit-signatures/#adding-a-gpg-key-on-github","text":"In the upper-right corner, click your profile photo, then click Settings. In the \"Access\" section of the sidebar, click SSH and GPG keys . Next to the \"GPG keys\" header, click New GPG key. In the \"Title\" field, type a name for your GPG key. In the \"Key\" field, paste the GPG key you copied when you generated your GPG key. Click Add GPG key . To confirm the action, authenticate to your GitHub account.","title":"Adding a GPG Key on Github"},{"location":"Developer-Reference/github/verify-gitHub-commit-signatures/#further-support-and-information","text":"https://docs.github.com/en/authentication/managing-commit-signature-verification","title":"Further support and information"},{"location":"Developer-Reference/github/verify-gitHub-commit-signatures/#signing-commits-using-wsl","text":"Generate new gpg key Adding a pgp key to your github account telling git about your signing key Still having problems committing try this? Signing old commits","title":"Signing commits using WSL"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/","text":"FCP Demo Services \u00b6 This documentation is to capture the existing design of demo/exemplar services in FFC platform. This will provide an overview of the components in the demo services and application flow. The demo service contains 6 containerized microservices orchestrated with Kubernetes. The purpose of these services are to prove the platform capability to provision the infrastructure required for developing a digital service along with CI/CD pipelines with minimal effort. This in turn allows the developers to focus on the core business logic. Language of choice \u00b6 Node.Js ASP.NET Core Tools & External Dependencies \u00b6 Azure Service Bus for messaging PgSql Existing Demo Services \u00b6 Below are the demo services that are present at the moment. Service Dev Platform Git Repo Payments Service Node.Js https://github.com/DEFRA/ffc-demo-payment-service Payments Service Core Asp.Net Core https://github.com/DEFRA/ffc-demo-payment-service-core Payments Web Node.Js https://github.com/DEFRA/ffc-demo-payment-web Claim Service Node.Js https://github.com/DEFRA/ffc-demo-claim-service Calculation Service Node.Js https://github.com/DEFRA/ffc-demo-calculation-service Collector Service Node.Js https://github.com/DEFRA/ffc-demo-collector Demo Web Node.Js https://github.com/DEFRA/ffc-demo-web Business Context \u00b6 Microservice Architecture \u00b6 Other Services \u00b6 Demo Apply Service Generated from Claims service. Needs further clarification from dev team. Demo Apply Web Generated from Demo web app. Needs further clarification from dev team. Testing \u00b6 Code Docker Compose Dev Test Pre-production Lint/Audit X Synk Test X Static Code Analysis/ SonarCloud X Functionional/ BDD X X Intergration Tests/ Contract testing using pact broker X X Performance Testing (JMeter) X Pen Testing (OWASP ZAP) X X Code Lint/Audit Synk Test Static Code Analysis/Sonar Cloud Docker Compose Functional/BDD Pen Testing Dev Functional/BDD Integrations Test/Contract testing using Pact Broker Test Integrations Test/Contract testing using Pact Broker Pre-production Performance testing (Jmeter) Pen Testing Challenges \u00b6 Docker compose is good if the application is full contained, but it has dependencies, which are unknown at present. Could remove Docker compose tests, run in SND2 or SND3 , then install dependencies and helm chart which update flux Considerations for discussion \u00b6 DAPR - with RabbitMQ for containerized testing Open Policy Agent (OPR) DAPR \u00b6 Distributed Application Runtime simplifies the authoring of distributed, microservice- based applications. Once DAPR is enabled for a container app, a secondary process is created alongside the application that enables communication with DAPR via HTTP or gRPC Open Policy Agent \u00b6 Azure Policy extends Gatekeeper v3, an admission controller webhook for Open Policy Agent (OPA), to apply at-scale enforcements and safeguards on your clusters in a centralized, consistent manner. Azure Policy makes it possible to manage and report on the compliance state of your Kubernetes clusters from one place. The add-on enacts the following functions: Checks with Azure Policy service for policy assignments to the cluster. Deploys policy definitions into the cluster as constraint template and constraint custom resources. Reports auditing and compliance details back to Azure Policy service. Azure Policy for Kubernetes supports the following cluster environments: Azure Kubernetes Service (AKS) Azure Arc enabled Kubernetes Further reading \u00b6 https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes https://learn.microsoft.com/en-us/azure/container-apps/dapr-overview?tabs=bicep1%2Cyaml Service Details \u00b6 TODO This page is a work in progress and will be updated in due course. Add to details about each service.","title":"Overview"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#fcp-demo-services","text":"This documentation is to capture the existing design of demo/exemplar services in FFC platform. This will provide an overview of the components in the demo services and application flow. The demo service contains 6 containerized microservices orchestrated with Kubernetes. The purpose of these services are to prove the platform capability to provision the infrastructure required for developing a digital service along with CI/CD pipelines with minimal effort. This in turn allows the developers to focus on the core business logic.","title":"FCP Demo Services"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#language-of-choice","text":"Node.Js ASP.NET Core","title":"Language of choice"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#tools-external-dependencies","text":"Azure Service Bus for messaging PgSql","title":"Tools &amp; External Dependencies"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#existing-demo-services","text":"Below are the demo services that are present at the moment. Service Dev Platform Git Repo Payments Service Node.Js https://github.com/DEFRA/ffc-demo-payment-service Payments Service Core Asp.Net Core https://github.com/DEFRA/ffc-demo-payment-service-core Payments Web Node.Js https://github.com/DEFRA/ffc-demo-payment-web Claim Service Node.Js https://github.com/DEFRA/ffc-demo-claim-service Calculation Service Node.Js https://github.com/DEFRA/ffc-demo-calculation-service Collector Service Node.Js https://github.com/DEFRA/ffc-demo-collector Demo Web Node.Js https://github.com/DEFRA/ffc-demo-web","title":"Existing Demo Services"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#business-context","text":"","title":"Business Context"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#microservice-architecture","text":"","title":"Microservice Architecture"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#other-services","text":"Demo Apply Service Generated from Claims service. Needs further clarification from dev team. Demo Apply Web Generated from Demo web app. Needs further clarification from dev team.","title":"Other Services"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#testing","text":"Code Docker Compose Dev Test Pre-production Lint/Audit X Synk Test X Static Code Analysis/ SonarCloud X Functionional/ BDD X X Intergration Tests/ Contract testing using pact broker X X Performance Testing (JMeter) X Pen Testing (OWASP ZAP) X X Code Lint/Audit Synk Test Static Code Analysis/Sonar Cloud Docker Compose Functional/BDD Pen Testing Dev Functional/BDD Integrations Test/Contract testing using Pact Broker Test Integrations Test/Contract testing using Pact Broker Pre-production Performance testing (Jmeter) Pen Testing","title":"Testing"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#challenges","text":"Docker compose is good if the application is full contained, but it has dependencies, which are unknown at present. Could remove Docker compose tests, run in SND2 or SND3 , then install dependencies and helm chart which update flux","title":"Challenges"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#considerations-for-discussion","text":"DAPR - with RabbitMQ for containerized testing Open Policy Agent (OPR)","title":"Considerations for discussion"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#dapr","text":"Distributed Application Runtime simplifies the authoring of distributed, microservice- based applications. Once DAPR is enabled for a container app, a secondary process is created alongside the application that enables communication with DAPR via HTTP or gRPC","title":"DAPR"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#open-policy-agent","text":"Azure Policy extends Gatekeeper v3, an admission controller webhook for Open Policy Agent (OPA), to apply at-scale enforcements and safeguards on your clusters in a centralized, consistent manner. Azure Policy makes it possible to manage and report on the compliance state of your Kubernetes clusters from one place. The add-on enacts the following functions: Checks with Azure Policy service for policy assignments to the cluster. Deploys policy definitions into the cluster as constraint template and constraint custom resources. Reports auditing and compliance details back to Azure Policy service. Azure Policy for Kubernetes supports the following cluster environments: Azure Kubernetes Service (AKS) Azure Arc enabled Kubernetes","title":"Open Policy Agent"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#further-reading","text":"https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes https://learn.microsoft.com/en-us/azure/container-apps/dapr-overview?tabs=bicep1%2Cyaml","title":"Further reading"},{"location":"Developer-Reference/reference-applications/fcp-demo-services/overview/#service-details","text":"TODO This page is a work in progress and will be updated in due course. Add to details about each service.","title":"Service Details"},{"location":"Getting-Started/onboarding-a-delivery-programme/","text":"Onboarding a delivery programme \u00b6 This getting started guide summarises the steps for onboarding a delivery programme onto ADP via the Portal. It also provides an overview of the automated processes involved. Prerequisites \u00b6 Before onboarding a delivery programme you will first need to ensure that: The Arms Length Body (ALB) for your programme has been created within the ADP Portal. You have an active user account within the ADP Portal. You are already a Delivery Programme Admin for another delivery programme. You have a unique \"Delivery Programme code\" or \"Service Code\" for your Delivery Programme. If you are not already a Delivery Programme Admin, you will need one of the members of the ADP Platform Team to create the programme for you. Overview \u00b6 By completing the steps in this guide you will be able to: Add a new delivery programme to the ADP portal database under the programme's ALB. Assign delivery programme admins to administer & maintain the delivery programme. Guide \u00b6 Creating a Delivery Programme \u00b6 Once you have navigated to the 'ADP Data' page you will be presented with the 'Delivery Programmes' option. By clicking 'View' you will have the ability to view existing Delivery Programmes and add new ones if you have the admin permissions. Entering Delivery Programme information \u00b6 You can start entering Delivery Programme information by clicking the 'Add Delivery Programme' button. You will be presented with various fields; some are optional. For example, the 'Finance Code', 'Website', and 'Alias' are not required, and you can add them later if you wish. If the Arms Length Body (ALB) for your programme has already been created it will appear in the Arms Length Body dropdown and you will be able to select it accordingly. This form includes validation. Once you have completed inputting the Delivery Programme Information and pressed 'create', the validation will run to check if any changes need to be made to your inputs. Updating Delivery Programme information \u00b6 Once you have created your Delivery Programme, the delivery programme will appear in the Delivery Programme list, as well as in the catalog as a new Group. You can click the link in the Name column to view the Delivery Programme in the catalog. Clicking the edit button will bring up a form to allow you to change the information about your Delivery Programme Managing Delivery Programme Admins \u00b6 You are able to manage Delivery Programme Admins through the Manage Members tab in the Delivery Programmes catalog page, which can be accessed by clicking the user config button. On this page you are able to add or remove users from the Delivery Programme. There is only one role for members of a Delivery Project which is admin. Anyone added to the Delivery Programme will be able to manage any projects under the programme, as well as edit the programme itself.","title":"Onboarding a delivery programme"},{"location":"Getting-Started/onboarding-a-delivery-programme/#onboarding-a-delivery-programme","text":"This getting started guide summarises the steps for onboarding a delivery programme onto ADP via the Portal. It also provides an overview of the automated processes involved.","title":"Onboarding a delivery programme"},{"location":"Getting-Started/onboarding-a-delivery-programme/#prerequisites","text":"Before onboarding a delivery programme you will first need to ensure that: The Arms Length Body (ALB) for your programme has been created within the ADP Portal. You have an active user account within the ADP Portal. You are already a Delivery Programme Admin for another delivery programme. You have a unique \"Delivery Programme code\" or \"Service Code\" for your Delivery Programme. If you are not already a Delivery Programme Admin, you will need one of the members of the ADP Platform Team to create the programme for you.","title":"Prerequisites"},{"location":"Getting-Started/onboarding-a-delivery-programme/#overview","text":"By completing the steps in this guide you will be able to: Add a new delivery programme to the ADP portal database under the programme's ALB. Assign delivery programme admins to administer & maintain the delivery programme.","title":"Overview"},{"location":"Getting-Started/onboarding-a-delivery-programme/#guide","text":"","title":"Guide"},{"location":"Getting-Started/onboarding-a-delivery-programme/#creating-a-delivery-programme","text":"Once you have navigated to the 'ADP Data' page you will be presented with the 'Delivery Programmes' option. By clicking 'View' you will have the ability to view existing Delivery Programmes and add new ones if you have the admin permissions.","title":"Creating a Delivery Programme"},{"location":"Getting-Started/onboarding-a-delivery-programme/#entering-delivery-programme-information","text":"You can start entering Delivery Programme information by clicking the 'Add Delivery Programme' button. You will be presented with various fields; some are optional. For example, the 'Finance Code', 'Website', and 'Alias' are not required, and you can add them later if you wish. If the Arms Length Body (ALB) for your programme has already been created it will appear in the Arms Length Body dropdown and you will be able to select it accordingly. This form includes validation. Once you have completed inputting the Delivery Programme Information and pressed 'create', the validation will run to check if any changes need to be made to your inputs.","title":"Entering Delivery Programme information"},{"location":"Getting-Started/onboarding-a-delivery-programme/#updating-delivery-programme-information","text":"Once you have created your Delivery Programme, the delivery programme will appear in the Delivery Programme list, as well as in the catalog as a new Group. You can click the link in the Name column to view the Delivery Programme in the catalog. Clicking the edit button will bring up a form to allow you to change the information about your Delivery Programme","title":"Updating Delivery Programme information"},{"location":"Getting-Started/onboarding-a-delivery-programme/#managing-delivery-programme-admins","text":"You are able to manage Delivery Programme Admins through the Manage Members tab in the Delivery Programmes catalog page, which can be accessed by clicking the user config button. On this page you are able to add or remove users from the Delivery Programme. There is only one role for members of a Delivery Project which is admin. Anyone added to the Delivery Programme will be able to manage any projects under the programme, as well as edit the programme itself.","title":"Managing Delivery Programme Admins"},{"location":"Getting-Started/onboarding-a-delivery-project/","text":"Onboarding a delivery project \u00b6 This getting started guide summarises the steps for onboarding a delivery project onto ADP via the Portal. It also provides an overview of the automated processes involved. Prerequisites \u00b6 Before onboarding a delivery project you will first need to ensure that: The delivery programme for your project has been onboarded onto ADP, see the Getting Started guide for Onboarding a delivery programme on to ADP . You have an active user account within the ADP Portal with admin permissions to create a delivery project within your selected delivery programme. You have a \"Service Code\" and \"Cost Centre\" for your delivery project. Overview \u00b6 By completing this guide you will have completed these actions: Adding a new delivery project to ADP portal database under your programme. Assign delivery project admins to adminster delivery project. Creation of a new ADO Team on a selected ADO project. Creation of new GitHub Teams for the delivery project. Adding Azure group(s) for the delivery project's tech users. Members of this group will be given access to common platform resources and project resource group in tenants, and Defra O365_DefraDev - SND3, data and control plane read/ write. Defra - DEV1, TST\u00bd, data and control plane read/ write. Defra - PRE1, PRD1, ready access on the control plane. No data plane access given Guide \u00b6 Creating a Delivery Project \u00b6 Once you have navigated to the 'ADP Data' page you will be presented with the 'Delivery Projects' option. By clicking 'View' you will have the ability to view existing Delivery Projects and add new ones if you have the admin permissions. Entering Delivery Project information \u00b6 You can start entering Delivery Projects information by clicking the 'Add Delivery Projects' button. You will be presented with various fields; some are optional. For example, the 'Alias', 'Website', 'Finance Code' and 'ADO Project' are not required, and you can add them later if you wish. If the Delivery Programme for your project has already been created and you are an admin for it, it will appear in the Delivery Programme dropdown, and you will be able to select it accordingly. This form includes validation. Once you have completed inputting the Delivery Project Information and pressed 'create', the validation will run to check if any changes need to be made to your inputs. Once the project is created, the Github Teams will automatically be created. There will be one team for users who are Technical Team Members of the project, and one for users who are both a Technical Team Member and Admin of the project. Your newly created Delivery Project will now show up in the table, and also in the Catalog as a new Group. You can edit the project by clicking the edit button, or view the project in the catalog by clicking the link in the Name column. Updating Delivery Project information \u00b6 Once you have created your Delivery Project, you will automatically be redirected to the view page which will allow you to look through existing projects and edit them. Manage delivery project members. \u00b6 Members of a delivery project are able to be managed through the Delivery Projects catalog page, under the Manage Members tab. You can easily access this page by clicking the user config button. When adding a new user to the project, you must select the role you wish to give to them. If you give them the Technical Team Member role, you will also need to supply their github username as they will be added to the relevant teams in github, giving them access to any repositories scaffolded for this Delivery Project. Adding creation of a new ADO Team on a selected ADO project. \u00b6 ... Adding Azure group(s) for the delivery project's tech users. \u00b6 ...","title":"Onboarding a delivery project"},{"location":"Getting-Started/onboarding-a-delivery-project/#onboarding-a-delivery-project","text":"This getting started guide summarises the steps for onboarding a delivery project onto ADP via the Portal. It also provides an overview of the automated processes involved.","title":"Onboarding a delivery project"},{"location":"Getting-Started/onboarding-a-delivery-project/#prerequisites","text":"Before onboarding a delivery project you will first need to ensure that: The delivery programme for your project has been onboarded onto ADP, see the Getting Started guide for Onboarding a delivery programme on to ADP . You have an active user account within the ADP Portal with admin permissions to create a delivery project within your selected delivery programme. You have a \"Service Code\" and \"Cost Centre\" for your delivery project.","title":"Prerequisites"},{"location":"Getting-Started/onboarding-a-delivery-project/#overview","text":"By completing this guide you will have completed these actions: Adding a new delivery project to ADP portal database under your programme. Assign delivery project admins to adminster delivery project. Creation of a new ADO Team on a selected ADO project. Creation of new GitHub Teams for the delivery project. Adding Azure group(s) for the delivery project's tech users. Members of this group will be given access to common platform resources and project resource group in tenants, and Defra O365_DefraDev - SND3, data and control plane read/ write. Defra - DEV1, TST\u00bd, data and control plane read/ write. Defra - PRE1, PRD1, ready access on the control plane. No data plane access given","title":"Overview"},{"location":"Getting-Started/onboarding-a-delivery-project/#guide","text":"","title":"Guide"},{"location":"Getting-Started/onboarding-a-delivery-project/#creating-a-delivery-project","text":"Once you have navigated to the 'ADP Data' page you will be presented with the 'Delivery Projects' option. By clicking 'View' you will have the ability to view existing Delivery Projects and add new ones if you have the admin permissions.","title":"Creating a Delivery Project"},{"location":"Getting-Started/onboarding-a-delivery-project/#entering-delivery-project-information","text":"You can start entering Delivery Projects information by clicking the 'Add Delivery Projects' button. You will be presented with various fields; some are optional. For example, the 'Alias', 'Website', 'Finance Code' and 'ADO Project' are not required, and you can add them later if you wish. If the Delivery Programme for your project has already been created and you are an admin for it, it will appear in the Delivery Programme dropdown, and you will be able to select it accordingly. This form includes validation. Once you have completed inputting the Delivery Project Information and pressed 'create', the validation will run to check if any changes need to be made to your inputs. Once the project is created, the Github Teams will automatically be created. There will be one team for users who are Technical Team Members of the project, and one for users who are both a Technical Team Member and Admin of the project. Your newly created Delivery Project will now show up in the table, and also in the Catalog as a new Group. You can edit the project by clicking the edit button, or view the project in the catalog by clicking the link in the Name column.","title":"Entering Delivery Project information"},{"location":"Getting-Started/onboarding-a-delivery-project/#updating-delivery-project-information","text":"Once you have created your Delivery Project, you will automatically be redirected to the view page which will allow you to look through existing projects and edit them.","title":"Updating Delivery Project information"},{"location":"Getting-Started/onboarding-a-delivery-project/#manage-delivery-project-members","text":"Members of a delivery project are able to be managed through the Delivery Projects catalog page, under the Manage Members tab. You can easily access this page by clicking the user config button. When adding a new user to the project, you must select the role you wish to give to them. If you give them the Technical Team Member role, you will also need to supply their github username as they will be added to the relevant teams in github, giving them access to any repositories scaffolded for this Delivery Project.","title":"Manage delivery project members."},{"location":"Getting-Started/onboarding-a-delivery-project/#adding-creation-of-a-new-ado-team-on-a-selected-ado-project","text":"...","title":"Adding creation of a new ADO Team on a selected ADO project."},{"location":"Getting-Started/onboarding-a-delivery-project/#adding-azure-groups-for-the-delivery-projects-tech-users","text":"...","title":"Adding Azure group(s) for the delivery project's tech users."},{"location":"Getting-Started/onboarding-a-user/","text":"Onboarding a user \u00b6 This getting started guide summarises the steps for onboarding a user onto your delivery project in ADP. It also provides an overview of the automated processes involved. Prerequisites \u00b6 Before onboarding a user on to your delivery project you will first need to ensure that: The delivery project has been onboarded via the ADP portal, see the Getting Started guide for Onboarding a delivery project on to ADP . You have an active user account within the ADP Portal with admin permissions to onboard users to your selected delivery project. The user you are onboarding has a valid cloud account (with the domain as: @defra.onmicrosoft.com or @defra.gov.uk). [Need to link to guidence on how to get a cloud account]. If the user you are onboarding is a tech user, they must have a valid GitHub handle. GitHub account added to DEFRA's SonarCloud organisation. Overview \u00b6 By completing this guide you will have completed these actions: Understanding of tech and non-tech users. Adding user to ADP portal database under your delivery team. [Not automated] Adding user to Azure AD ADP portal group, allowing basic read acess to the ADP portal. [Not automated] Adding user to ADO Team, allowing access to delivery project's ADO project. [Not automated] Adding user to Defra's VPN group. Adding tech user to GitHub Team. Adding tech user to Azure group, allowing access to delivery project's Azure resources. Guide \u00b6 You can add a user to a Delivery Project from the Manage Members tab on the Delivery Projects page. You can get to this page by searching for your delivery project in the catalog with the kind set to Group and the type to delivery-project . Alternatively, you can find the delivery project via the 'Delivery Projects' card in the ADP Onboarding page. Onboarding a Delivery Project Member \u00b6 When adding a user to a delivery project, you will need to select which user to add as well as what roles to grant them. There are 2 roles which can be assigned to a user for a Delivery Project: - Technical User - A user who is able to scaffold new services and contribute to existing ones - Admin - A user who can manage the Delivery Project, including editing it and managing its members If a user is going to be a Technical user, you will need to provide a github handle so they can be added to the correct teams in github. If neither 'additional permission' roles are selected, the user will be added to the delivery project as a \"Team Member\"","title":"Onboarding a user"},{"location":"Getting-Started/onboarding-a-user/#onboarding-a-user","text":"This getting started guide summarises the steps for onboarding a user onto your delivery project in ADP. It also provides an overview of the automated processes involved.","title":"Onboarding a user"},{"location":"Getting-Started/onboarding-a-user/#prerequisites","text":"Before onboarding a user on to your delivery project you will first need to ensure that: The delivery project has been onboarded via the ADP portal, see the Getting Started guide for Onboarding a delivery project on to ADP . You have an active user account within the ADP Portal with admin permissions to onboard users to your selected delivery project. The user you are onboarding has a valid cloud account (with the domain as: @defra.onmicrosoft.com or @defra.gov.uk). [Need to link to guidence on how to get a cloud account]. If the user you are onboarding is a tech user, they must have a valid GitHub handle. GitHub account added to DEFRA's SonarCloud organisation.","title":"Prerequisites"},{"location":"Getting-Started/onboarding-a-user/#overview","text":"By completing this guide you will have completed these actions: Understanding of tech and non-tech users. Adding user to ADP portal database under your delivery team. [Not automated] Adding user to Azure AD ADP portal group, allowing basic read acess to the ADP portal. [Not automated] Adding user to ADO Team, allowing access to delivery project's ADO project. [Not automated] Adding user to Defra's VPN group. Adding tech user to GitHub Team. Adding tech user to Azure group, allowing access to delivery project's Azure resources.","title":"Overview"},{"location":"Getting-Started/onboarding-a-user/#guide","text":"You can add a user to a Delivery Project from the Manage Members tab on the Delivery Projects page. You can get to this page by searching for your delivery project in the catalog with the kind set to Group and the type to delivery-project . Alternatively, you can find the delivery project via the 'Delivery Projects' card in the ADP Onboarding page.","title":"Guide"},{"location":"Getting-Started/onboarding-a-user/#onboarding-a-delivery-project-member","text":"When adding a user to a delivery project, you will need to select which user to add as well as what roles to grant them. There are 2 roles which can be assigned to a user for a Delivery Project: - Technical User - A user who is able to scaffold new services and contribute to existing ones - Admin - A user who can manage the Delivery Project, including editing it and managing its members If a user is going to be a Technical user, you will need to provide a github handle so they can be added to the correct teams in github. If neither 'additional permission' roles are selected, the user will be added to the delivery project as a \"Team Member\"","title":"Onboarding a Delivery Project Member"},{"location":"How-to-guides/Platform-Services/how-to-create-a-database/","text":"How to create a database for a platform service \u00b6 PostgreSQL is the preferred relational database for microservices. This guide describes the process for creating a database for a microservice and configuring the microservice to use it. Note The ADP Platform currently supports PostgreSQL only as the option available for a relational database. How to create a Postgres Database \u00b6 There are two ways for creating a Postgres Database in the ADP. When scaffolding a new Backend service using the ADP Portal. You have the option to specify the name of the database. Refer to the section on Selecting a template For an existing service, you can add values to the Infrastructure Helm Chart values. Refer to Infrastructure section on Database for Flexible Server Tip An example of how to specify the Helm Chart values is provided in the ffc-demo-claim-service repository , refer to the configuration in the values.yaml . How to appy Database migrations \u00b6 The ADP Platform CI and deployment pipelines support database migrations using Liquibase . Create a Liquibase changelog defining the structure of your database available from the root of your microservice repository in changelog/db.changelog.xml . Guidance on creating a Liquibase changelog is outside of the scope of this guide. Update Docker Compose files to use Postgres service and environment variables \u00b6 Update docker-compose.yaml , docker-compose.override.yaml , and docker-compose.test.yaml to include a Postgres service and add Postgres environment variables to the microservice. Replace <programme code> and <service> as per naming convention described above. Local Development \u00b6 The following scripts and files are scaffolded as part of your backend service to provide a good local development experience. A docker-compose.migrate.yaml in the root of your microservice repository that spins up Postgres in a Docker container. The scripts/ folder contains three bash scripts start , test and postgres-wait . The scripts/migration/ folder contains two scripts to apply and remove migrations. The two scripts are database-up and database-up . Execute the start script to start the Postgres container. 1 2 3 4 5 6 7 8 9 # snippet of the code in the start script cd \" ${ projectRoot } \" # Guarantee clean environment docker-compose down -v docker-compose -f docker-compose.migrate.yaml down -v # Ensure container images are up to date docker-compose -f docker-compose.migrate.yaml run database-up docker-compose up --build How to Enable POSTGRES Extensions \u00b6 Some microservices require Postgres extensions to be installed in the database. Below is the list of the enabled extensions: VECTOR UUID-OSSP This is a two step process. Step 1. Enable extensions on the server \u00b6 Tip Request the ADP Platform Team to enable the extension on the Postgres Flexible server if it is not in the list above of enabled extensions. Step 2. Enable extensions on the database using a user account that has database admin permissions \u00b6 When scripting the database migrations for creating extensions, use IF NOT EXISTS . This will ensure that the scripts can both run locally and an in Azure. When running the Postgres database locally in Docker, you will have sufficient permissions to create the extensions. However, in Azure, the ADP Platform will apply the migrations to the database instead of using the microservice's managed identity. If you don't use IF NOT EXISTS , the migrations on the Azure Postgres database will fail due to insufficient permissions. Below is an example of a SQL script you can use in your migration to enable an extension. 1 2 CREATE EXTENSION IF NOT EXISTS vector ; CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\" ;","title":"How to create a database"},{"location":"How-to-guides/Platform-Services/how-to-create-a-database/#how-to-create-a-database-for-a-platform-service","text":"PostgreSQL is the preferred relational database for microservices. This guide describes the process for creating a database for a microservice and configuring the microservice to use it. Note The ADP Platform currently supports PostgreSQL only as the option available for a relational database.","title":"How to create a database for a platform service"},{"location":"How-to-guides/Platform-Services/how-to-create-a-database/#how-to-create-a-postgres-database","text":"There are two ways for creating a Postgres Database in the ADP. When scaffolding a new Backend service using the ADP Portal. You have the option to specify the name of the database. Refer to the section on Selecting a template For an existing service, you can add values to the Infrastructure Helm Chart values. Refer to Infrastructure section on Database for Flexible Server Tip An example of how to specify the Helm Chart values is provided in the ffc-demo-claim-service repository , refer to the configuration in the values.yaml .","title":"How to create a Postgres Database"},{"location":"How-to-guides/Platform-Services/how-to-create-a-database/#how-to-appy-database-migrations","text":"The ADP Platform CI and deployment pipelines support database migrations using Liquibase . Create a Liquibase changelog defining the structure of your database available from the root of your microservice repository in changelog/db.changelog.xml . Guidance on creating a Liquibase changelog is outside of the scope of this guide.","title":"How to appy Database migrations"},{"location":"How-to-guides/Platform-Services/how-to-create-a-database/#update-docker-compose-files-to-use-postgres-service-and-environment-variables","text":"Update docker-compose.yaml , docker-compose.override.yaml , and docker-compose.test.yaml to include a Postgres service and add Postgres environment variables to the microservice. Replace <programme code> and <service> as per naming convention described above.","title":"Update Docker Compose files to use Postgres service and environment variables"},{"location":"How-to-guides/Platform-Services/how-to-create-a-database/#local-development","text":"The following scripts and files are scaffolded as part of your backend service to provide a good local development experience. A docker-compose.migrate.yaml in the root of your microservice repository that spins up Postgres in a Docker container. The scripts/ folder contains three bash scripts start , test and postgres-wait . The scripts/migration/ folder contains two scripts to apply and remove migrations. The two scripts are database-up and database-up . Execute the start script to start the Postgres container. 1 2 3 4 5 6 7 8 9 # snippet of the code in the start script cd \" ${ projectRoot } \" # Guarantee clean environment docker-compose down -v docker-compose -f docker-compose.migrate.yaml down -v # Ensure container images are up to date docker-compose -f docker-compose.migrate.yaml run database-up docker-compose up --build","title":"Local Development"},{"location":"How-to-guides/Platform-Services/how-to-create-a-database/#how-to-enable-postgres-extensions","text":"Some microservices require Postgres extensions to be installed in the database. Below is the list of the enabled extensions: VECTOR UUID-OSSP This is a two step process.","title":"How to Enable POSTGRES Extensions"},{"location":"How-to-guides/Platform-Services/how-to-create-a-database/#step-1-enable-extensions-on-the-server","text":"Tip Request the ADP Platform Team to enable the extension on the Postgres Flexible server if it is not in the list above of enabled extensions.","title":"Step 1. Enable extensions on the server"},{"location":"How-to-guides/Platform-Services/how-to-create-a-database/#step-2-enable-extensions-on-the-database-using-a-user-account-that-has-database-admin-permissions","text":"When scripting the database migrations for creating extensions, use IF NOT EXISTS . This will ensure that the scripts can both run locally and an in Azure. When running the Postgres database locally in Docker, you will have sufficient permissions to create the extensions. However, in Azure, the ADP Platform will apply the migrations to the database instead of using the microservice's managed identity. If you don't use IF NOT EXISTS , the migrations on the Azure Postgres database will fail due to insufficient permissions. Below is an example of a SQL script you can use in your migration to enable an extension. 1 2 CREATE EXTENSION IF NOT EXISTS vector ; CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\" ;","title":"Step 2. Enable extensions on the database using a user account that has database admin permissions"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/","text":"How to create a platform service \u00b6 In this how to guide you will learn how to create a new Platform service on ADP for your delivery project and team. You will also learn what automated actions will take place, and any areas of support that may be needed. Prerequisites \u00b6 Before creating Platform business service (microservice), you will first need to ensure that you have: Onboarded delivery project on to ADP . An active user account on the ADP Portal with admin or user access to/for the delivery project team. You will need an Azure DevOps project for Pipeline management for your project/programme. In your teams Azure DevOps (ADO) project, you will need to ensure that the ADP Platform Service Account has the correct permissions for scaffolding your service Pipelines: These permissions are Project, Build, and Endpoint Administrator. Read & Manage Environments. The ADP Platform Engineers or CCoE can manage this for you. Note Please contact the ADP Platform Engineering team for support if you don\u2019t have, and cannot setup/configure, these prerequisites. Overview \u00b6 By completing this guide, you will have completed these actions: Created a GitHub team for the services' delivery project (if not already present) in Defra GitHub org. Developers are added to your GitHub team. Scaffolded an \u2018exemplar\u2019 Platform service (microservice application) in your chosen development language, with any optional infrastructure. Scaffolding of an ADO project if not already done: Share service connection and agent pools with ADO project. Create ADO environment and \u2018secrets\u2019 variable group in ADO project. Authorizes the Service Connection between the created ADO Pipeline & GitHub repository. Creation of an ADO CI/CD Pipeline for the Service in the selected ADO project. Starts the running of your Service CI/CD Pipeline: Builds and deploys your service into the Sandpit / Development environment. Initialization of Sonar Cloud project, Snyk scanning, Service Manifests, etc. Register your Service in the Backstage Catalog (via Catalog info YAML file). Areas of support \u00b6 The following areas require the support of the ADP Platform Team for your service initial setup: Domain (URL) creation Note The initial domain (Frontend service or external API URL) creation is currently done via the Platform team pipelines. Please contact the Platform team to create this per environment once the service is scaffolded. Guide \u00b6 Selecting a template \u00b6 On the ADP portal click \u2018Create...\u2019 on the left-hand side menu. Select the \u2018CHOOSE\u2019 button of the template of the service you would like to create. Tip You can choose a Node.js for Frontends, or for Backends and APIs in Node.Js or C#. Entering Component Information \u00b6 Enter the properties describing your component/service: Enter Component Name (service name). It must be a unique name of the component which will be used for the repository, package, and image name. This should be in the format {programme}-{project}-{service} . For example, fcp-grants-web. Enter Description . Describes what this component does. This description will be used in the component's README and package.json. Select the System that this component/service will be a part of. Systems are a collection of related components and resources (i.e., your entire service and associations). Select the Project Phase which suits your service. Refer to the GDS service manual for more information. Select the Owner (team) who will own this component (i.e., your delivery team). Optionally: Select the initial infrastructure (Queues/Topics, Database etc) you want to deploy with your service. More infra can be added/updated later via the YAML file in your repo! Click the Next button to continue. Entering Git Repository information \u00b6 To encourage coding in the open the repository will be public by default. Refer to the GDS service manual for more information. You can select a \u2018private\u2019 repository by selecting the \u2018private repo\u2019 flag in GitHub. The scaffolder will create a new repository and an associated team with \u2018Write\u2019 permissions: The host where the repository will be created \u2013 the default will be the GitHub organisation of DEFRA. Enter name of the repository . Should be the same as component name (service name). Enter GitHub Team Name . This team will be granted \u2018Write\u2019 access to the repository. Enter GitHub Team Description . An optional description of the team. Enter GitHub Team Members . Using comma-separated list of GitHub usernames to be added to the team. For example: GitHubName1,GitHubName2. Select GitHub Team Visibility . This is privacy level this team should have. By selecting Visible teams can be seen by all members in the organization. Secret teams can only be seen by the organization owners and team members. Click Next to move to the next page. Entering CI/CD information \u00b6 CI/CD pipelines will be created in Azure DevOps: Azure DevOps Organization . *This will be defaulted to: DefraGovUK and not changeable. Enter your projects \u2018 Azure DevOps Project Name \u2019. This is the name of your project you are a member of and wish to scaffold your pipelines into. Service Connection Name . *This will be defaulted and not changeable. Enter the Pipeline Folder . The Folder Path is the directory structure for the Pipeline which will be created in your project. For example: ADP/fcp-grants-web. Hint: You can group many pipelines into one Folder structure. Click Review to move to the next page. Reviewing entered information \u00b6 Review the information entered and click back if you would like to amend any of the provided information. If you think all entered information is correct click the create button to begin creation of your new service. Creating the service \u00b6 Now you have reviewed and confirmed your details, your new Platform service will be created! It will complete the actions detailed in the overview section. Once this process completes, you will be given links to your new GitHub repository, the Portal Catalog location, and your Pipelines. You now have an ADP business service! Creation of additional infrastructure \u00b6 We use HELM Charts to deploy, manage and update Platform service applications and their dedicated and associated infrastructure. This is \u2018self-service\u2019 managed by the platform development teams/tenants. We use Azure Bicep/PowerShell for all other Azure infrastructure and Entra ID configuration, including Platform shared and \u2018core\u2019 infrastructure. This is managed by the ADP Platform Engineers team. An Azure Managed Identity ( Workload ID ) will be automatically created for every service (microservice app) for your usage (i.e. assigning RBAC roles to it). How do I use the HELM Charts for infrastructure with my application? \u00b6 The creation of infrastructure dedicated for your business service/application is done via your microservice HELM Chart in your repository, and deployed by your Service CI/CD pipeline that you created earlier. A \u2018helm\u2019 folder will be created in every scaffolded service with 2 subfolders. The one ending with \u2018-infra\u2019 is where you define your service\u2019s infrastructure requirements in a simple YAML format. Note The full list of supported \u2018self-service\u2019 infrastructure can be found in the ADP ASO Helm Library Documentation on GitHub with instructions on how to use it. Image below is an example of how-to self-service create additional infrastructure by updating the HELM charts \u2018values.yaml\u2019 file with what you require to be deployed: Warning Please contact the ADP Platform Engineers Team if you require any support after reading the provided documentation or if you\u2019re stuck.","title":"How to create a platform service"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#how-to-create-a-platform-service","text":"In this how to guide you will learn how to create a new Platform service on ADP for your delivery project and team. You will also learn what automated actions will take place, and any areas of support that may be needed.","title":"How to create a platform service"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#prerequisites","text":"Before creating Platform business service (microservice), you will first need to ensure that you have: Onboarded delivery project on to ADP . An active user account on the ADP Portal with admin or user access to/for the delivery project team. You will need an Azure DevOps project for Pipeline management for your project/programme. In your teams Azure DevOps (ADO) project, you will need to ensure that the ADP Platform Service Account has the correct permissions for scaffolding your service Pipelines: These permissions are Project, Build, and Endpoint Administrator. Read & Manage Environments. The ADP Platform Engineers or CCoE can manage this for you. Note Please contact the ADP Platform Engineering team for support if you don\u2019t have, and cannot setup/configure, these prerequisites.","title":"Prerequisites"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#overview","text":"By completing this guide, you will have completed these actions: Created a GitHub team for the services' delivery project (if not already present) in Defra GitHub org. Developers are added to your GitHub team. Scaffolded an \u2018exemplar\u2019 Platform service (microservice application) in your chosen development language, with any optional infrastructure. Scaffolding of an ADO project if not already done: Share service connection and agent pools with ADO project. Create ADO environment and \u2018secrets\u2019 variable group in ADO project. Authorizes the Service Connection between the created ADO Pipeline & GitHub repository. Creation of an ADO CI/CD Pipeline for the Service in the selected ADO project. Starts the running of your Service CI/CD Pipeline: Builds and deploys your service into the Sandpit / Development environment. Initialization of Sonar Cloud project, Snyk scanning, Service Manifests, etc. Register your Service in the Backstage Catalog (via Catalog info YAML file).","title":"Overview"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#areas-of-support","text":"The following areas require the support of the ADP Platform Team for your service initial setup: Domain (URL) creation Note The initial domain (Frontend service or external API URL) creation is currently done via the Platform team pipelines. Please contact the Platform team to create this per environment once the service is scaffolded.","title":"Areas of support"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#guide","text":"","title":"Guide"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#selecting-a-template","text":"On the ADP portal click \u2018Create...\u2019 on the left-hand side menu. Select the \u2018CHOOSE\u2019 button of the template of the service you would like to create. Tip You can choose a Node.js for Frontends, or for Backends and APIs in Node.Js or C#.","title":"Selecting a template"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#entering-component-information","text":"Enter the properties describing your component/service: Enter Component Name (service name). It must be a unique name of the component which will be used for the repository, package, and image name. This should be in the format {programme}-{project}-{service} . For example, fcp-grants-web. Enter Description . Describes what this component does. This description will be used in the component's README and package.json. Select the System that this component/service will be a part of. Systems are a collection of related components and resources (i.e., your entire service and associations). Select the Project Phase which suits your service. Refer to the GDS service manual for more information. Select the Owner (team) who will own this component (i.e., your delivery team). Optionally: Select the initial infrastructure (Queues/Topics, Database etc) you want to deploy with your service. More infra can be added/updated later via the YAML file in your repo! Click the Next button to continue.","title":"Entering Component Information"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#entering-git-repository-information","text":"To encourage coding in the open the repository will be public by default. Refer to the GDS service manual for more information. You can select a \u2018private\u2019 repository by selecting the \u2018private repo\u2019 flag in GitHub. The scaffolder will create a new repository and an associated team with \u2018Write\u2019 permissions: The host where the repository will be created \u2013 the default will be the GitHub organisation of DEFRA. Enter name of the repository . Should be the same as component name (service name). Enter GitHub Team Name . This team will be granted \u2018Write\u2019 access to the repository. Enter GitHub Team Description . An optional description of the team. Enter GitHub Team Members . Using comma-separated list of GitHub usernames to be added to the team. For example: GitHubName1,GitHubName2. Select GitHub Team Visibility . This is privacy level this team should have. By selecting Visible teams can be seen by all members in the organization. Secret teams can only be seen by the organization owners and team members. Click Next to move to the next page.","title":"Entering Git Repository information"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#entering-cicd-information","text":"CI/CD pipelines will be created in Azure DevOps: Azure DevOps Organization . *This will be defaulted to: DefraGovUK and not changeable. Enter your projects \u2018 Azure DevOps Project Name \u2019. This is the name of your project you are a member of and wish to scaffold your pipelines into. Service Connection Name . *This will be defaulted and not changeable. Enter the Pipeline Folder . The Folder Path is the directory structure for the Pipeline which will be created in your project. For example: ADP/fcp-grants-web. Hint: You can group many pipelines into one Folder structure. Click Review to move to the next page.","title":"Entering CI/CD information"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#reviewing-entered-information","text":"Review the information entered and click back if you would like to amend any of the provided information. If you think all entered information is correct click the create button to begin creation of your new service.","title":"Reviewing entered information"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#creating-the-service","text":"Now you have reviewed and confirmed your details, your new Platform service will be created! It will complete the actions detailed in the overview section. Once this process completes, you will be given links to your new GitHub repository, the Portal Catalog location, and your Pipelines. You now have an ADP business service!","title":"Creating the service"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#creation-of-additional-infrastructure","text":"We use HELM Charts to deploy, manage and update Platform service applications and their dedicated and associated infrastructure. This is \u2018self-service\u2019 managed by the platform development teams/tenants. We use Azure Bicep/PowerShell for all other Azure infrastructure and Entra ID configuration, including Platform shared and \u2018core\u2019 infrastructure. This is managed by the ADP Platform Engineers team. An Azure Managed Identity ( Workload ID ) will be automatically created for every service (microservice app) for your usage (i.e. assigning RBAC roles to it).","title":"Creation of additional infrastructure"},{"location":"How-to-guides/Platform-Services/how-to-create-a-platform-service/#how-do-i-use-the-helm-charts-for-infrastructure-with-my-application","text":"The creation of infrastructure dedicated for your business service/application is done via your microservice HELM Chart in your repository, and deployed by your Service CI/CD pipeline that you created earlier. A \u2018helm\u2019 folder will be created in every scaffolded service with 2 subfolders. The one ending with \u2018-infra\u2019 is where you define your service\u2019s infrastructure requirements in a simple YAML format. Note The full list of supported \u2018self-service\u2019 infrastructure can be found in the ADP ASO Helm Library Documentation on GitHub with instructions on how to use it. Image below is an example of how-to self-service create additional infrastructure by updating the HELM charts \u2018values.yaml\u2019 file with what you require to be deployed: Warning Please contact the ADP Platform Engineers Team if you require any support after reading the provided documentation or if you\u2019re stuck.","title":"How do I use the HELM Charts for infrastructure with my application?"},{"location":"How-to-guides/Platform-Services/how-to-create-a-system/","text":"What is a system? \u00b6 A system is a label used to group together multiple related services. This label is recognized and used by backstage in order to make it clear what services interact with eachother. They are a concept which is provided by backstage out of the box, and is documented by them here How to create a system \u00b6 In order to create a system, you simply need to add a new definition for it to the ADP software templates repository . There is an example system to show the format that should be used. Once this system is added, you need to add a link to it from the all.yaml file . You will also need to choose a name for your system, which should be in the format {delivery-project-id}-{system-name}-system e.g. fcp-demo-example-system . Once the system has been added and the all.yaml file has been updated, you will need to wait for the ADP portal to re-scan the repository which happens every hour. If you need the system to be available sooner than that, then an ADP admin can trigger a refresh at any time by requesting a refresh of the project-systems location . The all.yaml file \u00b6 The all.yaml file is what tells the ADP portal where to find the systems, and so every file containing a definition for a system must be pointed to by this file. To point to a new file, you will need to add a new entry to the targets array which should be the relative path from the all.yaml file to your new system file. all.yaml 1 2 3 4 5 6 7 8 9 10 apiVersion : backstage.io/v1alpha1 kind : Location metadata : name : project-systems description : Systems defined and owned by projects using ADP. spec : targets : # There is no need to modify any of the file above this point. - ./another-projects-system.yaml - ./my-system.yaml # Reference a system by a relative path like this. The {system}.yaml file \u00b6 Your system will actually be defined inside its own .yaml file. The name of this file should be the name of the system you are creating to make it easier to track which system is defined where. The format of this file should follow this example: my-system.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : backstage.io/v1alpha1 kind : System metadata : # This is the id of the system you want to create. It should be less than 100 characters long and # only contain letters, numbers and hyphens and be in the format `{delivery-project-id}-{system-name}-system` name : fcp-demo-example-system # The description is meant to give users a bit of information about what sort of services are meant # to be linked to this system and what it represents. This can be any text that you like, as long as it is valid yaml. description : An example system, intended to be used to help projects to create their own systems. spec : # This needs to be a reference to the project that owns the system you are creating. This will always # start with `group:default/`, and the ending bit should be the id of your project. You can find the id # of your project in the url when you view it in the ADP portal, it will be the last segment of the URL owner : \"group:default/fcp-demo\"","title":"How to create a system"},{"location":"How-to-guides/Platform-Services/how-to-create-a-system/#what-is-a-system","text":"A system is a label used to group together multiple related services. This label is recognized and used by backstage in order to make it clear what services interact with eachother. They are a concept which is provided by backstage out of the box, and is documented by them here","title":"What is a system?"},{"location":"How-to-guides/Platform-Services/how-to-create-a-system/#how-to-create-a-system","text":"In order to create a system, you simply need to add a new definition for it to the ADP software templates repository . There is an example system to show the format that should be used. Once this system is added, you need to add a link to it from the all.yaml file . You will also need to choose a name for your system, which should be in the format {delivery-project-id}-{system-name}-system e.g. fcp-demo-example-system . Once the system has been added and the all.yaml file has been updated, you will need to wait for the ADP portal to re-scan the repository which happens every hour. If you need the system to be available sooner than that, then an ADP admin can trigger a refresh at any time by requesting a refresh of the project-systems location .","title":"How to create a system"},{"location":"How-to-guides/Platform-Services/how-to-create-a-system/#the-allyaml-file","text":"The all.yaml file is what tells the ADP portal where to find the systems, and so every file containing a definition for a system must be pointed to by this file. To point to a new file, you will need to add a new entry to the targets array which should be the relative path from the all.yaml file to your new system file. all.yaml 1 2 3 4 5 6 7 8 9 10 apiVersion : backstage.io/v1alpha1 kind : Location metadata : name : project-systems description : Systems defined and owned by projects using ADP. spec : targets : # There is no need to modify any of the file above this point. - ./another-projects-system.yaml - ./my-system.yaml # Reference a system by a relative path like this.","title":"The all.yaml file"},{"location":"How-to-guides/Platform-Services/how-to-create-a-system/#the-systemyaml-file","text":"Your system will actually be defined inside its own .yaml file. The name of this file should be the name of the system you are creating to make it easier to track which system is defined where. The format of this file should follow this example: my-system.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : backstage.io/v1alpha1 kind : System metadata : # This is the id of the system you want to create. It should be less than 100 characters long and # only contain letters, numbers and hyphens and be in the format `{delivery-project-id}-{system-name}-system` name : fcp-demo-example-system # The description is meant to give users a bit of information about what sort of services are meant # to be linked to this system and what it represents. This can be any text that you like, as long as it is valid yaml. description : An example system, intended to be used to help projects to create their own systems. spec : # This needs to be a reference to the project that owns the system you are creating. This will always # start with `group:default/`, and the ending bit should be the id of your project. You can find the id # of your project in the url when you view it in the ADP portal, it will be the last segment of the URL owner : \"group:default/fcp-demo\"","title":"The {system}.yaml file"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/","text":"How to delegate database access between microservices - Draft version \u00b6 Overview \u00b6 This documentation outlines a proposed design for enabling shared access for databases between microservices. Considerations \u00b6 Both microservices which share the database need to be in the same namespace The delegated microservice will only have read/write access Proposed Design \u00b6 A k8s job template will be added to the application helmchart which can execute a powershell script to validate and provide the necessary permissions to the delegatee service. Delegator (Microservice that owns the database) Delegatee (Microservice that requests access to the database) Delegator service needs to mark the database as delegatable and which microservices can access the database. Helm library \u00b6 A new k8s Job template will be added to adp-aso-helm-library to run the postgresql script to create the role and provide necessary permissions for delegatee service in the target database. By default, the role is created for a service that owns a database and it is part of flux pre-deploy kustomization. Database template on the library will also need to be modified to handle the conditional creation/delegation of database with appropriate flags. Job template \u00b6 The k8s job uses the existing pattern of running a powershell script inside a docker container. This job will run the provided script that will validate and grant the access to delegatee service. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : batch/v1 kind : Job metadata : name : delegate-db-access namespace : flux-config labels : azure.workload.identity/use : \"true\" backstage.io/kubernetes-id : <<service-name>> backstage.io/kubernetes-team : <<namespace>> spec : template : metadata : labels : azure.workload.identity/use : \"true\" backstage.io/kubernetes-id : <<service-name>> backstage.io/kubernetes-team : <<namespace>> spec : serviceAccountName : <<service-account-for-workload-id>> restartPolicy : Never volumes : - name : custom-root-ca secret : secretName : custom-ca-trust-secret optional : true containers : - name : <<service-name>>-delegate-db-access imagePullPolicy : Always image : <<acr-name>>.azurecr.io/image/powershell-executor:491667 env : - name : GIT_REPO_URL value : https://github.com/DEFRA/adp-flux-services.git - name : GIT_BRANCH value : main - name : SCRIPT_FILE_NAME value : common/scripts/access-control/flexible-server/<<powershell-script-filename>> - name : POSTGRES_HOST value : <<postgres-server>>.postgres.database.azure.com - name : POSTGRES_DATABASE value : <<postgres-db>> ... resources : requests : cpu : 100m memory : 250Mi limits : cpu : 500m memory : 600Mi volumeMounts : - name : custom-root-ca readOnly : true mountPath : /etc/ssl/certs/defra-egress-firewall-cert-01.crt subPath : defra-egress-firewall-cert-01.crt backoffLimit : 2 Microservice Infra Helm Chart (TBD) \u00b6 The above job template can be easily included in the application helm chart templates with the below code. 1 { { - include \"adp-aso-helm-library.delegate-db-access\" . - } } In the values.yaml for infra helm chart, the delegator service would add a label/flag to denote that the db can be provided delegate permissions. By default, delegate will be set to 'no' . 1 2 3 4 5 6 postgres : db : name : claim delegate : 'yes' --> optional charset : UTF8 collation : en_US.utf8 On the other hand, the delegatee service would have the delegated parameter set to 'yes' . Setting delegated parameter ensures that a new database won't be created and it will provide roleassignments on the existing database. This will require roleassignment to be set to read or readwrite and ignore any other parameters set for the database in values.yaml. 1 2 3 4 5 6 postgres : db : name : claim delegated : 'yes' roleAssignments : - roleName : 'readwrite' --> required if delegated is set to 'yes' Job - Powershell script (TBD) \u00b6 Validate: The powershell script needs to validate the below before providing necessary access to the database. The database should have been set to delegate by the delegator service to enable sharing access. Check if the delegatee service name is listed by the delegator service to share access to db. Grant access: Runs a set of SQL scripts on the postgres db with the team's postgres-db-writer AD group to create delegatee service role and grant necessary access. Validate \u00b6 Powershell script to check if the database is delegatable and the service is authorized by delegator service to allow granting access. Grant Access \u00b6 Create Role \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 DO $$ BEGIN IF NOT EXISTS ( SELECT 1 FROM pgaadauth_list_principals ( false ) WHERE rolname = '<<delegatee-service-MI-name>>' ) THEN RAISE NOTICE 'CREATING PRINCIPAL FOR MANAGED IDENTITY:<<delegatee-service-MI-name>>' ; PERFORM pgaadauth_create_principal ( '<<delegatee-service-MI-name>>' , false , false ); RAISE NOTICE 'PRINCIPAL FOR MANAGED IDENTITY CREATED:<<delegatee-service-MI-name>>' ; ELSE RAISE NOTICE 'PRINCIPAL FOR MANAGED IDENTITY ALREADY EXISTS:<<delegatee-service-MI-name>>' ; END IF ; EXECUTE ( 'GRANT CONNECT ON DATABASE \"crai-mcu-knowledge\" TO \"<<delegatee-service-MI-name>>\"' ); RAISE NOTICE 'GRANTED CONNECT TO DATABASE' ; EXCEPTION WHEN OTHERS THEN RAISE EXCEPTION 'ERROR DURING PRINCIPAL CREATION/GRANT CONNECT: %' , SQLERRM ; END $$ Reader perms \u00b6 1 2 3 4 GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; GRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO \"<<delegatee-service-MI-name>>\" ; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON SEQUENCES TO \"<<delegatee-service-MI-name>>\" ; Writer perms \u00b6 1 2 3 4 5 6 7 8 GRANT CREATE , USAGE ON SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; GRANT SELECT , UPDATE , INSERT , REFERENCES , TRIGGER ON ALL TABLES IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; GRANT SELECT , UPDATE , USAGE ON ALL SEQUENCES IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; GRANT EXECUTE ON ALL PROCEDURES IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT , UPDATE , INSERT , REFERENCES , TRIGGER ON TABLES TO \"<<delegatee-service-MI-name>>\" ; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT , USAGE ON SEQUENCES TO \"<<delegatee-service-MI-name>>\" ; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT EXECUTE ON FUNCTIONS TO \"<<delegatee-service-MI-name>>\" ;","title":"How to delegate database access between microservices"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#how-to-delegate-database-access-between-microservices-draft-version","text":"","title":"How to delegate database access between microservices - Draft version"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#overview","text":"This documentation outlines a proposed design for enabling shared access for databases between microservices.","title":"Overview"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#considerations","text":"Both microservices which share the database need to be in the same namespace The delegated microservice will only have read/write access","title":"Considerations"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#proposed-design","text":"A k8s job template will be added to the application helmchart which can execute a powershell script to validate and provide the necessary permissions to the delegatee service. Delegator (Microservice that owns the database) Delegatee (Microservice that requests access to the database) Delegator service needs to mark the database as delegatable and which microservices can access the database.","title":"Proposed Design"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#helm-library","text":"A new k8s Job template will be added to adp-aso-helm-library to run the postgresql script to create the role and provide necessary permissions for delegatee service in the target database. By default, the role is created for a service that owns a database and it is part of flux pre-deploy kustomization. Database template on the library will also need to be modified to handle the conditional creation/delegation of database with appropriate flags.","title":"Helm library"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#job-template","text":"The k8s job uses the existing pattern of running a powershell script inside a docker container. This job will run the provided script that will validate and grant the access to delegatee service. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : batch/v1 kind : Job metadata : name : delegate-db-access namespace : flux-config labels : azure.workload.identity/use : \"true\" backstage.io/kubernetes-id : <<service-name>> backstage.io/kubernetes-team : <<namespace>> spec : template : metadata : labels : azure.workload.identity/use : \"true\" backstage.io/kubernetes-id : <<service-name>> backstage.io/kubernetes-team : <<namespace>> spec : serviceAccountName : <<service-account-for-workload-id>> restartPolicy : Never volumes : - name : custom-root-ca secret : secretName : custom-ca-trust-secret optional : true containers : - name : <<service-name>>-delegate-db-access imagePullPolicy : Always image : <<acr-name>>.azurecr.io/image/powershell-executor:491667 env : - name : GIT_REPO_URL value : https://github.com/DEFRA/adp-flux-services.git - name : GIT_BRANCH value : main - name : SCRIPT_FILE_NAME value : common/scripts/access-control/flexible-server/<<powershell-script-filename>> - name : POSTGRES_HOST value : <<postgres-server>>.postgres.database.azure.com - name : POSTGRES_DATABASE value : <<postgres-db>> ... resources : requests : cpu : 100m memory : 250Mi limits : cpu : 500m memory : 600Mi volumeMounts : - name : custom-root-ca readOnly : true mountPath : /etc/ssl/certs/defra-egress-firewall-cert-01.crt subPath : defra-egress-firewall-cert-01.crt backoffLimit : 2","title":"Job template"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#microservice-infra-helm-chart-tbd","text":"The above job template can be easily included in the application helm chart templates with the below code. 1 { { - include \"adp-aso-helm-library.delegate-db-access\" . - } } In the values.yaml for infra helm chart, the delegator service would add a label/flag to denote that the db can be provided delegate permissions. By default, delegate will be set to 'no' . 1 2 3 4 5 6 postgres : db : name : claim delegate : 'yes' --> optional charset : UTF8 collation : en_US.utf8 On the other hand, the delegatee service would have the delegated parameter set to 'yes' . Setting delegated parameter ensures that a new database won't be created and it will provide roleassignments on the existing database. This will require roleassignment to be set to read or readwrite and ignore any other parameters set for the database in values.yaml. 1 2 3 4 5 6 postgres : db : name : claim delegated : 'yes' roleAssignments : - roleName : 'readwrite' --> required if delegated is set to 'yes'","title":"Microservice Infra Helm Chart (TBD)"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#job-powershell-script-tbd","text":"Validate: The powershell script needs to validate the below before providing necessary access to the database. The database should have been set to delegate by the delegator service to enable sharing access. Check if the delegatee service name is listed by the delegator service to share access to db. Grant access: Runs a set of SQL scripts on the postgres db with the team's postgres-db-writer AD group to create delegatee service role and grant necessary access.","title":"Job - Powershell script (TBD)"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#validate","text":"Powershell script to check if the database is delegatable and the service is authorized by delegator service to allow granting access.","title":"Validate"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#grant-access","text":"","title":"Grant Access"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#create-role","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 DO $$ BEGIN IF NOT EXISTS ( SELECT 1 FROM pgaadauth_list_principals ( false ) WHERE rolname = '<<delegatee-service-MI-name>>' ) THEN RAISE NOTICE 'CREATING PRINCIPAL FOR MANAGED IDENTITY:<<delegatee-service-MI-name>>' ; PERFORM pgaadauth_create_principal ( '<<delegatee-service-MI-name>>' , false , false ); RAISE NOTICE 'PRINCIPAL FOR MANAGED IDENTITY CREATED:<<delegatee-service-MI-name>>' ; ELSE RAISE NOTICE 'PRINCIPAL FOR MANAGED IDENTITY ALREADY EXISTS:<<delegatee-service-MI-name>>' ; END IF ; EXECUTE ( 'GRANT CONNECT ON DATABASE \"crai-mcu-knowledge\" TO \"<<delegatee-service-MI-name>>\"' ); RAISE NOTICE 'GRANTED CONNECT TO DATABASE' ; EXCEPTION WHEN OTHERS THEN RAISE EXCEPTION 'ERROR DURING PRINCIPAL CREATION/GRANT CONNECT: %' , SQLERRM ; END $$","title":"Create Role"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#reader-perms","text":"1 2 3 4 GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; GRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO \"<<delegatee-service-MI-name>>\" ; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON SEQUENCES TO \"<<delegatee-service-MI-name>>\" ;","title":"Reader perms"},{"location":"How-to-guides/Platform-Services/how-to-delegate-db-access-between-services/#writer-perms","text":"1 2 3 4 5 6 7 8 GRANT CREATE , USAGE ON SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; GRANT SELECT , UPDATE , INSERT , REFERENCES , TRIGGER ON ALL TABLES IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; GRANT SELECT , UPDATE , USAGE ON ALL SEQUENCES IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; GRANT EXECUTE ON ALL PROCEDURES IN SCHEMA public TO \"<<delegatee-service-MI-name>>\" ; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT , UPDATE , INSERT , REFERENCES , TRIGGER ON TABLES TO \"<<delegatee-service-MI-name>>\" ; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT , USAGE ON SEQUENCES TO \"<<delegatee-service-MI-name>>\" ; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT EXECUTE ON FUNCTIONS TO \"<<delegatee-service-MI-name>>\" ;","title":"Writer perms"},{"location":"How-to-guides/Platform-Services/how-to-deploy-a-platform-service/","text":"How to deploy a Platform Service \u00b6 In this how to guide you will learn how to build, deploy, and monitor a Platform service (Web App, User Interface, API etc) for your team. It includes information about Pipelines specifically and how the ADP Backstage Portal supports this. Prerequisites \u00b6 Before building and deploying a service, you will first need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project Overview \u00b6 By completing this guide, you will have completed these actions: Learned how to build and run a CI pipeline for your service. Learned how to deploy your platform service to an environment. Learned how to monitor your platform service, and check the status of it Understand how ADP uses Azure DevOps Pipelines for builds and deployments. How to customize your pipeline to your needs and where you can find it. How the ADP Portal supports you Guide \u00b6 How do I find my service\u2019s CI/CD pipeline? \u00b6 All pipelines in ADP are created in your projects/programmes Azure DevOps project. This is specific to your team. It\u2019s the one you chose on your scaffolder creation of a service. We use YAML Azure Pipelines and Defra GitHub to store all code. Pipelines are mapped 1-1 per microservice, and can deploy the Web App, Infra, App Configuration and Database schema together as an immutable unit. In your scaffolded repository: Once you have scaffolded your project/service , you will have a \u2018 build.yaml \u2019 in an .azureDevOps folder in your microservice repository in GitHub. This is your pipeline instantiation. In your chosen Azure DevOps project: On the Pipelines page, either at the root level or under a Pipelines Folder name you defined/chose, you will find your Pipeline. Your pipeline is convention based including naming. Your pipeline will be called the same name as your repository: <your-service-name> . E.g: <projectcode>-<servicename>-api Above image an example of a Pipeline scaffolded called \u2018 adp-demo99 \u2019 in the DEMO folder. Can I find this in the ADP Portal? Yes! Simply go to your components page that you scaffolded/created via the ADP Portal, and click on the CI/CD tab, which will give you information on your pipeline, and will link off to the exact location. How do I run my service pipelines? \u00b6 We promote continuous integration (CI) and continuous delivery (CD). Your pipeline will trigger (run the CI build) automatically on any change to the \u2018main\u2019 branch, or any feature branch you create and anytime you check-in. This includes PR branches. You simply run your pipeline from the ADO Pipelines interface by clicking \u2018Run pipeline\u2019. You can: Just run a CI build on check-in on feature branches \u2013 default approach. Run a CI build and an automatic deployment into Sandpit/Dev from a feature branch. Run a CI from the main branch and promote your code from development to production. Customise your CI, PR and build triggers, deploy configuration only or the full app, in your services \u2018build.yaml\u2019. Run your pipeline manually/on-demand, by selecting \u2018Run pipeline\u2019 blue-button on the top-right hand corner of your Azure Pipelines project page. You can select any feature and main branches and customise any flags for deployment. You can run from Commit ID and from GitHub Release Tag. You can override variables if required. Are there any requirements for my CI pipeline to run? \u00b6 You must update your App Version using sematic versioning, at least once. This is an update in your \u2018package.json\u2019 or your .\u2019csproj\u2019 file to the Major, Minor or Patch. You must be building a NodeJs or C# app that is containerised with a HELM Chart. Pipeline documentation and parameters and configuration options can be found here . Above image of pipeline run example. How do I customise my CI/CD pipelines? \u00b6 You can change some basic functionality of your pipeline. A lot of it is defined for you in a convention-based manner, including the running of unit tests, reporting, environments that are available etc, and some are selectable, such as build of .NET or NodeJS apps, location of test files, PR and CI triggers, and the parameters to deploy configuration only or automatic deploy on every feature build. Full details can be found on the Pipelines documentation GitHub page. Above image is an example of what can be changed in terms of Pipeline Parameters (triggers, deployment types, paths to include/exclude). The below image is an example of what can be changed. You can change things like your config locations, test paths, what ADO Secret variable groups you wish to import, what App Framework (Node or C#) etc. What does my progression look like through environments? \u00b6 To promote your code through environments, you can use the Azure Pipelines user interface for your team/project to either: Allow automated progression via automated checks or and continuous delivery or, Manually push/promote the code to environments on demand. Your environments and any default gates or checks will be automatically plotted for you. This is an example of a full pipeline run. You can select, based on the Platform route-to-live documentation, which environments you promote code to. You don\u2019t need to go to all environments to go live. This is an example of a waiting \u2018stage\u2019 which is an environment: To promote code, you can select \u2018Review\u2019 in the top-right hand corner and click approve . Full Azure Pipelines documentation can be found here. What ways can I monitor my pipelines and service, and the steps that are run? \u00b6 Every pipeline run includes steps such as unit tests, integration tests, acceptance tests, app builds, code linting, static code analysis including Sonar Cloud, OWASP checks, performance testing capability, container/app scanning with Snyk etc. We report out metrics in Azure DevOps Pipelines user interface for your project and service, for things like Unit Test coverage, test passes and failures, and any step failures. Full details are covered in Pipelines documentation. Details can also be found in your projects Snyk or Sonar Cloud report. From the ADP Backstage Portal , you can find the following information for all environments: Deployments and HELM release status, including app versions and which environments. Services deployed into AKS (Kubernetes) and their health status and stats. Any health status codes, errors, and issues Any log outputs for running services, including any errors. Last reconciliation (deployment) date and time via FluxCD, and any successes or failures Debug and log information Overview of Pipeline runs in ADO. Pull Requests open/closed etc. Access to Grafana and Dashboards, to monitor and maintain your service: Includes Prometheus logs, status, errors, consumption etc. Any service documentation for your app Any dependencies for your apps, and how it links to other projects, programmes, and services. Any API specifications (Open API) and associated components Key links for your service \u2013 GitHub, ADO, Snyk, Sonar Cloud, Azure Portal, Dashboards, your Service\u2019s frontend URL etc. The portal is fully self-service. And each component deployed details the above. You should use the ADP Portal to monitor, manage and view data about your service if it isn\u2019t included in your Pipeline run.","title":"How to deploy a platform service"},{"location":"How-to-guides/Platform-Services/how-to-deploy-a-platform-service/#how-to-deploy-a-platform-service","text":"In this how to guide you will learn how to build, deploy, and monitor a Platform service (Web App, User Interface, API etc) for your team. It includes information about Pipelines specifically and how the ADP Backstage Portal supports this.","title":"How to deploy a Platform Service"},{"location":"How-to-guides/Platform-Services/how-to-deploy-a-platform-service/#prerequisites","text":"Before building and deploying a service, you will first need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project","title":"Prerequisites"},{"location":"How-to-guides/Platform-Services/how-to-deploy-a-platform-service/#overview","text":"By completing this guide, you will have completed these actions: Learned how to build and run a CI pipeline for your service. Learned how to deploy your platform service to an environment. Learned how to monitor your platform service, and check the status of it Understand how ADP uses Azure DevOps Pipelines for builds and deployments. How to customize your pipeline to your needs and where you can find it. How the ADP Portal supports you","title":"Overview"},{"location":"How-to-guides/Platform-Services/how-to-deploy-a-platform-service/#guide","text":"","title":"Guide"},{"location":"How-to-guides/Platform-Services/how-to-deploy-a-platform-service/#how-do-i-find-my-services-cicd-pipeline","text":"All pipelines in ADP are created in your projects/programmes Azure DevOps project. This is specific to your team. It\u2019s the one you chose on your scaffolder creation of a service. We use YAML Azure Pipelines and Defra GitHub to store all code. Pipelines are mapped 1-1 per microservice, and can deploy the Web App, Infra, App Configuration and Database schema together as an immutable unit. In your scaffolded repository: Once you have scaffolded your project/service , you will have a \u2018 build.yaml \u2019 in an .azureDevOps folder in your microservice repository in GitHub. This is your pipeline instantiation. In your chosen Azure DevOps project: On the Pipelines page, either at the root level or under a Pipelines Folder name you defined/chose, you will find your Pipeline. Your pipeline is convention based including naming. Your pipeline will be called the same name as your repository: <your-service-name> . E.g: <projectcode>-<servicename>-api Above image an example of a Pipeline scaffolded called \u2018 adp-demo99 \u2019 in the DEMO folder. Can I find this in the ADP Portal? Yes! Simply go to your components page that you scaffolded/created via the ADP Portal, and click on the CI/CD tab, which will give you information on your pipeline, and will link off to the exact location.","title":"How do I find my service\u2019s CI/CD pipeline?"},{"location":"How-to-guides/Platform-Services/how-to-deploy-a-platform-service/#how-do-i-run-my-service-pipelines","text":"We promote continuous integration (CI) and continuous delivery (CD). Your pipeline will trigger (run the CI build) automatically on any change to the \u2018main\u2019 branch, or any feature branch you create and anytime you check-in. This includes PR branches. You simply run your pipeline from the ADO Pipelines interface by clicking \u2018Run pipeline\u2019. You can: Just run a CI build on check-in on feature branches \u2013 default approach. Run a CI build and an automatic deployment into Sandpit/Dev from a feature branch. Run a CI from the main branch and promote your code from development to production. Customise your CI, PR and build triggers, deploy configuration only or the full app, in your services \u2018build.yaml\u2019. Run your pipeline manually/on-demand, by selecting \u2018Run pipeline\u2019 blue-button on the top-right hand corner of your Azure Pipelines project page. You can select any feature and main branches and customise any flags for deployment. You can run from Commit ID and from GitHub Release Tag. You can override variables if required.","title":"How do I run my service pipelines?"},{"location":"How-to-guides/Platform-Services/how-to-deploy-a-platform-service/#are-there-any-requirements-for-my-ci-pipeline-to-run","text":"You must update your App Version using sematic versioning, at least once. This is an update in your \u2018package.json\u2019 or your .\u2019csproj\u2019 file to the Major, Minor or Patch. You must be building a NodeJs or C# app that is containerised with a HELM Chart. Pipeline documentation and parameters and configuration options can be found here . Above image of pipeline run example.","title":"Are there any requirements for my CI pipeline to run?"},{"location":"How-to-guides/Platform-Services/how-to-deploy-a-platform-service/#how-do-i-customise-my-cicd-pipelines","text":"You can change some basic functionality of your pipeline. A lot of it is defined for you in a convention-based manner, including the running of unit tests, reporting, environments that are available etc, and some are selectable, such as build of .NET or NodeJS apps, location of test files, PR and CI triggers, and the parameters to deploy configuration only or automatic deploy on every feature build. Full details can be found on the Pipelines documentation GitHub page. Above image is an example of what can be changed in terms of Pipeline Parameters (triggers, deployment types, paths to include/exclude). The below image is an example of what can be changed. You can change things like your config locations, test paths, what ADO Secret variable groups you wish to import, what App Framework (Node or C#) etc.","title":"How do I customise my CI/CD pipelines?"},{"location":"How-to-guides/Platform-Services/how-to-deploy-a-platform-service/#what-does-my-progression-look-like-through-environments","text":"To promote your code through environments, you can use the Azure Pipelines user interface for your team/project to either: Allow automated progression via automated checks or and continuous delivery or, Manually push/promote the code to environments on demand. Your environments and any default gates or checks will be automatically plotted for you. This is an example of a full pipeline run. You can select, based on the Platform route-to-live documentation, which environments you promote code to. You don\u2019t need to go to all environments to go live. This is an example of a waiting \u2018stage\u2019 which is an environment: To promote code, you can select \u2018Review\u2019 in the top-right hand corner and click approve . Full Azure Pipelines documentation can be found here.","title":"What does my progression look like through environments?"},{"location":"How-to-guides/Platform-Services/how-to-deploy-a-platform-service/#what-ways-can-i-monitor-my-pipelines-and-service-and-the-steps-that-are-run","text":"Every pipeline run includes steps such as unit tests, integration tests, acceptance tests, app builds, code linting, static code analysis including Sonar Cloud, OWASP checks, performance testing capability, container/app scanning with Snyk etc. We report out metrics in Azure DevOps Pipelines user interface for your project and service, for things like Unit Test coverage, test passes and failures, and any step failures. Full details are covered in Pipelines documentation. Details can also be found in your projects Snyk or Sonar Cloud report. From the ADP Backstage Portal , you can find the following information for all environments: Deployments and HELM release status, including app versions and which environments. Services deployed into AKS (Kubernetes) and their health status and stats. Any health status codes, errors, and issues Any log outputs for running services, including any errors. Last reconciliation (deployment) date and time via FluxCD, and any successes or failures Debug and log information Overview of Pipeline runs in ADO. Pull Requests open/closed etc. Access to Grafana and Dashboards, to monitor and maintain your service: Includes Prometheus logs, status, errors, consumption etc. Any service documentation for your app Any dependencies for your apps, and how it links to other projects, programmes, and services. Any API specifications (Open API) and associated components Key links for your service \u2013 GitHub, ADO, Snyk, Sonar Cloud, Azure Portal, Dashboards, your Service\u2019s frontend URL etc. The portal is fully self-service. And each component deployed details the above. You should use the ADP Portal to monitor, manage and view data about your service if it isn\u2019t included in your Pipeline run.","title":"What ways can I monitor my pipelines and service, and the steps that are run?"},{"location":"How-to-guides/Testing/how-to-create-acceptance-test/","text":"How to acceptance test \u00b6 In this how to guide you will learn how to create, deploy, and run an acceptance test for a Platform service (Frontend Web App or an API) for your team. Prerequisites \u00b6 Before adding acceptance tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project Overview \u00b6 By completing this guide, you will have completed these actions: Learned how to add an acceptance test for your service. Learned how to run an acceptance test locally. How to customize your pipeline to run an acceptace tests based on tags for different env. Guide \u00b6 These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. Note The pipeline will check for the existence of the file test\\acceptance\\docker-compose.yaml to determine if acceptance tests have been defined. How to add acceptance test for your service? \u00b6 You may add tags to features and scenarios. There are no restrictions on the name of the tag. Recommended tags include the following: @sanity, @smoke, @regression refer If custom tags are defined, then the pipeline should be customized to run those tests as detailed in following sections. How to run acceptance test locally? \u00b6 Set required tags, default is empty string which will run all tests \u00b6 pwsh : $ENV:TEST_TAGS = \"@sanity or @smoke\" shell: export TEST_TAGS = \"@sanity or @smoke\" Run the acceptance test script under scripts folder within the repo \u00b6 1 2 3 docker-compose up -d cd test/acceptance docker-compose run --rm wdio-cucumber How to customize your pipeline to run acceptace tests? \u00b6 Note Every pipeline run includes steps to run various post deployment tests. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. You can customize the tags and environments where you would like to run specific features or scenarios of acceptance test 1 2 3 4 5 6 7 8 9 10 postDeployTest : testEnvs : performanceTests : snd4, pre1 accessibilityTests : snd4, tst1, acceptanceTests : - env : snd4 tags : '@demotag' - env : dev1 tags : '@sanity or @smoke' envToTest : snd4,dev1,tst1,pre1 If not defined, the pipeline will run with following default settings. 1 2 3 4 5 6 7 8 9 10 11 12 postDeployTest : testEnvs : performanceTests : snd4, pre1 accessibilityTests : snd4, dev1, tst1 acceptanceTests : - env : snd4 tags : '@sanity or @smoke' - env : dev1 tags : '@smoke' - env : tst1 tags : '@smoke or @regression' envToTest : snd4,dev1,tst1,pre1 Please refer ffc-demo-web pipeline: Test execution reports will be available via Azure DevOps Pipelines user interface for your project and service. How to disable test? \u00b6 if you want to disable the test for any reason please refer Disable Test","title":"How to create an Acceptance test"},{"location":"How-to-guides/Testing/how-to-create-acceptance-test/#how-to-acceptance-test","text":"In this how to guide you will learn how to create, deploy, and run an acceptance test for a Platform service (Frontend Web App or an API) for your team.","title":"How to acceptance test"},{"location":"How-to-guides/Testing/how-to-create-acceptance-test/#prerequisites","text":"Before adding acceptance tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project","title":"Prerequisites"},{"location":"How-to-guides/Testing/how-to-create-acceptance-test/#overview","text":"By completing this guide, you will have completed these actions: Learned how to add an acceptance test for your service. Learned how to run an acceptance test locally. How to customize your pipeline to run an acceptace tests based on tags for different env.","title":"Overview"},{"location":"How-to-guides/Testing/how-to-create-acceptance-test/#guide","text":"These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. Note The pipeline will check for the existence of the file test\\acceptance\\docker-compose.yaml to determine if acceptance tests have been defined.","title":"Guide"},{"location":"How-to-guides/Testing/how-to-create-acceptance-test/#how-to-add-acceptance-test-for-your-service","text":"You may add tags to features and scenarios. There are no restrictions on the name of the tag. Recommended tags include the following: @sanity, @smoke, @regression refer If custom tags are defined, then the pipeline should be customized to run those tests as detailed in following sections.","title":"How to add acceptance test for your service?"},{"location":"How-to-guides/Testing/how-to-create-acceptance-test/#how-to-run-acceptance-test-locally","text":"","title":"How to run acceptance test locally?"},{"location":"How-to-guides/Testing/how-to-create-acceptance-test/#set-required-tags-default-is-empty-string-which-will-run-all-tests","text":"pwsh : $ENV:TEST_TAGS = \"@sanity or @smoke\" shell: export TEST_TAGS = \"@sanity or @smoke\"","title":"Set required tags, default is empty string which will run all tests"},{"location":"How-to-guides/Testing/how-to-create-acceptance-test/#run-the-acceptance-test-script-under-scripts-folder-within-the-repo","text":"1 2 3 docker-compose up -d cd test/acceptance docker-compose run --rm wdio-cucumber","title":"Run the acceptance test script under scripts folder within the repo"},{"location":"How-to-guides/Testing/how-to-create-acceptance-test/#how-to-customize-your-pipeline-to-run-acceptace-tests","text":"Note Every pipeline run includes steps to run various post deployment tests. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. You can customize the tags and environments where you would like to run specific features or scenarios of acceptance test 1 2 3 4 5 6 7 8 9 10 postDeployTest : testEnvs : performanceTests : snd4, pre1 accessibilityTests : snd4, tst1, acceptanceTests : - env : snd4 tags : '@demotag' - env : dev1 tags : '@sanity or @smoke' envToTest : snd4,dev1,tst1,pre1 If not defined, the pipeline will run with following default settings. 1 2 3 4 5 6 7 8 9 10 11 12 postDeployTest : testEnvs : performanceTests : snd4, pre1 accessibilityTests : snd4, dev1, tst1 acceptanceTests : - env : snd4 tags : '@sanity or @smoke' - env : dev1 tags : '@smoke' - env : tst1 tags : '@smoke or @regression' envToTest : snd4,dev1,tst1,pre1 Please refer ffc-demo-web pipeline: Test execution reports will be available via Azure DevOps Pipelines user interface for your project and service.","title":"How to customize your pipeline to run acceptace tests?"},{"location":"How-to-guides/Testing/how-to-create-acceptance-test/#how-to-disable-test","text":"if you want to disable the test for any reason please refer Disable Test","title":"How to disable test?"},{"location":"How-to-guides/Testing/how-to-create-accessibility-test/","text":"How to Accessibility test \u00b6 In this how to guide you will learn how to create, deploy, and run an Accessibility test for a Platform service (Web App, User Interface etc) for your team. Prerequisites \u00b6 Before adding tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project Overview \u00b6 By completing this guide, you will have completed these actions: Learned how to add an Accessibility test for your service. Learned how to run an Accessibility test locally. How to customize your pipeline to run Accessibility tests for different env. Guide \u00b6 Note Every pipeline run includes steps to run varoious tests pre deployment and post deployment. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. The pipeline will check for the existence of the file .docker-compose.axe.yaml to determine if Accessibility tests have been defined. How to add an Accessibility test for your service? \u00b6 Refer to the ffc-demo-web example . Requirments for local development \u00b6 Docker Desktop 2.2.0.3 (42716) or higher How to run an Accessibility test locally? \u00b6 Executre the above commands in bash or PowerShell 1 2 # this will execute the docker-compose at the root folder to create an instance of the service and its dependences docker-compose -f docker-compose . yaml -f docker-compose . axe . yaml run - -rm axe How to customize your pipeline to run Accessibility tests? \u00b6 You can customize the environments where you would like to run Accessibility test (within the pipeline it is referred as integration test) 1 2 3 4 5 postDeployTest : envToTest : 'snd4, dev1' testEnvs : accessibilityTests : 'snd4, dev1' testsToRun : 'accessibility' If not defined, the pipeline will run with following default settings. 1 2 3 4 5 postDeployTest : envToTest : 'snd4, dev1, tst1, pre1' testEnvs : accessibilityTests : 'snd4' testsToRun : 'owasp;accessibility;performance;service-acceptance;acceptance' Please refer ffc-demo-web pipeline: How to disable test? \u00b6 if you want to disable the test for any reason please refer Disable Test","title":"How to create an Accessibility test"},{"location":"How-to-guides/Testing/how-to-create-accessibility-test/#how-to-accessibility-test","text":"In this how to guide you will learn how to create, deploy, and run an Accessibility test for a Platform service (Web App, User Interface etc) for your team.","title":"How to Accessibility test"},{"location":"How-to-guides/Testing/how-to-create-accessibility-test/#prerequisites","text":"Before adding tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project","title":"Prerequisites"},{"location":"How-to-guides/Testing/how-to-create-accessibility-test/#overview","text":"By completing this guide, you will have completed these actions: Learned how to add an Accessibility test for your service. Learned how to run an Accessibility test locally. How to customize your pipeline to run Accessibility tests for different env.","title":"Overview"},{"location":"How-to-guides/Testing/how-to-create-accessibility-test/#guide","text":"Note Every pipeline run includes steps to run varoious tests pre deployment and post deployment. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. The pipeline will check for the existence of the file .docker-compose.axe.yaml to determine if Accessibility tests have been defined.","title":"Guide"},{"location":"How-to-guides/Testing/how-to-create-accessibility-test/#how-to-add-an-accessibility-test-for-your-service","text":"Refer to the ffc-demo-web example .","title":"How to add an Accessibility test for your service?"},{"location":"How-to-guides/Testing/how-to-create-accessibility-test/#requirments-for-local-development","text":"Docker Desktop 2.2.0.3 (42716) or higher","title":"Requirments for local development"},{"location":"How-to-guides/Testing/how-to-create-accessibility-test/#how-to-run-an-accessibility-test-locally","text":"Executre the above commands in bash or PowerShell 1 2 # this will execute the docker-compose at the root folder to create an instance of the service and its dependences docker-compose -f docker-compose . yaml -f docker-compose . axe . yaml run - -rm axe","title":"How to run an Accessibility test locally?"},{"location":"How-to-guides/Testing/how-to-create-accessibility-test/#how-to-customize-your-pipeline-to-run-accessibility-tests","text":"You can customize the environments where you would like to run Accessibility test (within the pipeline it is referred as integration test) 1 2 3 4 5 postDeployTest : envToTest : 'snd4, dev1' testEnvs : accessibilityTests : 'snd4, dev1' testsToRun : 'accessibility' If not defined, the pipeline will run with following default settings. 1 2 3 4 5 postDeployTest : envToTest : 'snd4, dev1, tst1, pre1' testEnvs : accessibilityTests : 'snd4' testsToRun : 'owasp;accessibility;performance;service-acceptance;acceptance' Please refer ffc-demo-web pipeline:","title":"How to customize your pipeline to run Accessibility tests?"},{"location":"How-to-guides/Testing/how-to-create-accessibility-test/#how-to-disable-test","text":"if you want to disable the test for any reason please refer Disable Test","title":"How to disable test?"},{"location":"How-to-guides/Testing/how-to-create-contract-test/","text":"How to contract test \u00b6 In this how to guide you will learn how to create, deploy, and run a contract test for a Platform service (Web App, User Interface etc) for your team. Prerequisites \u00b6 Before adding tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project Overview \u00b6 By completing this guide, you will have completed these actions: Learned how to add a contract test for your service. Learned how to run a contract test locally. How to customize your pipeline to run contract tests for different env. Guide \u00b6 Note Every pipeline run includes steps to run varoious tests pre deployment and post deployment. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. The pipeline will check for the existence of the file ./docker-compose.contract.test.yaml to determine if contract tests have been defined. How to add a contract test for your service? \u00b6 The contract Test scripts should be added to the test\\contract folder in the GitHub repository of the service. Refer to the ffc-demo-web example . This folder should contain *.test.js files which are required to execute the tests. Tests will be run using PACT BROKER service. Requirments for local development \u00b6 Docker Desktop 2.2.0.3 (42716) or higher How to run a contract test locally? \u00b6 Executre the above commands in bash or PowerShell 1 2 # this will execute the docker-compose at the root folder to create an instance of the service and its dependences docker-compose -f docker-compose . yaml -f docker-compose . contract . test . yaml -p \"<<servicename>>-test\" up How to parameterising your Tests \u00b6 Set required PACT Broker credentials to run tests \u00b6 1 2 3 export PACT_BROKER_URL = \"PACT Broker URL\" export PACT_BROKER_USERNAME = \"PACT Broker username\" export PACT_BROKER_PASSWORD = \"PACT Broker password\" How to customize your pipeline to run contract tests? \u00b6 You can customize the environments where you would like to run contract test (within the pipeline it is referred as integration test) 1 2 3 4 5 postDeployTest : envToTest : snd4 testEnvs : integrationTests : snd4 testsToRun : 'contract' Please refer ffc-demo-web pipeline: How to disable test? \u00b6 if you want to disable the test for any reason please refer Disable Test","title":"How to create a Contract test"},{"location":"How-to-guides/Testing/how-to-create-contract-test/#how-to-contract-test","text":"In this how to guide you will learn how to create, deploy, and run a contract test for a Platform service (Web App, User Interface etc) for your team.","title":"How to contract test"},{"location":"How-to-guides/Testing/how-to-create-contract-test/#prerequisites","text":"Before adding tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project","title":"Prerequisites"},{"location":"How-to-guides/Testing/how-to-create-contract-test/#overview","text":"By completing this guide, you will have completed these actions: Learned how to add a contract test for your service. Learned how to run a contract test locally. How to customize your pipeline to run contract tests for different env.","title":"Overview"},{"location":"How-to-guides/Testing/how-to-create-contract-test/#guide","text":"Note Every pipeline run includes steps to run varoious tests pre deployment and post deployment. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. The pipeline will check for the existence of the file ./docker-compose.contract.test.yaml to determine if contract tests have been defined.","title":"Guide"},{"location":"How-to-guides/Testing/how-to-create-contract-test/#how-to-add-a-contract-test-for-your-service","text":"The contract Test scripts should be added to the test\\contract folder in the GitHub repository of the service. Refer to the ffc-demo-web example . This folder should contain *.test.js files which are required to execute the tests. Tests will be run using PACT BROKER service.","title":"How to add a contract test for your service?"},{"location":"How-to-guides/Testing/how-to-create-contract-test/#requirments-for-local-development","text":"Docker Desktop 2.2.0.3 (42716) or higher","title":"Requirments for local development"},{"location":"How-to-guides/Testing/how-to-create-contract-test/#how-to-run-a-contract-test-locally","text":"Executre the above commands in bash or PowerShell 1 2 # this will execute the docker-compose at the root folder to create an instance of the service and its dependences docker-compose -f docker-compose . yaml -f docker-compose . contract . test . yaml -p \"<<servicename>>-test\" up","title":"How to run a contract test locally?"},{"location":"How-to-guides/Testing/how-to-create-contract-test/#how-to-parameterising-your-tests","text":"","title":"How to parameterising your Tests"},{"location":"How-to-guides/Testing/how-to-create-contract-test/#set-required-pact-broker-credentials-to-run-tests","text":"1 2 3 export PACT_BROKER_URL = \"PACT Broker URL\" export PACT_BROKER_USERNAME = \"PACT Broker username\" export PACT_BROKER_PASSWORD = \"PACT Broker password\"","title":"Set required PACT Broker credentials to run tests"},{"location":"How-to-guides/Testing/how-to-create-contract-test/#how-to-customize-your-pipeline-to-run-contract-tests","text":"You can customize the environments where you would like to run contract test (within the pipeline it is referred as integration test) 1 2 3 4 5 postDeployTest : envToTest : snd4 testEnvs : integrationTests : snd4 testsToRun : 'contract' Please refer ffc-demo-web pipeline:","title":"How to customize your pipeline to run contract tests?"},{"location":"How-to-guides/Testing/how-to-create-contract-test/#how-to-disable-test","text":"if you want to disable the test for any reason please refer Disable Test","title":"How to disable test?"},{"location":"How-to-guides/Testing/how-to-create-integration-test/","text":"How to Integration test \u00b6 In this how to guide you will learn how to create, deploy, and run an Integration test for a Platform service (Web App, User Interface etc) for your team. Prerequisites \u00b6 Before adding tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project Overview \u00b6 By completing this guide, you will have completed these actions: Learned how to add an Integration test for your service. Learned how to run an Integration test locally. How to customize your pipeline to run Integration tests for different env. Guide \u00b6 Note Every pipeline run includes steps to run varoious tests pre deployment and post deployment. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. The pipeline will check for the existence of the file ./docker-compose.test.yaml to determine if integration tests have been defined. How to add an Integration test for your service? \u00b6 The Integration Test scripts should be added to the test\\integration folder in the GitHub repository of the service. Refer to the ffc-demo-web example . This folder should contain *.test.js files which are required to execute the tests. Tests will be run using PACT BROKER service. Requirments for local development \u00b6 Docker Desktop 2.2.0.3 (42716) or higher How to run an Integration test locally? \u00b6 Executre the above commands in bash or PowerShell 1 2 # this will execute the docker-compose at the root folder to create an instance of the service and its dependences docker-compose -f docker-compose . yaml -f docker-compose . test . yaml -p \"<<servicename>>-test\" up How to customize your pipeline to run Integration tests? \u00b6 You can customize the environments where you would like to run Integration test (within the pipeline it is referred as integration test) 1 2 3 4 5 postDeployTest : envToTest : snd4 testEnvs : integrationTests : snd4 testsToRun : 'integration' Please refer ffc-demo-web pipeline: How to disable test? \u00b6 if you want to disable the test for any reason please refer Disable Test","title":"How to create an Integration test"},{"location":"How-to-guides/Testing/how-to-create-integration-test/#how-to-integration-test","text":"In this how to guide you will learn how to create, deploy, and run an Integration test for a Platform service (Web App, User Interface etc) for your team.","title":"How to Integration test"},{"location":"How-to-guides/Testing/how-to-create-integration-test/#prerequisites","text":"Before adding tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project","title":"Prerequisites"},{"location":"How-to-guides/Testing/how-to-create-integration-test/#overview","text":"By completing this guide, you will have completed these actions: Learned how to add an Integration test for your service. Learned how to run an Integration test locally. How to customize your pipeline to run Integration tests for different env.","title":"Overview"},{"location":"How-to-guides/Testing/how-to-create-integration-test/#guide","text":"Note Every pipeline run includes steps to run varoious tests pre deployment and post deployment. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. The pipeline will check for the existence of the file ./docker-compose.test.yaml to determine if integration tests have been defined.","title":"Guide"},{"location":"How-to-guides/Testing/how-to-create-integration-test/#how-to-add-an-integration-test-for-your-service","text":"The Integration Test scripts should be added to the test\\integration folder in the GitHub repository of the service. Refer to the ffc-demo-web example . This folder should contain *.test.js files which are required to execute the tests. Tests will be run using PACT BROKER service.","title":"How to add an Integration test for your service?"},{"location":"How-to-guides/Testing/how-to-create-integration-test/#requirments-for-local-development","text":"Docker Desktop 2.2.0.3 (42716) or higher","title":"Requirments for local development"},{"location":"How-to-guides/Testing/how-to-create-integration-test/#how-to-run-an-integration-test-locally","text":"Executre the above commands in bash or PowerShell 1 2 # this will execute the docker-compose at the root folder to create an instance of the service and its dependences docker-compose -f docker-compose . yaml -f docker-compose . test . yaml -p \"<<servicename>>-test\" up","title":"How to run an Integration test locally?"},{"location":"How-to-guides/Testing/how-to-create-integration-test/#how-to-customize-your-pipeline-to-run-integration-tests","text":"You can customize the environments where you would like to run Integration test (within the pipeline it is referred as integration test) 1 2 3 4 5 postDeployTest : envToTest : snd4 testEnvs : integrationTests : snd4 testsToRun : 'integration' Please refer ffc-demo-web pipeline:","title":"How to customize your pipeline to run Integration tests?"},{"location":"How-to-guides/Testing/how-to-create-integration-test/#how-to-disable-test","text":"if you want to disable the test for any reason please refer Disable Test","title":"How to disable test?"},{"location":"How-to-guides/Testing/how-to-create-owasp-test/","text":"How to OWASP test \u00b6 In this how to guide you will learn how to create, deploy, and run a OWASP test for a Platform service (Web App, User Interface etc) for your team. Prerequisites \u00b6 Before adding tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project Overview \u00b6 By completing this guide, you will have completed these actions: Learned how to add a OWASP test for your service. Learned how to run a OWASP test locally. How to customize your pipeline to run OWASP tests for different env. Guide \u00b6 Note Every pipeline run includes steps to run varoious tests pre deployment and post deployment. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. The pipeline will check for the existence of the file .docker-compose.zap.yaml to determine if contract tests have been defined. How to add a OWASP test for your service? \u00b6 The OWASP Test Config should be added to the zap folder in the GitHub repository of the service. Refer to the ffc-demo-web example . This folder should contain zap.conf file which is required to execute the tests. Requirments for local development \u00b6 Docker Desktop 2.2.0.3 (42716) or higher How to run a OWASP test locally? \u00b6 Executre the above commands in bash or PowerShell 1 2 # this will execute the docker-compose at the root folder to create an instance of the service and its dependences docker-compose -f docker-compose . yaml -f docker-compose . zap . yaml run - -rm zap-baseline-scan How to customize your pipeline to run OWASP tests? \u00b6 You can customize the environments where you would like to run OWASP test (within the pipeline it is referred as integration test) 1 2 3 4 5 postDeployTest : envToTest : 'snd4, dev1' testEnvs : owaspTests : 'snd4, dev1' testsToRun : 'owasp' If not defined, the pipeline will run with following default settings. 1 2 3 4 5 postDeployTest : envToTest : 'snd4, dev1, tst1, pre1' testEnvs : owaspTests : 'snd4, dev1, tst1, pre1' testsToRun : 'owasp;accessibility;performance;service-acceptance;acceptance;contract;integration' Please refer ffc-demo-web pipeline: How to disable test? \u00b6 if you want to disable the test for any reason please refer Disable Test","title":"How to create an OWASP test"},{"location":"How-to-guides/Testing/how-to-create-owasp-test/#how-to-owasp-test","text":"In this how to guide you will learn how to create, deploy, and run a OWASP test for a Platform service (Web App, User Interface etc) for your team.","title":"How to OWASP test"},{"location":"How-to-guides/Testing/how-to-create-owasp-test/#prerequisites","text":"Before adding tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project","title":"Prerequisites"},{"location":"How-to-guides/Testing/how-to-create-owasp-test/#overview","text":"By completing this guide, you will have completed these actions: Learned how to add a OWASP test for your service. Learned how to run a OWASP test locally. How to customize your pipeline to run OWASP tests for different env.","title":"Overview"},{"location":"How-to-guides/Testing/how-to-create-owasp-test/#guide","text":"Note Every pipeline run includes steps to run varoious tests pre deployment and post deployment. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. The pipeline will check for the existence of the file .docker-compose.zap.yaml to determine if contract tests have been defined.","title":"Guide"},{"location":"How-to-guides/Testing/how-to-create-owasp-test/#how-to-add-a-owasp-test-for-your-service","text":"The OWASP Test Config should be added to the zap folder in the GitHub repository of the service. Refer to the ffc-demo-web example . This folder should contain zap.conf file which is required to execute the tests.","title":"How to add a OWASP test for your service?"},{"location":"How-to-guides/Testing/how-to-create-owasp-test/#requirments-for-local-development","text":"Docker Desktop 2.2.0.3 (42716) or higher","title":"Requirments for local development"},{"location":"How-to-guides/Testing/how-to-create-owasp-test/#how-to-run-a-owasp-test-locally","text":"Executre the above commands in bash or PowerShell 1 2 # this will execute the docker-compose at the root folder to create an instance of the service and its dependences docker-compose -f docker-compose . yaml -f docker-compose . zap . yaml run - -rm zap-baseline-scan","title":"How to run a OWASP test locally?"},{"location":"How-to-guides/Testing/how-to-create-owasp-test/#how-to-customize-your-pipeline-to-run-owasp-tests","text":"You can customize the environments where you would like to run OWASP test (within the pipeline it is referred as integration test) 1 2 3 4 5 postDeployTest : envToTest : 'snd4, dev1' testEnvs : owaspTests : 'snd4, dev1' testsToRun : 'owasp' If not defined, the pipeline will run with following default settings. 1 2 3 4 5 postDeployTest : envToTest : 'snd4, dev1, tst1, pre1' testEnvs : owaspTests : 'snd4, dev1, tst1, pre1' testsToRun : 'owasp;accessibility;performance;service-acceptance;acceptance;contract;integration' Please refer ffc-demo-web pipeline:","title":"How to customize your pipeline to run OWASP tests?"},{"location":"How-to-guides/Testing/how-to-create-owasp-test/#how-to-disable-test","text":"if you want to disable the test for any reason please refer Disable Test","title":"How to disable test?"},{"location":"How-to-guides/Testing/how-to-create-performance-test/","text":"How to performance test \u00b6 In this how to guide you will learn how to create, deploy, and run a performance test for a Platform service (Web App, User Interface etc) for your team. Prerequisites \u00b6 Before adding performance tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project Overview \u00b6 By completing this guide, you will have completed these actions: Learned how to add a performance test for your service. Learned how to run a performance test locally. How to customize your pipeline to run performance tests for different env. Guide \u00b6 Note Every pipeline run includes steps to run varoious tests pre deployment and post deployment. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. The pipeline will check for the existence of the file test\\performance\\docker-compose.jmeter.yaml to determine if performance tests have been defined. How to add a performance test for your service? \u00b6 The Performance Test scripts should be added to the test\\performance folder in the GitHub repository of the service. Refer to the ffc-demo-web example . This folder should contain a docker-compose.jmeter.yaml file is used to build up the docker containers required to execute the tests. As a minimum, this will create a JMeter container and optionally create Selenium Grid containers. Using BrowserStack is preferred to running the tests using Selenium Grid hosted in Docker containers because you get better performance and scalability as the test load increases. Requirments for local development \u00b6 Docker Desktop 2.2.0.3 (42716) or higher JMeter v5.5 or above How to run a performance test locally? \u00b6 Executre the above commands in bash or PowerShell 1 2 3 4 5 cd test / performance # this will execute the docker-compose at the root folder to create an instance of the service and its dependences # and then it will create the performance testing containers (JMeter and any other containers specified in docker-compose.jmeter.yaml) docker-compose -f ../../ docker-compose . yaml -f docker-compose . jmeter . yaml run jmeter-test How to parameterising your Tests \u00b6 You can modify the number of virtual users, loop count and ramp-up duration by changing the settings in the file perf-test.properties. 1 2 3 4 5 6 7 # Sample user.properties file #--------------------------------------------------------------------------- # Properties added to manage noThreads rampUp lCount values #--------------------------------------------------------------------------- noThreads=15 rampUp=1 lCount=2 You can then reference these variables in your JMeter Script. How to customize your pipeline to run performance tests? \u00b6 You can customize the environments where you would like to run specific features or scenarios of performance test 1 2 3 4 postDeployTest : testEnvs : performanceTests : pre1 envToTest : snd4,dev1,tst1,pre1 if not defined, the pipeline will run with following default settings 1 2 3 4 postDeployTest : testEnvs : performanceTests : snd4, pre1 envToTest : snd4,dev1,tst1,pre1 Please refer ffc-demo-web pipeline: How to disable test? \u00b6 if you want to disable the test for any reason please refer Disable Test","title":"How to create a Performance test"},{"location":"How-to-guides/Testing/how-to-create-performance-test/#how-to-performance-test","text":"In this how to guide you will learn how to create, deploy, and run a performance test for a Platform service (Web App, User Interface etc) for your team.","title":"How to performance test"},{"location":"How-to-guides/Testing/how-to-create-performance-test/#prerequisites","text":"Before adding performance tests for your service, you will need to ensure that: Onboarded delivery project on to ADP Created a Platform Service for your team/delivery project","title":"Prerequisites"},{"location":"How-to-guides/Testing/how-to-create-performance-test/#overview","text":"By completing this guide, you will have completed these actions: Learned how to add a performance test for your service. Learned how to run a performance test locally. How to customize your pipeline to run performance tests for different env.","title":"Overview"},{"location":"How-to-guides/Testing/how-to-create-performance-test/#guide","text":"Note Every pipeline run includes steps to run varoious tests pre deployment and post deployment. These tests may include unit, integration, acceptance, performance, accessibilty etc as long as they are defined for the service. The pipeline will check for the existence of the file test\\performance\\docker-compose.jmeter.yaml to determine if performance tests have been defined.","title":"Guide"},{"location":"How-to-guides/Testing/how-to-create-performance-test/#how-to-add-a-performance-test-for-your-service","text":"The Performance Test scripts should be added to the test\\performance folder in the GitHub repository of the service. Refer to the ffc-demo-web example . This folder should contain a docker-compose.jmeter.yaml file is used to build up the docker containers required to execute the tests. As a minimum, this will create a JMeter container and optionally create Selenium Grid containers. Using BrowserStack is preferred to running the tests using Selenium Grid hosted in Docker containers because you get better performance and scalability as the test load increases.","title":"How to add a performance test for your service?"},{"location":"How-to-guides/Testing/how-to-create-performance-test/#requirments-for-local-development","text":"Docker Desktop 2.2.0.3 (42716) or higher JMeter v5.5 or above","title":"Requirments for local development"},{"location":"How-to-guides/Testing/how-to-create-performance-test/#how-to-run-a-performance-test-locally","text":"Executre the above commands in bash or PowerShell 1 2 3 4 5 cd test / performance # this will execute the docker-compose at the root folder to create an instance of the service and its dependences # and then it will create the performance testing containers (JMeter and any other containers specified in docker-compose.jmeter.yaml) docker-compose -f ../../ docker-compose . yaml -f docker-compose . jmeter . yaml run jmeter-test","title":"How to run a performance test locally?"},{"location":"How-to-guides/Testing/how-to-create-performance-test/#how-to-parameterising-your-tests","text":"You can modify the number of virtual users, loop count and ramp-up duration by changing the settings in the file perf-test.properties. 1 2 3 4 5 6 7 # Sample user.properties file #--------------------------------------------------------------------------- # Properties added to manage noThreads rampUp lCount values #--------------------------------------------------------------------------- noThreads=15 rampUp=1 lCount=2 You can then reference these variables in your JMeter Script.","title":"How to parameterising your Tests"},{"location":"How-to-guides/Testing/how-to-create-performance-test/#how-to-customize-your-pipeline-to-run-performance-tests","text":"You can customize the environments where you would like to run specific features or scenarios of performance test 1 2 3 4 postDeployTest : testEnvs : performanceTests : pre1 envToTest : snd4,dev1,tst1,pre1 if not defined, the pipeline will run with following default settings 1 2 3 4 postDeployTest : testEnvs : performanceTests : snd4, pre1 envToTest : snd4,dev1,tst1,pre1 Please refer ffc-demo-web pipeline:","title":"How to customize your pipeline to run performance tests?"},{"location":"How-to-guides/Testing/how-to-create-performance-test/#how-to-disable-test","text":"if you want to disable the test for any reason please refer Disable Test","title":"How to disable test?"},{"location":"How-to-guides/Testing/how-to-disable-test/","text":"How to disable test \u00b6 In this how to guide you will learn how to disable test for the service How to disable one specific test? \u00b6 if you want to disable a specific test for any reason, please remove the test from 'testsToRun' line. 1 2 postDeployTest : testsToRun : 'owasp;accessibility;performance;service-acceptance;acceptance;contract;integration' example performance test 1 2 postDeployTest : testsToRun : 'owasp;accessibility;service-acceptance;acceptance;contract;integration' How to disable all tests? \u00b6 if you want to disable all tests, please set 'testsToRun' to 'none' 1 2 postDeployTest : testsToRun : 'none' Please refer ffc-demo-web pipeline:","title":"How to disable test"},{"location":"How-to-guides/Testing/how-to-disable-test/#how-to-disable-test","text":"In this how to guide you will learn how to disable test for the service","title":"How to disable test"},{"location":"How-to-guides/Testing/how-to-disable-test/#how-to-disable-one-specific-test","text":"if you want to disable a specific test for any reason, please remove the test from 'testsToRun' line. 1 2 postDeployTest : testsToRun : 'owasp;accessibility;performance;service-acceptance;acceptance;contract;integration' example performance test 1 2 postDeployTest : testsToRun : 'owasp;accessibility;service-acceptance;acceptance;contract;integration'","title":"How to disable one specific test?"},{"location":"How-to-guides/Testing/how-to-disable-test/#how-to-disable-all-tests","text":"if you want to disable all tests, please set 'testsToRun' to 'none' 1 2 postDeployTest : testsToRun : 'none' Please refer ffc-demo-web pipeline:","title":"How to disable all tests?"},{"location":"Migrate-to-ADP/migrate-a-delivery-project/","text":"Project Migration Process \u00b6 Details on migrating your existing project and its services to ADP. Project Migration Stages \u00b6 Project Migration Timeline \u00b6 Pre-migration \u00b6 Please fill in the migration questionnaire. Template is in the internal ADP documentation GitHub repository [Link]. Once complete you will agree with the ADP Platform Team & CCoE when to be migrated and onboarded on to ADP. Infrastructure Migration \u00b6 ADP Portal Setup: Onboarding of delivery programme if not present already. Onboarding of delivery project Onboarding of your team members For each of your platform services you now need to migrate them over to ADP and create the needed infrastructure to support them. Link to guide . Once all services/ infrastructure are created and verified in SND3 (O365_DefraDev), will be begin the process of pushing the services/ infrastructure to environment in the DEFRA tenant, DEV1, TST\u00bd, and PRE1. Once deployment is complete and tested in lower we will be able to progress to PRD1 ensure that the DEFRA release management progress is adhered to. Developer Migration \u00b6 Developers of the delivery project actively learning and using the platform to develop new features. As a new developer we recommend starting at \" Why ADP \" to under the platforms benefits and challenges. Data Migration \u00b6 Near to completion of the migration before the service goes live on ADP. Data from the old production environment will need to be moved into data services wih in ADP that was created as part of the infrastructure migrations stage. In order for this stage to go smoothly the old production traffic will need to be stopped in order to stop the flow of traffic in to the old data services. Allowing the data of old services to be transferred into the selected ADP data services. Depending on the selected data service it will require different methods to transfer data between production environment which is detailed in \" migrate-production-data \". Switch Over \u00b6 ... Business As Usual (BAU) \u00b6 Migration complete continue to","title":"Migrate a delivery project"},{"location":"Migrate-to-ADP/migrate-a-delivery-project/#project-migration-process","text":"Details on migrating your existing project and its services to ADP.","title":"Project Migration Process"},{"location":"Migrate-to-ADP/migrate-a-delivery-project/#project-migration-stages","text":"","title":"Project Migration Stages"},{"location":"Migrate-to-ADP/migrate-a-delivery-project/#project-migration-timeline","text":"","title":"Project Migration Timeline"},{"location":"Migrate-to-ADP/migrate-a-delivery-project/#pre-migration","text":"Please fill in the migration questionnaire. Template is in the internal ADP documentation GitHub repository [Link]. Once complete you will agree with the ADP Platform Team & CCoE when to be migrated and onboarded on to ADP.","title":"Pre-migration"},{"location":"Migrate-to-ADP/migrate-a-delivery-project/#infrastructure-migration","text":"ADP Portal Setup: Onboarding of delivery programme if not present already. Onboarding of delivery project Onboarding of your team members For each of your platform services you now need to migrate them over to ADP and create the needed infrastructure to support them. Link to guide . Once all services/ infrastructure are created and verified in SND3 (O365_DefraDev), will be begin the process of pushing the services/ infrastructure to environment in the DEFRA tenant, DEV1, TST\u00bd, and PRE1. Once deployment is complete and tested in lower we will be able to progress to PRD1 ensure that the DEFRA release management progress is adhered to.","title":"Infrastructure Migration"},{"location":"Migrate-to-ADP/migrate-a-delivery-project/#developer-migration","text":"Developers of the delivery project actively learning and using the platform to develop new features. As a new developer we recommend starting at \" Why ADP \" to under the platforms benefits and challenges.","title":"Developer Migration"},{"location":"Migrate-to-ADP/migrate-a-delivery-project/#data-migration","text":"Near to completion of the migration before the service goes live on ADP. Data from the old production environment will need to be moved into data services wih in ADP that was created as part of the infrastructure migrations stage. In order for this stage to go smoothly the old production traffic will need to be stopped in order to stop the flow of traffic in to the old data services. Allowing the data of old services to be transferred into the selected ADP data services. Depending on the selected data service it will require different methods to transfer data between production environment which is detailed in \" migrate-production-data \".","title":"Data Migration"},{"location":"Migrate-to-ADP/migrate-a-delivery-project/#switch-over","text":"...","title":"Switch Over"},{"location":"Migrate-to-ADP/migrate-a-delivery-project/#business-as-usual-bau","text":"Migration complete continue to","title":"Business As Usual (BAU)"},{"location":"Migrate-to-ADP/migrate-a-platform-service/","text":"Migrate a service to ADP \u00b6 In this guides you will learn how to migrate your existing service to ADP. Prerequisites \u00b6 Onboarded delivery project on to ADP . GitHub repository in the Defra Org of your existing service. GitHub repository added to GitHub Apps. Azure DevOps with ADP service connection added. Team/ Delivery Project created in the ADP portal. Access to Azure environments including in O356_DefraDev Tenant (SND3) & Defra Tenant (DEV1+).","title":"Migrate platform service"},{"location":"Migrate-to-ADP/migrate-a-platform-service/#migrate-a-service-to-adp","text":"In this guides you will learn how to migrate your existing service to ADP.","title":"Migrate a service to ADP"},{"location":"Migrate-to-ADP/migrate-a-platform-service/#prerequisites","text":"Onboarded delivery project on to ADP . GitHub repository in the Defra Org of your existing service. GitHub repository added to GitHub Apps. Azure DevOps with ADP service connection added. Team/ Delivery Project created in the ADP portal. Access to Azure environments including in O356_DefraDev Tenant (SND3) & Defra Tenant (DEV1+).","title":"Prerequisites"},{"location":"Migrate-to-ADP/migrate-production-data/","text":"Migrate Production Data \u00b6 In this guide you will learn how to migrate your existing service to ADP.","title":"Migrate production data"},{"location":"Migrate-to-ADP/migrate-production-data/#migrate-production-data","text":"In this guide you will learn how to migrate your existing service to ADP.","title":"Migrate Production Data"},{"location":"Platform-Architecture/architecture-overview/","text":"Architecture Overview \u00b6 Whos is this for: Platform Team Delivery Project Tech Leads Delivery Project Architects","title":"Architecture Overview"},{"location":"Platform-Architecture/architecture-overview/#architecture-overview","text":"Whos is this for: Platform Team Delivery Project Tech Leads Delivery Project Architects","title":"Architecture Overview"},{"location":"Platform-Architecture/environments/","text":"TODO This page is a work in progress and will be updated in due course. Needs environments updated. Environments in ADP \u00b6 The table below details the environments the Platform supports, there purposes, and whether they're mandatory for going live / on the RTL path. 1-1 mapping between Services/Tenants Environments & Azure Subscriptions The number at the end of the environment code designates the Subscription/Environment number for CCoE reference. A Tenant/Customer/Service means a 'customer or team user the platform' and not the Platform team itself. Infrastructure-Dev is only for Platform Engineers. Core ADP Environments for Tenants & Infrastructure teams \u00b6 Principal Environment Name Use case Route-to-live Azure Environment Code/Subscription Additional Information Azure Tenant Tenant-Production Live Services, Public & Private beta. Yes AZR-ADP-PRD1 Defra Tenant-Pre-Production Automated Acceptance testing prior to production. Yes AZR-ADP-PRE1 VPN Required. Defra Tenant-Demo External demonstrations, PEN tests etc. No AZR-ADP-TST2 VPN Required. Defra Tenant-Test General testing, performance testing, exploratory testing No AZR-ADP-TST1 Intended for demo's to external and internal stakeholders Defra Tenant-Development Development Yes AZR-ADP-DEV1 VPN Required. Defra Tenant-Sandpit Pre-merge automated tests, Pull Request checks etc. No AZR-ADP-SND4 VPN Required. Defra Infrastructure-Dev Testing infrastructure changes, proof of concepts, experimentations. Platform Team Only. No/N/A AZD-ADP-SND1, SND2, SND3 VPN Required. 0365_DefraDev Shared Services and Management Subscriptions \u00b6 Principal Environment Name Use case Route-to-live Azure Environment Code/Subscription Additional Information Azure Tenant Shared Services 3 Management and Shared Services - Test and all environments below. POC/development area. No/N/A AZD-ADP-SSV3 DefraDev Shared Services/management 0365_DefraDev Shared Services 5 Management and Shared Tests - Production and all environments below. Live services. No/N/A AZR-ADP-SSV5 Contains live ACR. Live shared services/management. Defra The Subscriptions that map to the environments documentation can be found here. Route to live overview \u00b6 For all Service teams, the defined minimum 'route to live' path is: Dev > Pre-Prod > Production (Live). -- All other environments are optional: SND, TST, DMO etc. This means that Service Teams must have passed automated checks/smoke tests in the Pre-prod environment and any initial merge and validation checks in Development only before going Live. All other environments are there on-demand for teams. We may have additional environments if the future if needed, such as a dedicated ITHC/PEN Test area (or Demo can be used), but again these would be optional. === note When deploying from a previous image/artefact already deployed to an environment, no CI/build is required. Environments are selectable.","title":"Environments"},{"location":"Platform-Architecture/environments/#environments-in-adp","text":"The table below details the environments the Platform supports, there purposes, and whether they're mandatory for going live / on the RTL path. 1-1 mapping between Services/Tenants Environments & Azure Subscriptions The number at the end of the environment code designates the Subscription/Environment number for CCoE reference. A Tenant/Customer/Service means a 'customer or team user the platform' and not the Platform team itself. Infrastructure-Dev is only for Platform Engineers.","title":"Environments in ADP"},{"location":"Platform-Architecture/environments/#core-adp-environments-for-tenants-infrastructure-teams","text":"Principal Environment Name Use case Route-to-live Azure Environment Code/Subscription Additional Information Azure Tenant Tenant-Production Live Services, Public & Private beta. Yes AZR-ADP-PRD1 Defra Tenant-Pre-Production Automated Acceptance testing prior to production. Yes AZR-ADP-PRE1 VPN Required. Defra Tenant-Demo External demonstrations, PEN tests etc. No AZR-ADP-TST2 VPN Required. Defra Tenant-Test General testing, performance testing, exploratory testing No AZR-ADP-TST1 Intended for demo's to external and internal stakeholders Defra Tenant-Development Development Yes AZR-ADP-DEV1 VPN Required. Defra Tenant-Sandpit Pre-merge automated tests, Pull Request checks etc. No AZR-ADP-SND4 VPN Required. Defra Infrastructure-Dev Testing infrastructure changes, proof of concepts, experimentations. Platform Team Only. No/N/A AZD-ADP-SND1, SND2, SND3 VPN Required. 0365_DefraDev","title":"Core ADP Environments for Tenants &amp; Infrastructure teams"},{"location":"Platform-Architecture/environments/#shared-services-and-management-subscriptions","text":"Principal Environment Name Use case Route-to-live Azure Environment Code/Subscription Additional Information Azure Tenant Shared Services 3 Management and Shared Services - Test and all environments below. POC/development area. No/N/A AZD-ADP-SSV3 DefraDev Shared Services/management 0365_DefraDev Shared Services 5 Management and Shared Tests - Production and all environments below. Live services. No/N/A AZR-ADP-SSV5 Contains live ACR. Live shared services/management. Defra The Subscriptions that map to the environments documentation can be found here.","title":"Shared Services and Management Subscriptions"},{"location":"Platform-Architecture/environments/#route-to-live-overview","text":"For all Service teams, the defined minimum 'route to live' path is: Dev > Pre-Prod > Production (Live). -- All other environments are optional: SND, TST, DMO etc. This means that Service Teams must have passed automated checks/smoke tests in the Pre-prod environment and any initial merge and validation checks in Development only before going Live. All other environments are there on-demand for teams. We may have additional environments if the future if needed, such as a dedicated ITHC/PEN Test area (or Demo can be used), but again these would be optional. === note When deploying from a previous image/artefact already deployed to an environment, no CI/build is required. Environments are selectable.","title":"Route to live overview"},{"location":"Platform-Architecture/permissions-model/","text":"ADP Permissions Model \u00b6 This page contains an overview of the roles and permissions within ADP (Azure Development Platform). It outlines the different roles such as Platform User, Technical Team Member, Delivery Team Admin, Delivery Programme Admin, and ADP Admin, along with their respective descriptions and responsibilities. Explains the permissions associated with each role in the ADP Portal, Azure DevOps, and GitHub. It describes how permissions are stored in a database and Azure AD using AAD groups. Users are assigned to specific groups based on their roles, granting them the necessary permissions in the ADP Portal, GitHub, Azure, and Azure DevOps. ADP Roles \u00b6 The table below details the roles in the Platform, their scope and description: Role Scope Description Platform User Platform A user of the ADP Platform, who has access to the ADP Portal and can be a member of a Delivery Project or Programme. To do this, they must have a Cloud or DefraGovUK Account. Technical Team Member Delivery Project Tech Lead, Tester, Developer, or Architect on the Delivery Project team. Delivery Team Member Delivery Project Member of the Delivery Project team. Delivery Team Admin Delivery Project Tech lead and/or Delivery Manager for the Delivery Project team. Delivery Programme Admin Delivery Programme Administers Delivery Programmes within the ADP Portal. ADP Admin Platform ADP Platform Engineering delivery team member. CCoE Engineer Organization Cloud Center of Excellence engineer. ADP Service Account Platform Service account used by automation within ADP. Info Please note: if a user holds multiple roles, they will receive the combined permissions associated with all their roles. This ensures that they have access to all the rights and privileges granted by the most significant role they possess. Essentially, the role with the highest level of permissions takes precedence. Portal Permissions \u00b6 The permissions for the portal are stored both in a database and in Azure AD with the use of AAD groups. The group assignments and naming convention are as follows: Delivery Non Technical Team Member are assigned to Delivery AAG-Users-ADP-{programme}-{delivery project}_NonTechUser AAD group. Technical Team Member are assigned to AAG-Users-ADP-{programme}-{delivery project}_TechUser AAD group. Delivery Team Admin are assigned to AAG-Users-ADP-{programme}-{delivery project}_Admin AAD group. Delivery Programme Admin are assigned to AAG-Users-ADP-{programme}_Admin AAD group. ADP Admins are assigned to AAG-User-ADP-PlatformEngineers AAD group. By being added to these groups in Azure AD via the ADP Portal, users will be granted the permissions for their role in the ADP Portal. The permissions for each role in the ADP Portal are detailed below. Platform User \u00b6 ADP Portal Permissions for the Platform User role: Access to the ADP Portal. Can be selected as a Delivery Project team member/admin or Delivery Programme Admin. Read access to all ALBs, delivery projects, programmes, etc. Delivery Project: Team Member \u00b6 ADP Portal Permissions for the Delivery Project Team Member role: Includes all Platform User permissions. Displayed as a Member of assigned Delivery Project teams. Delivery Project: Technical Team Member \u00b6 ADP Portal Permissions for the Technical Team Member role: Includes all Delivery Project Team Member permissions. Scaffold/create new services for their delivery project (inc. repos). Delivery Project: Team Admin \u00b6 ADP Portal Permissions for the Delivery Team Admin role: Includes all Delivery Team Member permissions. Has the ability to invite/add users to their Delivery Project team as: Team members, Technical Team Members, Team Admins, and Technical Team Admins via the ADP Portal. The function adding an team member will add them to required GitHub team, Azure DevOps project, and Azure AAD groups required for Azure resource access, depending on new team members roles in the Delivery Project. Edit delivery project details in the ADP Portal. Delivery Programme Admin \u00b6 ADP Portal Permissions for the Delivery Programme Admin role: Includes all Delivery Team Admin permissions for all Delivery Projects in the programme. Can create new Delivery Projects in the programme. Can edit programme details for programmes they administrator in the ADP Portal. Can invite/add other admins to the programmes they administer. ADP Admin \u00b6 ADP Portal Permissions for the ADP Admin role: - Full access to the ADP Portal and is admin for all ALBs, delivery projects, programmes, etc. GitHub Permissions \u00b6 GitHub Permissions are assigned and managed using GitHub teams. The following GitHub teams are automatically assigned to each repository owned by a Delivery Project: ADP-{programme}-{delivery project}-Contributors GitHub team: contains all Delivery Project Technical Team Members ADP-{programme}-{delivery project}-Admins GitHub team: contains users that have been assigned both the Technical Team Member & Delivery Team Admin role for the Delivery Project ADP-Platform-Admins GitHub team: contains the ADP Admins. Info Please Note: Users that have not been asssigned the Technical Team Member role for a Delivery Project will not be given any GitHub permissions. Delivery Programme Admins & Delivery Project Team Admins can use the ADP Portal to add and remove users from their GitHub teams in via the add/ remove user functionality. Info Please Note: By default all repositories are public and can be accessed by anyone. Anyone can clone, fork, and view the repository. However, only members of the GitHub team will be able to push changes to the repository. Technical Team Member \u00b6 Technical Team Members are given the following permissions in GitHub: Write access to Delivery Projects GitHub repositories, which will allow triage permissions plus read, clone and push to repositories. Technical Team Member with Delivery Team Admin \u00b6 Users that have been given both the Technical Team Member & Team Admin role within a Delivery Project are given the following permissions in GitHub: All permissions of a Technical Team Member. Admin access to Delivery Projects GitHub repositories, which will allow full access to their repositories including sensitive and destructive actions. ADP Admin \u00b6 ADP Admins are given the following permissions in GitHub: All permissions of a Technical Team Member with Delivery Team Admin. Full access to all ADP repositories in the DEFRA GitHub organization. Azure Permissions \u00b6 TODO TBC For Azure permissions we use AAD group to given users the correct level of permissions. There are the key groups are for Azure permissions are as follows: Technical Team Member are assigned to AAG-Users-ADP-{programme}-{delivery project}_TechUser AAD group. ADP Admins are assigned to AAG-User-ADP-PlatformEngineers AAD group. Info Users with Delivery Team Admins, Delivery Programme Admins, or Delivery Team Members roles only will not be given any Azure permissions. They can add, edit, or remove users from their delivery projects AAD groups in the ADP Portal by the add/ remove user functionality in the ADP Portal. Technical Team Member \u00b6 Technical Team Members are given the following permissions in Azure: - ... Spell out permissions for each group in each of Azure resources, etc \u00b6 Should this be done here or in an another page? Resource group Database AAG-Azure-ADP-{programme}-{delivery project}-{environment}-PostgressDB_Reader AAG-Azure-ADP-{programme}-{delivery project}-{environment}-PostgressDB_Writer Azure DevOps Permissions \u00b6 TODO TBC ADP-ALB-ProgrammeName-DeliveryProjectName-Contributors - For Technical Team Members (write access level to the repo) Sonar Cloud Permissions \u00b6 ADP will use Technical Team members GitHub account to assign permissions in SonarCloud. Assuming that this GitHub account has been added to the DEFRA's SonarCloud organisation, ADP will assign their GitHub account to the their Delivery Project's SonarCloud group when they are added to a Delivery Project in the ADP Portal. Giving them access to do the required actions for their Delivery Project within SonarCloud. Info By default all Sonar Cloud projects are public and can be accessed by anyone in read only mode. ADP portal creates a SonarCloud user group and permissions template per Delivery Project on creation using the {Delivery Project Team name} as the groups name. This group will filter on SonarCloud projects by the Delivery Project's ADP namespace or alias fields. For example if project FCP ACD has a ADP namespace of fcp-acd and a alias of ffc-acd group will have permissions on Sonar Cloud project starting with fcp-acd* or ffc-acd* (ffc-acd-frontend, fcp-acd-backend, etc). Warning SonarCloud projects that do not include the delivery projects ADP namespace or alias in the name of the project in Sonar Cloud will not be included in the group permissions. An Sonar Cloud Organisation Admin will need to add the service to the group permissions manually. Technical Team Member \u00b6 Each Technical Team Member will be added to the SonarCloud user group for the Delivery Project they are a member of in Sonar Cloud. The permissions for the group are as follows for each service in Sonar Cloud: Administer Issues: Change the type and severity of issues, resolve issues as being \"fixed\", \"accepted\" or \"false-positive\" (users also need \"Browse\" permission). Administer Security Hotspots: Resolve a Security Hotspot as reviewed (fixed or safe), reset it as to review (users also need Browse permission). ADP Admin \u00b6 ADP Admins will be able to see all services (SonarCloud projects) created by ADP's automation in Sonar Cloud. These are the permissions for the ADP user group in Sonar Cloud as the Sonar Cloud project level: Administer Issues: Change the type and severity of issues, resolve issues as being \"fixed\", \"accepted\" or \"false-positive\" (users also need \"Browse\" permission). Administer Security Hotspots: Resolve a Security Hotspot as reviewed (fixed or safe), reset it as to review (users also need Browse permission). Administer: Access project settings and perform administration tasks. (Users will also need \"Browse\" permission) Execute Analysis: Ability to get all settings required to perform an analysis (including the secured settings like passwords) and to push analysis results to the SonarCloud server. ADP Service Account & ADP SonarCloud Automation \u00b6 ADP requires these permissions in order to run perform API administration tasks in Sonar Cloud at the organisation level. These permissions are required to create the user groups, permissions templates, and add users to the permissions templates in Sonar Cloud. The permissions are as follows: Administer: Allows you to perform any action on both Quality Profiles and Quality Gates. Execute Analysis: Allows you to trigger an analysis and to push analysis results to the SonarCloud server. Create Project: Allows you to initialize a project and configure its settings before the initial first analysis is performed. Administer Organization: Allows you to perform all administrative functions for an organization. Details of SonarCloud permissions . Current known Sonar Cloud Web API Actions: Create User Group - Create a group. Requires the following permission: 'Administer System'. Search for User - Search for users. Requires the following permission: 'Administer System'. Add user to User Group - Add a user to a group. 'id' or 'name' must be provided. Requires the following permission: 'Administer System'. Create Permissions Template -Create a permission template.Requires the permission 'Administer' on the organization. Update Permissions Template - Update a permission template. Requires the permission 'Administer' on the organization. Add User Group to Permission Template - Add a group to a permission template. Requires the permission 'Administer' on the organization. Giving a group the permission of codeviewer , issueadmin , securityhotspotadmin , scan , and user to the group added to permissions template. Info Not possible to add new users directly to github organisation . User will need to be added to the Sonar Cloud organisation manually by a Sonar Cloud Organisation Admin or allow for member synchronization on DEFRA GitHub organisation .","title":"Permissions Model"},{"location":"Platform-Architecture/permissions-model/#adp-permissions-model","text":"This page contains an overview of the roles and permissions within ADP (Azure Development Platform). It outlines the different roles such as Platform User, Technical Team Member, Delivery Team Admin, Delivery Programme Admin, and ADP Admin, along with their respective descriptions and responsibilities. Explains the permissions associated with each role in the ADP Portal, Azure DevOps, and GitHub. It describes how permissions are stored in a database and Azure AD using AAD groups. Users are assigned to specific groups based on their roles, granting them the necessary permissions in the ADP Portal, GitHub, Azure, and Azure DevOps.","title":"ADP Permissions Model"},{"location":"Platform-Architecture/permissions-model/#adp-roles","text":"The table below details the roles in the Platform, their scope and description: Role Scope Description Platform User Platform A user of the ADP Platform, who has access to the ADP Portal and can be a member of a Delivery Project or Programme. To do this, they must have a Cloud or DefraGovUK Account. Technical Team Member Delivery Project Tech Lead, Tester, Developer, or Architect on the Delivery Project team. Delivery Team Member Delivery Project Member of the Delivery Project team. Delivery Team Admin Delivery Project Tech lead and/or Delivery Manager for the Delivery Project team. Delivery Programme Admin Delivery Programme Administers Delivery Programmes within the ADP Portal. ADP Admin Platform ADP Platform Engineering delivery team member. CCoE Engineer Organization Cloud Center of Excellence engineer. ADP Service Account Platform Service account used by automation within ADP. Info Please note: if a user holds multiple roles, they will receive the combined permissions associated with all their roles. This ensures that they have access to all the rights and privileges granted by the most significant role they possess. Essentially, the role with the highest level of permissions takes precedence.","title":"ADP Roles"},{"location":"Platform-Architecture/permissions-model/#portal-permissions","text":"The permissions for the portal are stored both in a database and in Azure AD with the use of AAD groups. The group assignments and naming convention are as follows: Delivery Non Technical Team Member are assigned to Delivery AAG-Users-ADP-{programme}-{delivery project}_NonTechUser AAD group. Technical Team Member are assigned to AAG-Users-ADP-{programme}-{delivery project}_TechUser AAD group. Delivery Team Admin are assigned to AAG-Users-ADP-{programme}-{delivery project}_Admin AAD group. Delivery Programme Admin are assigned to AAG-Users-ADP-{programme}_Admin AAD group. ADP Admins are assigned to AAG-User-ADP-PlatformEngineers AAD group. By being added to these groups in Azure AD via the ADP Portal, users will be granted the permissions for their role in the ADP Portal. The permissions for each role in the ADP Portal are detailed below.","title":"Portal Permissions"},{"location":"Platform-Architecture/permissions-model/#platform-user","text":"ADP Portal Permissions for the Platform User role: Access to the ADP Portal. Can be selected as a Delivery Project team member/admin or Delivery Programme Admin. Read access to all ALBs, delivery projects, programmes, etc.","title":"Platform User"},{"location":"Platform-Architecture/permissions-model/#delivery-project-team-member","text":"ADP Portal Permissions for the Delivery Project Team Member role: Includes all Platform User permissions. Displayed as a Member of assigned Delivery Project teams.","title":"Delivery Project: Team Member"},{"location":"Platform-Architecture/permissions-model/#delivery-project-technical-team-member","text":"ADP Portal Permissions for the Technical Team Member role: Includes all Delivery Project Team Member permissions. Scaffold/create new services for their delivery project (inc. repos).","title":"Delivery Project: Technical Team Member"},{"location":"Platform-Architecture/permissions-model/#delivery-project-team-admin","text":"ADP Portal Permissions for the Delivery Team Admin role: Includes all Delivery Team Member permissions. Has the ability to invite/add users to their Delivery Project team as: Team members, Technical Team Members, Team Admins, and Technical Team Admins via the ADP Portal. The function adding an team member will add them to required GitHub team, Azure DevOps project, and Azure AAD groups required for Azure resource access, depending on new team members roles in the Delivery Project. Edit delivery project details in the ADP Portal.","title":"Delivery Project: Team Admin"},{"location":"Platform-Architecture/permissions-model/#delivery-programme-admin","text":"ADP Portal Permissions for the Delivery Programme Admin role: Includes all Delivery Team Admin permissions for all Delivery Projects in the programme. Can create new Delivery Projects in the programme. Can edit programme details for programmes they administrator in the ADP Portal. Can invite/add other admins to the programmes they administer.","title":"Delivery Programme Admin"},{"location":"Platform-Architecture/permissions-model/#adp-admin","text":"ADP Portal Permissions for the ADP Admin role: - Full access to the ADP Portal and is admin for all ALBs, delivery projects, programmes, etc.","title":"ADP Admin"},{"location":"Platform-Architecture/permissions-model/#github-permissions","text":"GitHub Permissions are assigned and managed using GitHub teams. The following GitHub teams are automatically assigned to each repository owned by a Delivery Project: ADP-{programme}-{delivery project}-Contributors GitHub team: contains all Delivery Project Technical Team Members ADP-{programme}-{delivery project}-Admins GitHub team: contains users that have been assigned both the Technical Team Member & Delivery Team Admin role for the Delivery Project ADP-Platform-Admins GitHub team: contains the ADP Admins. Info Please Note: Users that have not been asssigned the Technical Team Member role for a Delivery Project will not be given any GitHub permissions. Delivery Programme Admins & Delivery Project Team Admins can use the ADP Portal to add and remove users from their GitHub teams in via the add/ remove user functionality. Info Please Note: By default all repositories are public and can be accessed by anyone. Anyone can clone, fork, and view the repository. However, only members of the GitHub team will be able to push changes to the repository.","title":"GitHub Permissions"},{"location":"Platform-Architecture/permissions-model/#technical-team-member","text":"Technical Team Members are given the following permissions in GitHub: Write access to Delivery Projects GitHub repositories, which will allow triage permissions plus read, clone and push to repositories.","title":"Technical Team Member"},{"location":"Platform-Architecture/permissions-model/#technical-team-member-with-delivery-team-admin","text":"Users that have been given both the Technical Team Member & Team Admin role within a Delivery Project are given the following permissions in GitHub: All permissions of a Technical Team Member. Admin access to Delivery Projects GitHub repositories, which will allow full access to their repositories including sensitive and destructive actions.","title":"Technical Team Member with Delivery Team Admin"},{"location":"Platform-Architecture/permissions-model/#adp-admin_1","text":"ADP Admins are given the following permissions in GitHub: All permissions of a Technical Team Member with Delivery Team Admin. Full access to all ADP repositories in the DEFRA GitHub organization.","title":"ADP Admin"},{"location":"Platform-Architecture/permissions-model/#azure-permissions","text":"TODO TBC For Azure permissions we use AAD group to given users the correct level of permissions. There are the key groups are for Azure permissions are as follows: Technical Team Member are assigned to AAG-Users-ADP-{programme}-{delivery project}_TechUser AAD group. ADP Admins are assigned to AAG-User-ADP-PlatformEngineers AAD group. Info Users with Delivery Team Admins, Delivery Programme Admins, or Delivery Team Members roles only will not be given any Azure permissions. They can add, edit, or remove users from their delivery projects AAD groups in the ADP Portal by the add/ remove user functionality in the ADP Portal.","title":"Azure Permissions"},{"location":"Platform-Architecture/permissions-model/#technical-team-member_1","text":"Technical Team Members are given the following permissions in Azure: - ...","title":"Technical Team Member"},{"location":"Platform-Architecture/permissions-model/#spell-out-permissions-for-each-group-in-each-of-azure-resources-etc","text":"Should this be done here or in an another page? Resource group Database AAG-Azure-ADP-{programme}-{delivery project}-{environment}-PostgressDB_Reader AAG-Azure-ADP-{programme}-{delivery project}-{environment}-PostgressDB_Writer","title":"Spell out permissions for each group in each of Azure resources, etc"},{"location":"Platform-Architecture/permissions-model/#azure-devops-permissions","text":"TODO TBC ADP-ALB-ProgrammeName-DeliveryProjectName-Contributors - For Technical Team Members (write access level to the repo)","title":"Azure DevOps Permissions"},{"location":"Platform-Architecture/permissions-model/#sonar-cloud-permissions","text":"ADP will use Technical Team members GitHub account to assign permissions in SonarCloud. Assuming that this GitHub account has been added to the DEFRA's SonarCloud organisation, ADP will assign their GitHub account to the their Delivery Project's SonarCloud group when they are added to a Delivery Project in the ADP Portal. Giving them access to do the required actions for their Delivery Project within SonarCloud. Info By default all Sonar Cloud projects are public and can be accessed by anyone in read only mode. ADP portal creates a SonarCloud user group and permissions template per Delivery Project on creation using the {Delivery Project Team name} as the groups name. This group will filter on SonarCloud projects by the Delivery Project's ADP namespace or alias fields. For example if project FCP ACD has a ADP namespace of fcp-acd and a alias of ffc-acd group will have permissions on Sonar Cloud project starting with fcp-acd* or ffc-acd* (ffc-acd-frontend, fcp-acd-backend, etc). Warning SonarCloud projects that do not include the delivery projects ADP namespace or alias in the name of the project in Sonar Cloud will not be included in the group permissions. An Sonar Cloud Organisation Admin will need to add the service to the group permissions manually.","title":"Sonar Cloud Permissions"},{"location":"Platform-Architecture/permissions-model/#technical-team-member_2","text":"Each Technical Team Member will be added to the SonarCloud user group for the Delivery Project they are a member of in Sonar Cloud. The permissions for the group are as follows for each service in Sonar Cloud: Administer Issues: Change the type and severity of issues, resolve issues as being \"fixed\", \"accepted\" or \"false-positive\" (users also need \"Browse\" permission). Administer Security Hotspots: Resolve a Security Hotspot as reviewed (fixed or safe), reset it as to review (users also need Browse permission).","title":"Technical Team Member"},{"location":"Platform-Architecture/permissions-model/#adp-admin_2","text":"ADP Admins will be able to see all services (SonarCloud projects) created by ADP's automation in Sonar Cloud. These are the permissions for the ADP user group in Sonar Cloud as the Sonar Cloud project level: Administer Issues: Change the type and severity of issues, resolve issues as being \"fixed\", \"accepted\" or \"false-positive\" (users also need \"Browse\" permission). Administer Security Hotspots: Resolve a Security Hotspot as reviewed (fixed or safe), reset it as to review (users also need Browse permission). Administer: Access project settings and perform administration tasks. (Users will also need \"Browse\" permission) Execute Analysis: Ability to get all settings required to perform an analysis (including the secured settings like passwords) and to push analysis results to the SonarCloud server.","title":"ADP Admin"},{"location":"Platform-Architecture/permissions-model/#adp-service-account-adp-sonarcloud-automation","text":"ADP requires these permissions in order to run perform API administration tasks in Sonar Cloud at the organisation level. These permissions are required to create the user groups, permissions templates, and add users to the permissions templates in Sonar Cloud. The permissions are as follows: Administer: Allows you to perform any action on both Quality Profiles and Quality Gates. Execute Analysis: Allows you to trigger an analysis and to push analysis results to the SonarCloud server. Create Project: Allows you to initialize a project and configure its settings before the initial first analysis is performed. Administer Organization: Allows you to perform all administrative functions for an organization. Details of SonarCloud permissions . Current known Sonar Cloud Web API Actions: Create User Group - Create a group. Requires the following permission: 'Administer System'. Search for User - Search for users. Requires the following permission: 'Administer System'. Add user to User Group - Add a user to a group. 'id' or 'name' must be provided. Requires the following permission: 'Administer System'. Create Permissions Template -Create a permission template.Requires the permission 'Administer' on the organization. Update Permissions Template - Update a permission template. Requires the permission 'Administer' on the organization. Add User Group to Permission Template - Add a group to a permission template. Requires the permission 'Administer' on the organization. Giving a group the permission of codeviewer , issueadmin , securityhotspotadmin , scan , and user to the group added to permissions template. Info Not possible to add new users directly to github organisation . User will need to be added to the Sonar Cloud organisation manually by a Sonar Cloud Organisation Admin or allow for member synchronization on DEFRA GitHub organisation .","title":"ADP Service Account &amp; ADP SonarCloud Automation"},{"location":"Platform-Architecture/scaling/","text":"Scaling of the Platform TODO This page is a work in progress and will be updated in due course. On the Platform we're acutely aware we'll be hosting a large variety of services, with a wide range of requirements. We need to understand and define how we'll manage and scale, based on legitimate requirements and workload demands. Here, we'll cover this in terms of tenant applications, azure infrastructure, storage capacity, service limits, pipelines, ADO projects etc. What do you scale? \u00b6 The platform. But specifically: Azure Kubernetes Service (AKS) - Cluster, Nodes, & Apps/Pods Azure Storage Locations (Log Analytics, Storage accounts, etc.) PostgreSQL Flexible Server - CPU, Memory & Storage Azure Cosmos DB Azure Front Door App Gateways and Ingress Controllers Deployment Pipelines Azure PaaS (Keyvault, App Config, etc.) Azure Redis Azure Service Bus Azure API Management How do you scale? \u00b6 We'll use a range of tools and features based on the component which we'll detail below. We'll cover both our infrastructure and service scaling configuration, as well as how we scale Platform Teams, Tenants and Services appropriately (i.e. supporting tooling, pipelines, projects, repos, etc.). Scaling Configurations \u00b6 Azure Kubernetes Services \u00b6 PostgreSQL Flexible Server \u00b6","title":"Scaling"},{"location":"Platform-Architecture/scaling/#what-do-you-scale","text":"The platform. But specifically: Azure Kubernetes Service (AKS) - Cluster, Nodes, & Apps/Pods Azure Storage Locations (Log Analytics, Storage accounts, etc.) PostgreSQL Flexible Server - CPU, Memory & Storage Azure Cosmos DB Azure Front Door App Gateways and Ingress Controllers Deployment Pipelines Azure PaaS (Keyvault, App Config, etc.) Azure Redis Azure Service Bus Azure API Management","title":"What do you scale?"},{"location":"Platform-Architecture/scaling/#how-do-you-scale","text":"We'll use a range of tools and features based on the component which we'll detail below. We'll cover both our infrastructure and service scaling configuration, as well as how we scale Platform Teams, Tenants and Services appropriately (i.e. supporting tooling, pipelines, projects, repos, etc.).","title":"How do you scale?"},{"location":"Platform-Architecture/scaling/#scaling-configurations","text":"","title":"Scaling Configurations"},{"location":"Platform-Architecture/scaling/#azure-kubernetes-services","text":"","title":"Azure Kubernetes Services"},{"location":"Platform-Architecture/scaling/#postgresql-flexible-server","text":"","title":"PostgreSQL Flexible Server"},{"location":"Platform-Architecture/tech-radar/","text":"Tech Radar for ADP \u00b6 The Tech Radar is a tool to inspire and support teams to pick the best technologies for new projects. It is a visualisation of the technologies that are in use and recommended by the majority of teams. The radar is split into 4 quadrants and 4 rings. Quadrants \u00b6 Description \u00b6 The Tech Radar should have 4 quadrants. Each Entry in the technology stack is represented by a blip in a quadrant. The quadrants represent broad categories that entries fit into. Languages & Frameworks Tooling Infrastructure Observability Rings \u00b6 Description \u00b6 The Tech Radar should have 4 rings with the central ring representing entries that are in use and recommended by the majority of teams. Whilst the outer ring represents entries that are not recommended and for which we recommend teams transitions to a recommended entry. Adopt This technology is recommended for use by the majority of teams with a specific use case. Trial This technology has been evaluated for specific use cases and has showed clear benefits. Some teams adopt it in production, although it should be limited to low-impact projects as it might incur a higher risk. Assess This technology has the potential to be beneficial for the organisation. Some teams are evaluating it and using it in experimental projects. Using it in production comes with a high cost and risk due to lack of in-house knowledge, maintenance, and support. Hold We don't want to further invest in this technology or we evaluated it and we don't see it as beneficial for the organisation. Teams should not use it in new projects and should plan on migrating to a supported alternative if they use it for historical reasons. For broadly adopted technologies, the Radar should refer to a migration path to a supported alternative. Entries FFC Technology Stack \u00b6 The below entries are taken from the technology stack in the FFC-Development-Guide . \u26a0\ufe0f Need to confirm whether any of the categories in the above linked technology stack would be appropriate Portal quadrants Technology & Services Stack \u00b6 Entry Quadrant Ring Note Node.js Languages & Frameworks Adopt Hapi.js Languages & Frameworks Adopt NPM Languages & Frameworks Adopt .Net Languages & Frameworks Adopt Python Languages & Frameworks Assess Nuget Tooling Adopt Docker Tooling Adopt Docker Compose Tooling Adopt Helm Tooling Adopt Bicep Tooling Adopt Azure CLI Tooling Adopt PowerShell Tooling Adopt Azure boards Tooling Adopt Jenkins Tooling Hold Azure pipelines Tooling Adopt Jest Test Tooling Adopt xUnit Test Tooling Assess nUnit Test Tooling Adopt Pact Broker Test Tooling Adopt Web Driver IO Test Tooling Adopt Cucumber Test Tooling Adopt Selenium Test Tooling Adopt BrowserStack Test Tooling Adopt JMeter Test Tooling Adopt Snyk Test Tooling Adopt OWASP Zap Test Tooling Adopt AXE? Test Tooling Adopt WAVE Test Tooling Adopt Anchor Engine Test Tooling Adopt Azure Kubernetes Service Infrastructure Adopt Flux CD Infrastructure Adopt Azure Service Operator Infrastructure Adopt Azure Container Registry Infrastructure Adopt Azure PostgreSQL (flexible Server) Infrastructure Adopt Azure Service Bus Infrastructure Adopt Azure Event Hubs Infrastructure Assess Azure App Configuration Infrastructure Adopt Azure Key Vault Infrastructure Adopt Azure Functions Infrastructure Hold *must be containerized in AKS Azure Storage Infrastructure Adopt Entra ID workload Identity Infrastructure Adopt Application Insights Observability Adopt Azure Repos Tooling Hold GitHub Tooling Adopt SonarCloud Tooling Adopt Docker Desktop Tooling Adopt Google Analytics Observability Adopt Prometheus Observability Adopt Grafana Observability Adopt Azure Monitor Observability Adopt Visual Studio 2022 Tooling Adopt Visual Studio Code Tooling Adopt App Reg's Tooling Adopt Azure CosmosDB (SQL) Tooling Adopt Azure CosmosDB (Mongo) Tooling Assess Azure AI Studio Infrastructure Assess Azure Machine Learning Infrastructure Assess Azure Cognitive Services Infrastructure Assess Azure AI Search Infrastructure Assess Azure Prompt Flow Infrastructure Assess","title":"Tech Radar"},{"location":"Platform-Architecture/tech-radar/#tech-radar-for-adp","text":"The Tech Radar is a tool to inspire and support teams to pick the best technologies for new projects. It is a visualisation of the technologies that are in use and recommended by the majority of teams. The radar is split into 4 quadrants and 4 rings.","title":"Tech Radar for ADP"},{"location":"Platform-Architecture/tech-radar/#quadrants","text":"","title":"Quadrants"},{"location":"Platform-Architecture/tech-radar/#description","text":"The Tech Radar should have 4 quadrants. Each Entry in the technology stack is represented by a blip in a quadrant. The quadrants represent broad categories that entries fit into. Languages & Frameworks Tooling Infrastructure Observability","title":"Description"},{"location":"Platform-Architecture/tech-radar/#rings","text":"","title":"Rings"},{"location":"Platform-Architecture/tech-radar/#description_1","text":"The Tech Radar should have 4 rings with the central ring representing entries that are in use and recommended by the majority of teams. Whilst the outer ring represents entries that are not recommended and for which we recommend teams transitions to a recommended entry. Adopt This technology is recommended for use by the majority of teams with a specific use case. Trial This technology has been evaluated for specific use cases and has showed clear benefits. Some teams adopt it in production, although it should be limited to low-impact projects as it might incur a higher risk. Assess This technology has the potential to be beneficial for the organisation. Some teams are evaluating it and using it in experimental projects. Using it in production comes with a high cost and risk due to lack of in-house knowledge, maintenance, and support. Hold We don't want to further invest in this technology or we evaluated it and we don't see it as beneficial for the organisation. Teams should not use it in new projects and should plan on migrating to a supported alternative if they use it for historical reasons. For broadly adopted technologies, the Radar should refer to a migration path to a supported alternative.","title":"Description"},{"location":"Platform-Architecture/tech-radar/#entries-ffc-technology-stack","text":"The below entries are taken from the technology stack in the FFC-Development-Guide . \u26a0\ufe0f Need to confirm whether any of the categories in the above linked technology stack would be appropriate Portal quadrants","title":"Entries FFC Technology Stack"},{"location":"Platform-Architecture/tech-radar/#technology-services-stack","text":"Entry Quadrant Ring Note Node.js Languages & Frameworks Adopt Hapi.js Languages & Frameworks Adopt NPM Languages & Frameworks Adopt .Net Languages & Frameworks Adopt Python Languages & Frameworks Assess Nuget Tooling Adopt Docker Tooling Adopt Docker Compose Tooling Adopt Helm Tooling Adopt Bicep Tooling Adopt Azure CLI Tooling Adopt PowerShell Tooling Adopt Azure boards Tooling Adopt Jenkins Tooling Hold Azure pipelines Tooling Adopt Jest Test Tooling Adopt xUnit Test Tooling Assess nUnit Test Tooling Adopt Pact Broker Test Tooling Adopt Web Driver IO Test Tooling Adopt Cucumber Test Tooling Adopt Selenium Test Tooling Adopt BrowserStack Test Tooling Adopt JMeter Test Tooling Adopt Snyk Test Tooling Adopt OWASP Zap Test Tooling Adopt AXE? Test Tooling Adopt WAVE Test Tooling Adopt Anchor Engine Test Tooling Adopt Azure Kubernetes Service Infrastructure Adopt Flux CD Infrastructure Adopt Azure Service Operator Infrastructure Adopt Azure Container Registry Infrastructure Adopt Azure PostgreSQL (flexible Server) Infrastructure Adopt Azure Service Bus Infrastructure Adopt Azure Event Hubs Infrastructure Assess Azure App Configuration Infrastructure Adopt Azure Key Vault Infrastructure Adopt Azure Functions Infrastructure Hold *must be containerized in AKS Azure Storage Infrastructure Adopt Entra ID workload Identity Infrastructure Adopt Application Insights Observability Adopt Azure Repos Tooling Hold GitHub Tooling Adopt SonarCloud Tooling Adopt Docker Desktop Tooling Adopt Google Analytics Observability Adopt Prometheus Observability Adopt Grafana Observability Adopt Azure Monitor Observability Adopt Visual Studio 2022 Tooling Adopt Visual Studio Code Tooling Adopt App Reg's Tooling Adopt Azure CosmosDB (SQL) Tooling Adopt Azure CosmosDB (Mongo) Tooling Assess Azure AI Studio Infrastructure Assess Azure Machine Learning Infrastructure Assess Azure Cognitive Services Infrastructure Assess Azure AI Search Infrastructure Assess Azure Prompt Flow Infrastructure Assess","title":"Technology &amp; Services Stack"},{"location":"Platform-Architecture/adp-portal/adp-copilot/","text":"ADP Copilot \u00b6 Overview of the ADP Copilot, a tool that provides a conversational interface to the Azure Development Platform (ADP). It outlines the features and capabilities of the ADP Copilot, such as the ability to interact with the ADP Portal, Azure DevOps, and GitHub using natural language. It describes how the ADP Copilot can be used to create, manage, and monitor resources in Azure, Azure DevOps, and GitHub, as well as how it can be used to automate tasks and workflows. The ADP Copilot is designed to streamline the development process and improve collaboration between team members by providing a unified interface for interacting with the ADP Platform. Key Features \u00b6 The ADP Copilot provides the following key features: Conversational Interface: Allows users to interact with the ADP Platform using natural language. Integration with ADP Documentation: Provides access to the ADP Documentation to view and search for information. Integration with ADP Portal: Provides access to the ADP Portal to view and manage resources. Architecture \u00b6 The ADP Copilot is built using the following components: ADP Documentation - Azure Pipeline \u00b6 Azure Pipeline for building and deploying the ADP Documentation. On commit to main branch to ADP External & Internal Documentation. Build and deploy the documentation to Azure Blob Storage. Run a python script to update the ADP Documentation search index storing each document into indexed/ vectorized chunks with the documentation formatter as metadata. Example of the metadata stored in formatter of the documentation: 1 2 3 4 5 6 7 8 --- title : ADP Copilot summary : Overview Architecture of ADP Copilot uri : https://defra.github.io/adp-documentation/Platform-Architecture/adp-portal/adp-copilot/ authors : - Logan Talbot date : 2024-04-22 --- Info All of these formatter fields are required for the documentation to be indexed correctly. ADP Portal API - AI Orchestrator \u00b6 ADP Portal API - used to the main AI orchestrator called as API endpoints by the ADP Portal. .NET Core Web API that uses Semantic Kernel to process the natural language queries made by the user and orchestrates the responses from the various services interacted. Semantic Kernel will use OpenAI GPT-4 to process the natural language queries made by the user. ADP Portal - Copilot \u00b6 ADP Portal will integrate a Chat Copilot into the UI allowing a user to interact with the ADP Platform using natural language. The Chat Copilot will called the ADP Portal API to process the natural language queries made by the user. Azure OpenAI - Models \u00b6 Uk South Azure OpenAI API used to process the natural language queries made by the user. This restricts which models can be used and the amount of data that can be processed. OpenAI GPT-4-turbo : Used to process the natural language queries made by the user. ADP will also experiment with other models like GPT 3.5 turbo . OpenAI text-embedding-ada-002 : Used vectorized and index the ADP Documentation to provide search capabilities. The preferred model would be text-embedding-3-large due to its capabilities but it is not available in any UK region. Azure AI Search - Search Index \u00b6 Azure AI Search Index used to store the vectorized and indexed ADP Documentation. There is current only one index used and requires no indexer to populate the index dye to a script that updates the index in the Azure Pipeline: Index fields: Field Name Type Retrievable Filterable Sortable Facetable Searchable Description id String Yes Yes No No No Unique identifier of the document content String Yes No No No Yes Content of the document content_vector SingleCollection Yes No No No Yes Vector representation of the document content title String Yes No No No Yes Title of the document source String Yes Yes No No No Source of the document uri String Yes Yes No No No URI of the document last_update DateTimeOffset Yes Yes No No No Last update timestamp of the document summary String Yes No No No No Summary of the document repository String Yes Yes No No No GitHub repository of the document metadata String Yes No No No Yes full metadata of a document Azure Cosmos DB - Chat History \u00b6 Azure Cosmos DB used to store the chat history of the user interactions with the ADP Copilot. This is used to provide a history of the interactions for an ADP user and to improve the AI orchestration used and auditing requirements ADP Copilot. Example: 1 Development Stages \u00b6 The ADP Copilot is currently in the development stage and is being built in the following stages: Note We are using the Intelligent Application Lifecycle to develop the ADP Copilot. Explore: Proof of Concept Integrate with the ADP Documentation (external only) with Manual local indexing to popular the Azure AI Search Index. Add front matter to all markdown files in the ADP Documentation (internal). Create a script to index the ADP Documentation including adding of metadata & chucking of the markdown files by headers. Basic conversational interface integrated with the ADP Portal. Basic Q&A AI Orchestrator using OpenAI GPT-4 & Azure AI Search in the ADP Portal API. Saving of chat history to Azure Cosmos DB. Increasing context of user interactions with the ability to include chat history in the AI Orchestrator. Build & Augment Adding to ADP Documentation Pipeline to update the search index with metadata on commit. Creation of infrastructure for the ADP Copilot. Including: Azure AI Search Azure Cosmos DB with Database & Containers Azure OpenAI API with models deployments (GPT-4-turbo & text-embedding-ada-002) Integrate external ADP Documentation with Azure AI Search Index. TBC Improve & Optimise TBC","title":"ADP Copilot"},{"location":"Platform-Architecture/adp-portal/adp-copilot/#adp-copilot","text":"Overview of the ADP Copilot, a tool that provides a conversational interface to the Azure Development Platform (ADP). It outlines the features and capabilities of the ADP Copilot, such as the ability to interact with the ADP Portal, Azure DevOps, and GitHub using natural language. It describes how the ADP Copilot can be used to create, manage, and monitor resources in Azure, Azure DevOps, and GitHub, as well as how it can be used to automate tasks and workflows. The ADP Copilot is designed to streamline the development process and improve collaboration between team members by providing a unified interface for interacting with the ADP Platform.","title":"ADP Copilot"},{"location":"Platform-Architecture/adp-portal/adp-copilot/#key-features","text":"The ADP Copilot provides the following key features: Conversational Interface: Allows users to interact with the ADP Platform using natural language. Integration with ADP Documentation: Provides access to the ADP Documentation to view and search for information. Integration with ADP Portal: Provides access to the ADP Portal to view and manage resources.","title":"Key Features"},{"location":"Platform-Architecture/adp-portal/adp-copilot/#architecture","text":"The ADP Copilot is built using the following components:","title":"Architecture"},{"location":"Platform-Architecture/adp-portal/adp-copilot/#adp-documentation-azure-pipeline","text":"Azure Pipeline for building and deploying the ADP Documentation. On commit to main branch to ADP External & Internal Documentation. Build and deploy the documentation to Azure Blob Storage. Run a python script to update the ADP Documentation search index storing each document into indexed/ vectorized chunks with the documentation formatter as metadata. Example of the metadata stored in formatter of the documentation: 1 2 3 4 5 6 7 8 --- title : ADP Copilot summary : Overview Architecture of ADP Copilot uri : https://defra.github.io/adp-documentation/Platform-Architecture/adp-portal/adp-copilot/ authors : - Logan Talbot date : 2024-04-22 --- Info All of these formatter fields are required for the documentation to be indexed correctly.","title":"ADP Documentation - Azure Pipeline"},{"location":"Platform-Architecture/adp-portal/adp-copilot/#adp-portal-api-ai-orchestrator","text":"ADP Portal API - used to the main AI orchestrator called as API endpoints by the ADP Portal. .NET Core Web API that uses Semantic Kernel to process the natural language queries made by the user and orchestrates the responses from the various services interacted. Semantic Kernel will use OpenAI GPT-4 to process the natural language queries made by the user.","title":"ADP Portal API - AI Orchestrator"},{"location":"Platform-Architecture/adp-portal/adp-copilot/#adp-portal-copilot","text":"ADP Portal will integrate a Chat Copilot into the UI allowing a user to interact with the ADP Platform using natural language. The Chat Copilot will called the ADP Portal API to process the natural language queries made by the user.","title":"ADP Portal - Copilot"},{"location":"Platform-Architecture/adp-portal/adp-copilot/#azure-openai-models","text":"Uk South Azure OpenAI API used to process the natural language queries made by the user. This restricts which models can be used and the amount of data that can be processed. OpenAI GPT-4-turbo : Used to process the natural language queries made by the user. ADP will also experiment with other models like GPT 3.5 turbo . OpenAI text-embedding-ada-002 : Used vectorized and index the ADP Documentation to provide search capabilities. The preferred model would be text-embedding-3-large due to its capabilities but it is not available in any UK region.","title":"Azure OpenAI - Models"},{"location":"Platform-Architecture/adp-portal/adp-copilot/#azure-ai-search-search-index","text":"Azure AI Search Index used to store the vectorized and indexed ADP Documentation. There is current only one index used and requires no indexer to populate the index dye to a script that updates the index in the Azure Pipeline: Index fields: Field Name Type Retrievable Filterable Sortable Facetable Searchable Description id String Yes Yes No No No Unique identifier of the document content String Yes No No No Yes Content of the document content_vector SingleCollection Yes No No No Yes Vector representation of the document content title String Yes No No No Yes Title of the document source String Yes Yes No No No Source of the document uri String Yes Yes No No No URI of the document last_update DateTimeOffset Yes Yes No No No Last update timestamp of the document summary String Yes No No No No Summary of the document repository String Yes Yes No No No GitHub repository of the document metadata String Yes No No No Yes full metadata of a document","title":"Azure AI Search - Search Index"},{"location":"Platform-Architecture/adp-portal/adp-copilot/#azure-cosmos-db-chat-history","text":"Azure Cosmos DB used to store the chat history of the user interactions with the ADP Copilot. This is used to provide a history of the interactions for an ADP user and to improve the AI orchestration used and auditing requirements ADP Copilot. Example: 1","title":"Azure Cosmos DB - Chat History"},{"location":"Platform-Architecture/adp-portal/adp-copilot/#development-stages","text":"The ADP Copilot is currently in the development stage and is being built in the following stages: Note We are using the Intelligent Application Lifecycle to develop the ADP Copilot. Explore: Proof of Concept Integrate with the ADP Documentation (external only) with Manual local indexing to popular the Azure AI Search Index. Add front matter to all markdown files in the ADP Documentation (internal). Create a script to index the ADP Documentation including adding of metadata & chucking of the markdown files by headers. Basic conversational interface integrated with the ADP Portal. Basic Q&A AI Orchestrator using OpenAI GPT-4 & Azure AI Search in the ADP Portal API. Saving of chat history to Azure Cosmos DB. Increasing context of user interactions with the ability to include chat history in the AI Orchestrator. Build & Augment Adding to ADP Documentation Pipeline to update the search index with metadata on commit. Creation of infrastructure for the ADP Copilot. Including: Azure AI Search Azure Cosmos DB with Database & Containers Azure OpenAI API with models deployments (GPT-4-turbo & text-embedding-ada-002) Integrate external ADP Documentation with Azure AI Search Index. TBC Improve & Optimise TBC","title":"Development Stages"},{"location":"Platform-Architecture/adp-portal/adp-portal-testing/","text":"ADP Portal Testing \u00b6 The table below details the roles and permissions that can be used for manual testing of the ADP portal and specific actions Name Role Permissions adptestuser1 Portal User No permissions but has portal access (Added to portal users group) adptestuser2 Team Member Non-tech team member of the Delivery Project team adptestuser3 Admin Tech Team Member Admin tech team member of the Delivery Project team adptestuser4 Programme Admin Delivery Programme Admin of a programme adptestuser5 Admin and Tech Member Admin for project A and also tech member for Project B","title":"ADP Portal Testing"},{"location":"Platform-Architecture/adp-portal/adp-portal-testing/#adp-portal-testing","text":"The table below details the roles and permissions that can be used for manual testing of the ADP portal and specific actions Name Role Permissions adptestuser1 Portal User No permissions but has portal access (Added to portal users group) adptestuser2 Team Member Non-tech team member of the Delivery Project team adptestuser3 Admin Tech Team Member Admin tech team member of the Delivery Project team adptestuser4 Programme Admin Delivery Programme Admin of a programme adptestuser5 Admin and Tech Member Admin for project A and also tech member for Project B","title":"ADP Portal Testing"},{"location":"Platform-Architecture/architectural-components/application-hosting/","text":"TODO This page is a work in progress and will be updated in due course. Application Hosting \u00b6 This article details the Service Application and Hosting Architecture for the solution at a high level. It will detail some of the initial decisions made and reasonings. This ADR is to record App Hosting services. Context - Application Hosting is a key part of building and delivering scalable and secure microservices for business services. TL;DR- ADP will build upon multiple Containerized Hosting options within Azure. Primarily, this will focus on Azure Kubernetes Service (AKS) to orchestrate, scale and run business services in a transparent manner. AKS has been chosen as a primary toolchain because of it's scalability, security and orchestration capabilities. Secondary options that are being continually evaluated and tested include Azure Container Apps (secondary) and Azure Functions (trigger or schedules). All applications that run be must containerized and the default choice is AKS. Requirements- All services must be containerized, and must support any containerized application Hosting options should be secure and scalable without degradation Must provide good orchestration & management of services Must promote CI/CD best practices and should support GitOps Should make best use of Cloud provider functionality and integrate well with Cloud Provider concepts and principles Meet Azure Well Architected Standards Support 100's of services without IP address constraints Should be built upon Open-Source technologies Decision - Primary : Azure Kubernetes Services (AKS) Secondary: Azure Container Apps (ACA) Azure Functions Azure Keyvault supports public CA integration with DigiCert, this is a fully Microsoft managed CSR submission and approval process, this will be used for \"Back end\" certificates in all environments. https://dev.azure.com/defragovuk/DEFRA-DEVOPS-COMMON/_git/Defra.Certificate.Renewals Approval- Platform Architecture Dan R, Mike B, Ken B.","title":"Application Hosting"},{"location":"Platform-Architecture/architectural-components/application-hosting/#application-hosting","text":"This article details the Service Application and Hosting Architecture for the solution at a high level. It will detail some of the initial decisions made and reasonings. This ADR is to record App Hosting services. Context - Application Hosting is a key part of building and delivering scalable and secure microservices for business services. TL;DR- ADP will build upon multiple Containerized Hosting options within Azure. Primarily, this will focus on Azure Kubernetes Service (AKS) to orchestrate, scale and run business services in a transparent manner. AKS has been chosen as a primary toolchain because of it's scalability, security and orchestration capabilities. Secondary options that are being continually evaluated and tested include Azure Container Apps (secondary) and Azure Functions (trigger or schedules). All applications that run be must containerized and the default choice is AKS. Requirements- All services must be containerized, and must support any containerized application Hosting options should be secure and scalable without degradation Must provide good orchestration & management of services Must promote CI/CD best practices and should support GitOps Should make best use of Cloud provider functionality and integrate well with Cloud Provider concepts and principles Meet Azure Well Architected Standards Support 100's of services without IP address constraints Should be built upon Open-Source technologies Decision - Primary : Azure Kubernetes Services (AKS) Secondary: Azure Container Apps (ACA) Azure Functions Azure Keyvault supports public CA integration with DigiCert, this is a fully Microsoft managed CSR submission and approval process, this will be used for \"Back end\" certificates in all environments. https://dev.azure.com/defragovuk/DEFRA-DEVOPS-COMMON/_git/Defra.Certificate.Renewals Approval- Platform Architecture Dan R, Mike B, Ken B.","title":"Application Hosting"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/","text":"Istio Architecture \u00b6 TODO This page is a work in progress and will be updated in due course. This document details the findings of Istio Service Mesh, including some features and integration with Flux. An Istio service mesh is logically split into a data plane and a control plane. The data plane is composed of a set of intelligent proxies (Envoy) deployed as sidecars. These proxies mediate and control all network communication between microservices. They also collect and report telemetry on all mesh traffic. The control plane manages and configures the proxies to route traffic. The following diagram shows the different components that make up each plane: Reference: Istio Architecture Installation \u00b6 Because we are using Nginx as our ingress controller, the following document was referenced to code the installation in adp-flux-core and adp-flux-services . NGINX Ingress Controller and Istio Service Mesh Features \u00b6 MTLS \u00b6 Istio automatically configures workload sidecars to use mutual TLS when calling other workloads. By default, Istio configures the destination workloads using PERMISSIVE mode. When PERMISSIVE mode is enabled, a service can accept both plaintext and mutual TLS traffic. In order to only allow mutual TLS traffic, the configuration needs to be changed to STRICT mode. Reference: MTLS Here is an example of applying STRICT mtls at the namespace level: 1 2 3 4 5 6 7 8 apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: ffc-demo spec: mtls: mode: STRICT Circuit Breaking \u00b6 Circuit breaking is an important pattern for creating resilient microservice applications. Circuit breaking allows you to write applications that limit the impact of failures, latency spikes, and other undesirable effects of network peculiarities. Reference: Circuit breaking Istio uses DestinationRule to configure circuit breakers. Here is a sample DestinationRule with circuit breaker rules: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: httpbin spec: host: httpbin trafficPolicy: connectionPool: tcp: maxConnections: 1 http: http1MaxPendingRequests: 1 # limits the number of requests that can be queued maxRequestsPerConnection: 1 # Previous request has to complete before next one is sent outlierDetection: consecutive5xxErrors: 2 interval: 1s baseEjectionTime: 30s maxEjectionPercent: 100 Fault Injection \u00b6 While Envoy sidecar/proxy provides a host of failure recovery mechanisms to services running on Istio, it is still imperative to test the end-to-end failure recovery capability of the application as a whole. Misconfigured failure recovery policies (e.g., incompatible/restrictive timeouts across service calls) could result in continued unavailability of critical services in the application, resulting in poor user experience. Istio enables protocol-specific fault injection into the network, instead of killing pods, delaying or corrupting packets at TCP layer. Our rationale is that the failures observed by the application layer are the same regardless of network level failures, and that more meaningful failures can be injected at the application layer (e.g., HTTP error codes) to exercise the resilience of an application. Operators can configure faults to be injected into requests that match specific criteria. Operators can further restrict the percentage of requests that should be subjected to faults. Two types of faults can be injected: delays and aborts. Delays are timing failures, mimicking increased network latency, or an overloaded upstream service. Aborts are crash failures that mimic failures in upstream services. Aborts usually manifest in the form of HTTP error codes, or TCP connection failures. Reference: Fault Injection , https://imesh.ai/blog/traffic-management-and-network-resiliency-with-istio-service-mesh/ Other Features \u00b6 Traffic Management Security Policy Enforcement Observability Observability tools \u00b6 Kiali Kiali is an observability console for Istio with service mesh configuration and validation capabilities. It helps you understand the structure and health of your service mesh by monitoring traffic flow to infer the topology and report errors. ) Reference: https://kiali.io/ , https://istio.io/latest/docs/ops/integrations/kiali/ Jaeger Jaeger is software for distributed tracing, which can help monitor and troubleshoot a complex microservices environment. Reference: https://www.jaegertracing.io/ , https://istio.io/latest/docs/tasks/observability/distributed-tracing/jaeger/ Multi-Cluster setup \u00b6 It is possible to have a multi-cluster setup for Istio. https://istio.io/latest/docs/setup/install/multicluster/ . Our setup is slightly different than the instructions because we are using Niginx Ingress Controller, so we will have to investigate how to get Multi-Cluster setup with Nginx and Istio. Istio Supported Releases \u00b6 Additional Notes: \u00b6 To enable end to end TLS, we will need Istio in place. We will need to investigate how to configure end to end TLS with Azure Frontdoor>Nginx>Istio There is also a Kiali plugin available for Backstage, which we should look into and implement if it's any good.","title":"Istio Service Mesh POC"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/#istio-architecture","text":"TODO This page is a work in progress and will be updated in due course. This document details the findings of Istio Service Mesh, including some features and integration with Flux. An Istio service mesh is logically split into a data plane and a control plane. The data plane is composed of a set of intelligent proxies (Envoy) deployed as sidecars. These proxies mediate and control all network communication between microservices. They also collect and report telemetry on all mesh traffic. The control plane manages and configures the proxies to route traffic. The following diagram shows the different components that make up each plane: Reference: Istio Architecture","title":"Istio Architecture"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/#installation","text":"Because we are using Nginx as our ingress controller, the following document was referenced to code the installation in adp-flux-core and adp-flux-services . NGINX Ingress Controller and Istio Service Mesh","title":"Installation"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/#features","text":"","title":"Features"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/#mtls","text":"Istio automatically configures workload sidecars to use mutual TLS when calling other workloads. By default, Istio configures the destination workloads using PERMISSIVE mode. When PERMISSIVE mode is enabled, a service can accept both plaintext and mutual TLS traffic. In order to only allow mutual TLS traffic, the configuration needs to be changed to STRICT mode. Reference: MTLS Here is an example of applying STRICT mtls at the namespace level: 1 2 3 4 5 6 7 8 apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: ffc-demo spec: mtls: mode: STRICT","title":"MTLS"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/#circuit-breaking","text":"Circuit breaking is an important pattern for creating resilient microservice applications. Circuit breaking allows you to write applications that limit the impact of failures, latency spikes, and other undesirable effects of network peculiarities. Reference: Circuit breaking Istio uses DestinationRule to configure circuit breakers. Here is a sample DestinationRule with circuit breaker rules: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: httpbin spec: host: httpbin trafficPolicy: connectionPool: tcp: maxConnections: 1 http: http1MaxPendingRequests: 1 # limits the number of requests that can be queued maxRequestsPerConnection: 1 # Previous request has to complete before next one is sent outlierDetection: consecutive5xxErrors: 2 interval: 1s baseEjectionTime: 30s maxEjectionPercent: 100","title":"Circuit Breaking"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/#fault-injection","text":"While Envoy sidecar/proxy provides a host of failure recovery mechanisms to services running on Istio, it is still imperative to test the end-to-end failure recovery capability of the application as a whole. Misconfigured failure recovery policies (e.g., incompatible/restrictive timeouts across service calls) could result in continued unavailability of critical services in the application, resulting in poor user experience. Istio enables protocol-specific fault injection into the network, instead of killing pods, delaying or corrupting packets at TCP layer. Our rationale is that the failures observed by the application layer are the same regardless of network level failures, and that more meaningful failures can be injected at the application layer (e.g., HTTP error codes) to exercise the resilience of an application. Operators can configure faults to be injected into requests that match specific criteria. Operators can further restrict the percentage of requests that should be subjected to faults. Two types of faults can be injected: delays and aborts. Delays are timing failures, mimicking increased network latency, or an overloaded upstream service. Aborts are crash failures that mimic failures in upstream services. Aborts usually manifest in the form of HTTP error codes, or TCP connection failures. Reference: Fault Injection , https://imesh.ai/blog/traffic-management-and-network-resiliency-with-istio-service-mesh/","title":"Fault Injection"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/#other-features","text":"Traffic Management Security Policy Enforcement Observability","title":"Other Features"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/#observability-tools","text":"Kiali Kiali is an observability console for Istio with service mesh configuration and validation capabilities. It helps you understand the structure and health of your service mesh by monitoring traffic flow to infer the topology and report errors. ) Reference: https://kiali.io/ , https://istio.io/latest/docs/ops/integrations/kiali/ Jaeger Jaeger is software for distributed tracing, which can help monitor and troubleshoot a complex microservices environment. Reference: https://www.jaegertracing.io/ , https://istio.io/latest/docs/tasks/observability/distributed-tracing/jaeger/","title":"Observability tools"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/#multi-cluster-setup","text":"It is possible to have a multi-cluster setup for Istio. https://istio.io/latest/docs/setup/install/multicluster/ . Our setup is slightly different than the instructions because we are using Niginx Ingress Controller, so we will have to investigate how to get Multi-Cluster setup with Nginx and Istio.","title":"Multi-Cluster setup"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/#istio-supported-releases","text":"","title":"Istio Supported Releases"},{"location":"Platform-Architecture/architectural-components/istio-service-mesh-poc/#additional-notes","text":"To enable end to end TLS, we will need Istio in place. We will need to investigate how to configure end to end TLS with Azure Frontdoor>Nginx>Istio There is also a Kiali plugin available for Backstage, which we should look into and implement if it's any good.","title":"Additional Notes:"},{"location":"Platform-Architecture/architectural-components/microservices-and-aks/","text":"Microservices & AKS \u00b6 This document details the example microservice architecture that is being developed on the Platform. It will detail the logic and physical separation between Clusters, Node Pools, Nodes, Namespaces and Pods and how they map to projects and programmes. Requirements \u00b6 Logical and Physical separation between environments, projects & programmes Allow for individual scalability and ensure other projects and programmes cannot starve other resources (resource limits and quotas) Decisions & Outcomes \u00b6 The System and User Nodepools will be separated. The System Nodepool will only contain system resources ( Flux, NGINX, Workload ID etc. ) Programmes will have separate Nodepools , per programme (BBaT, FCP, etc.) or specific resource requirements and demands There will be an individual Namespace per project in a given Nodepool ( Project A, Project B, namespace etc. ) Namespaces will have resource and quota limits defined NGINX Plus is the Ingress Controller (load balancing internal traffic) with a Private IP Azure Front Door is the edge WAF and CDN, with Public IP Node (VM) sizes to be defined / used existing FCP convention The ILB defined below allows a Private Link Service from the AFD to connect into AKS Examples \u00b6 The below diagram generally illustrates these requirements and separation. The namespaces provide the Logical Separation, and the separate Clusters provide Physical Separation. An example service is illustrated below, with the types of resources that can be deployed, how they integrate with the Hub/Spoke networking and Egress through the outbound firewall - CCoE Managed Palo Alto's.","title":"Microservices & AKS"},{"location":"Platform-Architecture/architectural-components/microservices-and-aks/#microservices-aks","text":"This document details the example microservice architecture that is being developed on the Platform. It will detail the logic and physical separation between Clusters, Node Pools, Nodes, Namespaces and Pods and how they map to projects and programmes.","title":"Microservices &amp; AKS"},{"location":"Platform-Architecture/architectural-components/microservices-and-aks/#requirements","text":"Logical and Physical separation between environments, projects & programmes Allow for individual scalability and ensure other projects and programmes cannot starve other resources (resource limits and quotas)","title":"Requirements"},{"location":"Platform-Architecture/architectural-components/microservices-and-aks/#decisions-outcomes","text":"The System and User Nodepools will be separated. The System Nodepool will only contain system resources ( Flux, NGINX, Workload ID etc. ) Programmes will have separate Nodepools , per programme (BBaT, FCP, etc.) or specific resource requirements and demands There will be an individual Namespace per project in a given Nodepool ( Project A, Project B, namespace etc. ) Namespaces will have resource and quota limits defined NGINX Plus is the Ingress Controller (load balancing internal traffic) with a Private IP Azure Front Door is the edge WAF and CDN, with Public IP Node (VM) sizes to be defined / used existing FCP convention The ILB defined below allows a Private Link Service from the AFD to connect into AKS","title":"Decisions &amp; Outcomes"},{"location":"Platform-Architecture/architectural-components/microservices-and-aks/#examples","text":"The below diagram generally illustrates these requirements and separation. The namespaces provide the Logical Separation, and the separate Clusters provide Physical Separation. An example service is illustrated below, with the types of resources that can be deployed, how they integrate with the Hub/Spoke networking and Egress through the outbound firewall - CCoE Managed Palo Alto's.","title":"Examples"},{"location":"Platform-Architecture/architectural-components/secrets-and-configuration/","text":"Secrets & Configuration \u00b6 Solution Overview \u00b6 Azure Architecture (green), per ADP environment: A single key vault which will be used by all business services to store their workload service's secrets. Each secret within this key vault will be RBAC controlled to selected Workload identity of the Workload service. A single App Configuration will store the configuration and links to key vault secrets for all business services and their workload services. Within the helm charts of each workload service, the App Configuration provider must filter via the label detailing the Workload Services name to get the applications new configuration. This will be applied to the AKS via flux. Inserting developer provided service secrets & configuration (black): Configuration of values and linking to key vault will be defined is the service repos. Within a appConfig folder at the root. (see section on Defining developer provided service secrets & configuration When the CI/CD pipeline is ran to build and deploy the workload service it will include these configuration files. The pipeline will provide an option to only push the configuration if needed. ADO Variable Groups will be used for the developers to define the secrets of their services which are not common or platform secrets (see ADO Variable Groups for Secrets) Secrets will be retrieved from the environment business service's ADO Variable Group but filtering on secrets which match the workload service name. Resulting in only the required secrets for the selected service being pushed into key vault. For any new secret in the key vault should have BBAC permissions assigned to allow the workload service's workload identity get the secret and allow the technical lead group of the business service to get & view the secret. Compile the appConfig files into a useable state (token replace, etc). Pushes the transformed appConfig into App Config within the chosen environment. Restart or deploy effected pods to allow configuration to be used by the application. Inserting common/ platform provided secrets & configuration for services to use (purple): Inside the ADP Infra Core on common platform configuration configuration and links key vaulted secrets will be defined as parameters within the App Configurations bicep params template. It will be deployed to the app configuration via the Infra Core pipeline in the same way any other infrastructure would be. These configurations will be assigned to label of \u201ccommon\u201d. In order to get common secrets into key vault ADP Platform team will need to input them manually in each environments key vault when necessary. Defining developer provided service secrets & configuration \u00b6 0.1 - 1 st pass \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 [ { \"key\": \"Logging:LogLevel:Debug\", \"value\": \"true\", \"label\": \"{{serviceName}}\", \"content_type\": \"\", \"tags\": {} }, { \"key\": \"exampleConfig\", \"value\": \"true\", \"label\": \"{{serviceName}}\", \"content_type\": \"\", \"tags\": {} }, { \"key\": \"exampleConfig2\", \"value\": \"Hello World! Welcome to {{serviceName}}, {{env}}!\", \"label\": \"{{serviceName}}\", \"content_type\": \"\", \"tags\": {} }, { \"key\": \"exampleSecret\", \"value\": \"{\\\"{{resourceid}}/{{servicename}}-examplesecret\\\"}\", \"label\": \"{{serviceName}}\", \"content_type\": \"{{keyVaultContentType}}\", \"tags\": {} } ] 1.0 - Ideal solution \u00b6 Example appConfig.yaml 1 2 3 4 5 6 7 8 9 - key: Logging:LogLevel:Debug value: \"true\" - key: exampleConfig value: \"true\" - key: exampleConfig2 value: Hello World! Welcome to {{serviceName}}, {{env}}! - key: exampleSecret value: \"{{servicename}}-examplesecret\" type: \"keyvault\" Example appConfig.dev.yaml 1 2 - key: exampleConfig3 value: My dev secret Example appConfig.test.yaml 1 2 - key: exampleConfig3 value: My test secret ADO Variable Groups for Secrets \u00b6 ADO Variable Groups will be created per environment per business service (namespace) and be created automatically at part of the business services setup. This will be used for each services secrets following this format {program}-{project}-{env}. For example: ffc-grants-snd1 ffc-grants-dev ffc-grants-test ffc-grants-pre ffc-grants-prd For secrets within these variable groups they will need to fellow this naming convention of {program}-{project}-{service}-{var name}. For example: ffc-gants-frontend-apikey1 ffc-gants-frontend-mysecret Developer Configuration \u00b6","title":"Secrets & Configuration"},{"location":"Platform-Architecture/architectural-components/secrets-and-configuration/#secrets-configuration","text":"","title":"Secrets &amp; Configuration"},{"location":"Platform-Architecture/architectural-components/secrets-and-configuration/#solution-overview","text":"Azure Architecture (green), per ADP environment: A single key vault which will be used by all business services to store their workload service's secrets. Each secret within this key vault will be RBAC controlled to selected Workload identity of the Workload service. A single App Configuration will store the configuration and links to key vault secrets for all business services and their workload services. Within the helm charts of each workload service, the App Configuration provider must filter via the label detailing the Workload Services name to get the applications new configuration. This will be applied to the AKS via flux. Inserting developer provided service secrets & configuration (black): Configuration of values and linking to key vault will be defined is the service repos. Within a appConfig folder at the root. (see section on Defining developer provided service secrets & configuration When the CI/CD pipeline is ran to build and deploy the workload service it will include these configuration files. The pipeline will provide an option to only push the configuration if needed. ADO Variable Groups will be used for the developers to define the secrets of their services which are not common or platform secrets (see ADO Variable Groups for Secrets) Secrets will be retrieved from the environment business service's ADO Variable Group but filtering on secrets which match the workload service name. Resulting in only the required secrets for the selected service being pushed into key vault. For any new secret in the key vault should have BBAC permissions assigned to allow the workload service's workload identity get the secret and allow the technical lead group of the business service to get & view the secret. Compile the appConfig files into a useable state (token replace, etc). Pushes the transformed appConfig into App Config within the chosen environment. Restart or deploy effected pods to allow configuration to be used by the application. Inserting common/ platform provided secrets & configuration for services to use (purple): Inside the ADP Infra Core on common platform configuration configuration and links key vaulted secrets will be defined as parameters within the App Configurations bicep params template. It will be deployed to the app configuration via the Infra Core pipeline in the same way any other infrastructure would be. These configurations will be assigned to label of \u201ccommon\u201d. In order to get common secrets into key vault ADP Platform team will need to input them manually in each environments key vault when necessary.","title":"Solution Overview"},{"location":"Platform-Architecture/architectural-components/secrets-and-configuration/#defining-developer-provided-service-secrets-configuration","text":"","title":"Defining developer provided service secrets &amp; configuration"},{"location":"Platform-Architecture/architectural-components/secrets-and-configuration/#01-1st-pass","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 [ { \"key\": \"Logging:LogLevel:Debug\", \"value\": \"true\", \"label\": \"{{serviceName}}\", \"content_type\": \"\", \"tags\": {} }, { \"key\": \"exampleConfig\", \"value\": \"true\", \"label\": \"{{serviceName}}\", \"content_type\": \"\", \"tags\": {} }, { \"key\": \"exampleConfig2\", \"value\": \"Hello World! Welcome to {{serviceName}}, {{env}}!\", \"label\": \"{{serviceName}}\", \"content_type\": \"\", \"tags\": {} }, { \"key\": \"exampleSecret\", \"value\": \"{\\\"{{resourceid}}/{{servicename}}-examplesecret\\\"}\", \"label\": \"{{serviceName}}\", \"content_type\": \"{{keyVaultContentType}}\", \"tags\": {} } ]","title":"0.1 - 1st pass"},{"location":"Platform-Architecture/architectural-components/secrets-and-configuration/#10-ideal-solution","text":"Example appConfig.yaml 1 2 3 4 5 6 7 8 9 - key: Logging:LogLevel:Debug value: \"true\" - key: exampleConfig value: \"true\" - key: exampleConfig2 value: Hello World! Welcome to {{serviceName}}, {{env}}! - key: exampleSecret value: \"{{servicename}}-examplesecret\" type: \"keyvault\" Example appConfig.dev.yaml 1 2 - key: exampleConfig3 value: My dev secret Example appConfig.test.yaml 1 2 - key: exampleConfig3 value: My test secret","title":"1.0 - Ideal solution"},{"location":"Platform-Architecture/architectural-components/secrets-and-configuration/#ado-variable-groups-for-secrets","text":"ADO Variable Groups will be created per environment per business service (namespace) and be created automatically at part of the business services setup. This will be used for each services secrets following this format {program}-{project}-{env}. For example: ffc-grants-snd1 ffc-grants-dev ffc-grants-test ffc-grants-pre ffc-grants-prd For secrets within these variable groups they will need to fellow this naming convention of {program}-{project}-{service}-{var name}. For example: ffc-gants-frontend-apikey1 ffc-gants-frontend-mysecret","title":"ADO Variable Groups for Secrets"},{"location":"Platform-Architecture/architectural-components/secrets-and-configuration/#developer-configuration","text":"","title":"Developer Configuration"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/","text":"Azure Service Operator for AKS \u00b6 What is Azure Service Operator? \u00b6 Azure Service Operator (ASO) allows you to deploy and maintain a wide variety of Azure Resources using the Kubernetes tooling you already know and use, i.e. HELM, YAML configuration files and FluxCD or Kubectl. Instead of deploying and managing your Azure resources separately from your Kubernetes application, ASO allows you to manage them together, automatically configuring your application as needed. For example, ASO can set up your Redis Cache or PostgreSQL database server and then configure your Kubernetes application to use them. Why use Azure Service Operator v2? \u00b6 K8s Native: we provide CRDs and Golang API structures to deploy and manage Azure resources through AKS. Azure Native: our CRDs understand Azure resource lifecycle and model it using K8s garbage collection via ownership references. Cloud Scale: we generate K8s CRDs from Azure Resource Manager schemas to move as fast as Azure. Async Reconciliation: we don\u2019t block on resource creation, and we can use things like FluxCD to manage infrastructure in a GitOps approach Developer Self-Service: it allows you, a developer, to manage and deploy your Azure infrastructure alongside your application, and be directly in control of your dependencies. Developer-centric tooling: our development community on AKS focuses on building applications hosted on AKS, and they can use their native toolsets, such as HELM Charts and YAML Configs to do it - without learning any other language, i.e. Bicep, ARM or Terraform. What is the Platform's goal here? To enable as much developer self-service as possible for 80% of common developer scenarios when building business services. Can I have an example diagram? - Sure! have one from Microsoft.. article here... What is our alternative approach? \u00b6 Currently, we have an approach where developers will use HELM Charts and FluxCD to self service deploy their applications to AKS in a GitOps manner securely. It is a developer-driven CI and CD processes without Platform team involvement for the majority of the work. Common HELM libraries are their to support you as well. With the addition of ASO, this expands to Azure infrastructure too ( storage, queues, identities ) outside of AKS that supports your applications. If ASO is not appropriate for the scenario or the component isn't supported, we can use our platform-native fallback: Azure Pipelines with Azure Bicep templates and/or PowerShell and CLI scripts. It's important to remember that Bicep and our supporting scripts are our bedrock, defined in such a way that allows for a scalable approach to manage a Service team's dedicated infrastructure. But it requires deeper context of Azure, the configuration and a good understanding of Bicep or PowerShell etc. With the dual track approach, we can scale and support a wide variety of teams, and have fallback options and Azure native tooling to ensure we can continue to deliver applications to production following assured processes. Any other tools exist to compete with ASO? CrossPlane Score What is supported for the Platforms ASO? \u00b6 Azure Component ASO Support? MVP? Self-Service? Manage base component? Description & considerations Resource Groups Y Y Y Y RG write on Sub only. PostgreSQL Database Y Y Y N Database write/read on Postgre only. Workload IDs assigned to specific per-service DBs. Managed Identities Y Y Y Y Can create and assign RBAC Storage Accounts Y Y Y Y Can create & manage, with templated support for networking & config Service Bus (Queues/Topics) Y Y Y N Can only create/read: Queues, Topics, Subscriptions, and RBAC on specific entities. Authorization (RBAC on components) Y Y Y N RBAC on designated components within Subscription. Azure Front Door Y N Y N TBD : allow self-creation of DNS entries, domains and routes to designated Cluster. What about other, unlisted resources? \u00b6 It is safe to assume, if it's not listed, it's not ASO supported and will be directly managed via Bicep templates, modules and PWSH Scripts. Further to this, you will not be able to fully manage the lifecycle of some resources, i.e. Service Bus or the PostgreSQL Flexible Server. This is by design as it's a Platform responsibility. How far will you roll ASO out? \u00b6 We simply don't know at this stage. It is in trial mode and our approach may differ as we expand, learn more and grow as a Platform. Azure Service Operator Setup \u00b6 We have setup ASO in a Single Operator mode with a multi-tenant configuration, enabling the use of separate credentials for managing resources in different Kubernetes namespace. Credential type \u00b6 Azure Service Operator supports four different styles of authentication today. Azure-Workload-Identity authentication (OIDC + Managed Identity) is being used by ASO in ADP. Credential scope \u00b6 Each supported credential type can be specified at one of three supported scopes: Global - The credential applies to all resources managed by ASO. Namespace - The credential applies to all resources managed by ASO in that namespace. Resource - The credential applies to only the specific resource it is referenced on. When presented with multiple credential choices, the operator chooses the most specific one: resource scope takes precedence over namespace scope which takes precedence over global scope. ASO in ADP is using Namespace scoped credentials. Each project team will have an ASO secret in their own namespace linked to a Managed Identity which will only have access to Azure resources the team should be allowed access to. The Platform Team will have their own ASO credential scoped at the Subscription level with Contributor and UAA access. This will allow the Platform Team to create Project Team resources using ASO. The Platform Team will create the following resources using ASO to onboard a Project Team: userassigned-identity federated-credential resource-group role-assignments-rg role-assignments-sb role-assignments-pgdb TODO \u00b6 We still need to work out how to inject certain values automatically into the ASO Kubernetes secrets managed by Flux. These are currently being added manually as post deployment step. The values we need to pass in for the Platform Team secret are: TenantID SubscrictionId ClientID The values we need to pass in for the Project Team secret and Managed Identity Federated credential are: TEAM_MI_CLIENT_ID CLUSTER_OIDC The below story has been raised to capture the above requirements: https://dev.azure.com/defragovuk/DEFRA-FFC/_workitems/edit/248355 We have also created a configmap which is manually installed on AKS in SND1. We did this so we didn't have these variables visible in our public repo. This will also need to be automated. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: ConfigMap metadata: name: platform-vars namespace: config-cluster-flux data: ENVIRONMENT: \"snd\" ACR_NAME: \"sndadpinfcr1401\" NAMESPACE: \"config-cluster-flux\" SERVICEBUS_RG: \"sndadpinfrg1401\" SERVICEBUS_NS: \"sndadpinfsb1401\" POSTGRES_SERVER_RG: \"sndadpdbsrg1401\" POSTGRES_SERVER: \"sndadpdbsps1401\" INFRA_RG: \"sndadpinfrg1401\" APPCONFIG_SERVICE: \"sndadpinfac1401\" TEAM_MI_PREFIX: \"sndadpinfmid1401\" CLUSTER_OIDC: \"https://uksouth.oic.prod-aks.azure.com/6f504113-6b64-43f2-ade9-242e05780007/e1f44edd-ac8e-4e4d-928a-2d2a7a52d4b7/\" TENANT_ID: \"6f504113-6b64-43f2-ade9-242e05780007\" SUBSCRIPTION_ID: \"55f3b8c6-6800-41c7-a40d-2adb5e4e1bd1\" SUBSCRIPTION_NAME: \"azd-adp-snd1\" CLUSTER: \"01\"","title":"Azure Service Operator for AKS"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/#azure-service-operator-for-aks","text":"","title":"Azure Service Operator for AKS"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/#what-is-azure-service-operator","text":"Azure Service Operator (ASO) allows you to deploy and maintain a wide variety of Azure Resources using the Kubernetes tooling you already know and use, i.e. HELM, YAML configuration files and FluxCD or Kubectl. Instead of deploying and managing your Azure resources separately from your Kubernetes application, ASO allows you to manage them together, automatically configuring your application as needed. For example, ASO can set up your Redis Cache or PostgreSQL database server and then configure your Kubernetes application to use them.","title":"What is Azure Service Operator?"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/#why-use-azure-service-operator-v2","text":"K8s Native: we provide CRDs and Golang API structures to deploy and manage Azure resources through AKS. Azure Native: our CRDs understand Azure resource lifecycle and model it using K8s garbage collection via ownership references. Cloud Scale: we generate K8s CRDs from Azure Resource Manager schemas to move as fast as Azure. Async Reconciliation: we don\u2019t block on resource creation, and we can use things like FluxCD to manage infrastructure in a GitOps approach Developer Self-Service: it allows you, a developer, to manage and deploy your Azure infrastructure alongside your application, and be directly in control of your dependencies. Developer-centric tooling: our development community on AKS focuses on building applications hosted on AKS, and they can use their native toolsets, such as HELM Charts and YAML Configs to do it - without learning any other language, i.e. Bicep, ARM or Terraform. What is the Platform's goal here? To enable as much developer self-service as possible for 80% of common developer scenarios when building business services. Can I have an example diagram? - Sure! have one from Microsoft.. article here...","title":"Why use Azure Service Operator v2?"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/#what-is-our-alternative-approach","text":"Currently, we have an approach where developers will use HELM Charts and FluxCD to self service deploy their applications to AKS in a GitOps manner securely. It is a developer-driven CI and CD processes without Platform team involvement for the majority of the work. Common HELM libraries are their to support you as well. With the addition of ASO, this expands to Azure infrastructure too ( storage, queues, identities ) outside of AKS that supports your applications. If ASO is not appropriate for the scenario or the component isn't supported, we can use our platform-native fallback: Azure Pipelines with Azure Bicep templates and/or PowerShell and CLI scripts. It's important to remember that Bicep and our supporting scripts are our bedrock, defined in such a way that allows for a scalable approach to manage a Service team's dedicated infrastructure. But it requires deeper context of Azure, the configuration and a good understanding of Bicep or PowerShell etc. With the dual track approach, we can scale and support a wide variety of teams, and have fallback options and Azure native tooling to ensure we can continue to deliver applications to production following assured processes. Any other tools exist to compete with ASO? CrossPlane Score","title":"What is our alternative approach?"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/#what-is-supported-for-the-platforms-aso","text":"Azure Component ASO Support? MVP? Self-Service? Manage base component? Description & considerations Resource Groups Y Y Y Y RG write on Sub only. PostgreSQL Database Y Y Y N Database write/read on Postgre only. Workload IDs assigned to specific per-service DBs. Managed Identities Y Y Y Y Can create and assign RBAC Storage Accounts Y Y Y Y Can create & manage, with templated support for networking & config Service Bus (Queues/Topics) Y Y Y N Can only create/read: Queues, Topics, Subscriptions, and RBAC on specific entities. Authorization (RBAC on components) Y Y Y N RBAC on designated components within Subscription. Azure Front Door Y N Y N TBD : allow self-creation of DNS entries, domains and routes to designated Cluster.","title":"What is supported for the Platforms ASO?"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/#what-about-other-unlisted-resources","text":"It is safe to assume, if it's not listed, it's not ASO supported and will be directly managed via Bicep templates, modules and PWSH Scripts. Further to this, you will not be able to fully manage the lifecycle of some resources, i.e. Service Bus or the PostgreSQL Flexible Server. This is by design as it's a Platform responsibility.","title":"What about other, unlisted resources?"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/#how-far-will-you-roll-aso-out","text":"We simply don't know at this stage. It is in trial mode and our approach may differ as we expand, learn more and grow as a Platform.","title":"How far will you roll ASO out?"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/#azure-service-operator-setup","text":"We have setup ASO in a Single Operator mode with a multi-tenant configuration, enabling the use of separate credentials for managing resources in different Kubernetes namespace.","title":"Azure Service Operator Setup"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/#credential-type","text":"Azure Service Operator supports four different styles of authentication today. Azure-Workload-Identity authentication (OIDC + Managed Identity) is being used by ASO in ADP.","title":"Credential type"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/#credential-scope","text":"Each supported credential type can be specified at one of three supported scopes: Global - The credential applies to all resources managed by ASO. Namespace - The credential applies to all resources managed by ASO in that namespace. Resource - The credential applies to only the specific resource it is referenced on. When presented with multiple credential choices, the operator chooses the most specific one: resource scope takes precedence over namespace scope which takes precedence over global scope. ASO in ADP is using Namespace scoped credentials. Each project team will have an ASO secret in their own namespace linked to a Managed Identity which will only have access to Azure resources the team should be allowed access to. The Platform Team will have their own ASO credential scoped at the Subscription level with Contributor and UAA access. This will allow the Platform Team to create Project Team resources using ASO. The Platform Team will create the following resources using ASO to onboard a Project Team: userassigned-identity federated-credential resource-group role-assignments-rg role-assignments-sb role-assignments-pgdb","title":"Credential scope"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/azure-service-operator-for-aks/#todo","text":"We still need to work out how to inject certain values automatically into the ASO Kubernetes secrets managed by Flux. These are currently being added manually as post deployment step. The values we need to pass in for the Platform Team secret are: TenantID SubscrictionId ClientID The values we need to pass in for the Project Team secret and Managed Identity Federated credential are: TEAM_MI_CLIENT_ID CLUSTER_OIDC The below story has been raised to capture the above requirements: https://dev.azure.com/defragovuk/DEFRA-FFC/_workitems/edit/248355 We have also created a configmap which is manually installed on AKS in SND1. We did this so we didn't have these variables visible in our public repo. This will also need to be automated. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: ConfigMap metadata: name: platform-vars namespace: config-cluster-flux data: ENVIRONMENT: \"snd\" ACR_NAME: \"sndadpinfcr1401\" NAMESPACE: \"config-cluster-flux\" SERVICEBUS_RG: \"sndadpinfrg1401\" SERVICEBUS_NS: \"sndadpinfsb1401\" POSTGRES_SERVER_RG: \"sndadpdbsrg1401\" POSTGRES_SERVER: \"sndadpdbsps1401\" INFRA_RG: \"sndadpinfrg1401\" APPCONFIG_SERVICE: \"sndadpinfac1401\" TEAM_MI_PREFIX: \"sndadpinfmid1401\" CLUSTER_OIDC: \"https://uksouth.oic.prod-aks.azure.com/6f504113-6b64-43f2-ade9-242e05780007/e1f44edd-ac8e-4e4d-928a-2d2a7a52d4b7/\" TENANT_ID: \"6f504113-6b64-43f2-ade9-242e05780007\" SUBSCRIPTION_ID: \"55f3b8c6-6800-41c7-a40d-2adb5e4e1bd1\" SUBSCRIPTION_NAME: \"azd-adp-snd1\" CLUSTER: \"01\"","title":"TODO"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/common-pipelines/","text":"Common Pipelines for delivery \u00b6 We are an Internal Development Platform that supports the delivery and maintenance of business services and applications for a variety of consumers. As part of this, we have a range of Common Pipelines that both Platform and Delivery teams can use to build and deploy their applications to all environments. Key Principles \u00b6 As a business service using the Azure Developer Platform (ADP), you are defined as a Platform Tenant. That means your 'service' or 'product' is deployed onto the ADP and follow certain standards and conventions to expedite delivery. All Pipelines are in Azure DevOps, written typically in YAML We support Service and Infrastructure deployments across all environments We use Bicep Modules to manage Infrastructure-as-Code All general configuration is in GitHub We can support pre-built packages from an external CI process, but we also have our own Infrastructure deployments \u00b6 Infrastructure is layered into the following levels: Product (Application) This represents an application (business service/product) and its associated infrastructure. Core (Platform) This represents the Core Platform components, and fully shared infrastructure Bootstrap (Platform) The minimum we need to deliver a baseline setup. Each level builds upon each other. So that means the Bootstrapping comes before the Core , and the Core before the Products are delivered. Finally, Service level, is the smallest deployment on the Platform and focuses on HELM (FuxCD) deployments and any DB Schema upgrades if required. We like to call these our continuous-delivery pipelines . In each environment, there will be exactly one set of all platform-level (Core & Bootstrap) infrastructure and exactly one set of each of the product-level infrastructure configurations are deployed. Finally, the Service-level are added as the most granular. Taken together, these infrastructure levels fully constitute an application / business service. Application/Service Deployments \u00b6 Service (Microservice) The Services deployment focuses on the HELM Chart Deployments, using FluxCD. This is a GitOps approach to application deployments and may contain database schema upgrades where relevant. This design can be found here. We also use this pipeline to deploy ADP Shared Services onto the AKS Cluster. The layered delivery - What's deployed by what? \u00b6 The following diagram shows the deployment layers, and the types of infrastructure that might be found in a given environment including the services. Pipeline Description Supported Components Bootstrap Contains pipelines used for bootstrapping e.g. setting up service connections VNET, Service Connections, Service Principals Core Contains pipelines used for install the ADP Platform Core resources e.g. AKS. These are all shared Platform components used by platform tenants. AKS Cluster, Networking (Vnet, NSGs, Subnets, UDRs), Key Vaults & config, Service Bus Core, Azure Front Door, Platform Storage Accounts, PostgreSQL Flexible server, Cosmos DB, Azure Redis Cache, App Insights, Log Analytics, Container Registry, AAD Configuration (App Registrations, AAD Groups etc.), App Configuration Service Core, Managed Identities Product Contains pipelines used for onboarding & deploying services onto the ADP Platform (i.e. their infrastructure components) Service Bus - Queues, Topics & Subscriptions, Managed Identities & RBAC, Service Databases, Front Door DNS Profiles (public and private), App Configuration Key-Value Pairs, KeyVault Key-Value pairs, App Registrations & AAD Groups, Service Storage Accounts (& tables/containers, etc.) Services Contains Service pipelines for deploying into AKS Cluster with FluxCD (GitOps Pipelines) Service & ADP HELM (Application) Deployments with FluxCD, Database Migrations (Liquibase?) CI Process \u00b6 TBC","title":"Common Pipelines"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/common-pipelines/#common-pipelines-for-delivery","text":"We are an Internal Development Platform that supports the delivery and maintenance of business services and applications for a variety of consumers. As part of this, we have a range of Common Pipelines that both Platform and Delivery teams can use to build and deploy their applications to all environments.","title":"Common Pipelines for delivery"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/common-pipelines/#key-principles","text":"As a business service using the Azure Developer Platform (ADP), you are defined as a Platform Tenant. That means your 'service' or 'product' is deployed onto the ADP and follow certain standards and conventions to expedite delivery. All Pipelines are in Azure DevOps, written typically in YAML We support Service and Infrastructure deployments across all environments We use Bicep Modules to manage Infrastructure-as-Code All general configuration is in GitHub We can support pre-built packages from an external CI process, but we also have our own","title":"Key Principles"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/common-pipelines/#infrastructure-deployments","text":"Infrastructure is layered into the following levels: Product (Application) This represents an application (business service/product) and its associated infrastructure. Core (Platform) This represents the Core Platform components, and fully shared infrastructure Bootstrap (Platform) The minimum we need to deliver a baseline setup. Each level builds upon each other. So that means the Bootstrapping comes before the Core , and the Core before the Products are delivered. Finally, Service level, is the smallest deployment on the Platform and focuses on HELM (FuxCD) deployments and any DB Schema upgrades if required. We like to call these our continuous-delivery pipelines . In each environment, there will be exactly one set of all platform-level (Core & Bootstrap) infrastructure and exactly one set of each of the product-level infrastructure configurations are deployed. Finally, the Service-level are added as the most granular. Taken together, these infrastructure levels fully constitute an application / business service.","title":"Infrastructure deployments"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/common-pipelines/#applicationservice-deployments","text":"Service (Microservice) The Services deployment focuses on the HELM Chart Deployments, using FluxCD. This is a GitOps approach to application deployments and may contain database schema upgrades where relevant. This design can be found here. We also use this pipeline to deploy ADP Shared Services onto the AKS Cluster.","title":"Application/Service Deployments"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/common-pipelines/#the-layered-delivery-whats-deployed-by-what","text":"The following diagram shows the deployment layers, and the types of infrastructure that might be found in a given environment including the services. Pipeline Description Supported Components Bootstrap Contains pipelines used for bootstrapping e.g. setting up service connections VNET, Service Connections, Service Principals Core Contains pipelines used for install the ADP Platform Core resources e.g. AKS. These are all shared Platform components used by platform tenants. AKS Cluster, Networking (Vnet, NSGs, Subnets, UDRs), Key Vaults & config, Service Bus Core, Azure Front Door, Platform Storage Accounts, PostgreSQL Flexible server, Cosmos DB, Azure Redis Cache, App Insights, Log Analytics, Container Registry, AAD Configuration (App Registrations, AAD Groups etc.), App Configuration Service Core, Managed Identities Product Contains pipelines used for onboarding & deploying services onto the ADP Platform (i.e. their infrastructure components) Service Bus - Queues, Topics & Subscriptions, Managed Identities & RBAC, Service Databases, Front Door DNS Profiles (public and private), App Configuration Key-Value Pairs, KeyVault Key-Value pairs, App Registrations & AAD Groups, Service Storage Accounts (& tables/containers, etc.) Services Contains Service pipelines for deploying into AKS Cluster with FluxCD (GitOps Pipelines) Service & ADP HELM (Application) Deployments with FluxCD, Database Migrations (Liquibase?)","title":"The layered delivery - What's deployed by what?"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/common-pipelines/#ci-process","text":"TBC","title":"CI Process"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/infrastructure-pipelines/","text":"Infrastructure Pipeline Enhancements \u00b6 The Platform core pipeline will be able to deploy all components (resources) that is within the defined environment. The pipeline will be able to deploy resources based on the resource category and/or resource type. This article describes the structure for the deployment of the resources based on ADO Pipelines and associated YAML files. Key Principles \u00b6 Prior to this enhancements, when deploying or testing a resource, developers will comment out some parts of codes within the deploy-platform-core.yaml file in order to deploy only the relevant templates. The other approach is to deploy the entire resources which takes more time and slows down processes. This latter approach may also come at an extra cost. Azure Resource Deployment \u00b6 Wih the enhancement, developers can deploy resources based on the below categories: - All - Network - All - Network - VNET - Network - NSGs - Network - Route Tables - Monitoring - All - Monitoring - Workspace - Monitoring - Insights - Monitoring - Azure Monitor Workspace - Monitoring - Grafana - Managed Cluster - All - Managed Cluster - Private DNS Zone - Managed Cluster - Cluster - Front Door - All - Front Door - Core - Front Door - WAF - Application - All - Application - App Configuration - Application - Apps Container Registry - Application - PostgreSql Server - Application - Redis Cache - Application - Storage Account - Application - Service Bus - Application - Key Vault Deployment process On ADO pipeline, the above category of resources can be selected in a dropdown menu. All resources can be deployed when the option 'All' is selected. This option will deploy all components of Network, Monitoring, Managed Cluster, Front Door and Application. However, if a developer or tester needs to deploy only network resources, the selection will be 'Network - All'. With this option, all the network resources (VNET, NSGS and Route Tables) will be deployed. Similarly, if the deployment is for testing a specific resource within network category, for example, VNET, they will be able to do so by selecting 'Network -VNET' option from the dropdown list. This will only deploy VNET template as defined in the yaml file. Implementation approach The categorisaton for resources have been applied with the use of 'parameters' defined within the deploy-platform-core-yaml file. The default value is 'All'. Furthermore, 'variables' have been used to define each of the values listed in the 'parameters' section. Conditional statements are subsequently applied to filter the resources in the 'groupedTemplates'.","title":"Infrastructure Pipelines"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/infrastructure-pipelines/#infrastructure-pipeline-enhancements","text":"The Platform core pipeline will be able to deploy all components (resources) that is within the defined environment. The pipeline will be able to deploy resources based on the resource category and/or resource type. This article describes the structure for the deployment of the resources based on ADO Pipelines and associated YAML files.","title":"Infrastructure Pipeline Enhancements"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/infrastructure-pipelines/#key-principles","text":"Prior to this enhancements, when deploying or testing a resource, developers will comment out some parts of codes within the deploy-platform-core.yaml file in order to deploy only the relevant templates. The other approach is to deploy the entire resources which takes more time and slows down processes. This latter approach may also come at an extra cost.","title":"Key Principles"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/infrastructure-pipelines/#azure-resource-deployment","text":"Wih the enhancement, developers can deploy resources based on the below categories: - All - Network - All - Network - VNET - Network - NSGs - Network - Route Tables - Monitoring - All - Monitoring - Workspace - Monitoring - Insights - Monitoring - Azure Monitor Workspace - Monitoring - Grafana - Managed Cluster - All - Managed Cluster - Private DNS Zone - Managed Cluster - Cluster - Front Door - All - Front Door - Core - Front Door - WAF - Application - All - Application - App Configuration - Application - Apps Container Registry - Application - PostgreSql Server - Application - Redis Cache - Application - Storage Account - Application - Service Bus - Application - Key Vault Deployment process On ADO pipeline, the above category of resources can be selected in a dropdown menu. All resources can be deployed when the option 'All' is selected. This option will deploy all components of Network, Monitoring, Managed Cluster, Front Door and Application. However, if a developer or tester needs to deploy only network resources, the selection will be 'Network - All'. With this option, all the network resources (VNET, NSGS and Route Tables) will be deployed. Similarly, if the deployment is for testing a specific resource within network category, for example, VNET, they will be able to do so by selecting 'Network -VNET' option from the dropdown list. This will only deploy VNET template as defined in the yaml file. Implementation approach The categorisaton for resources have been applied with the use of 'parameters' defined within the deploy-platform-core-yaml file. The default value is 'All'. Furthermore, 'variables' have been used to define each of the values listed in the 'parameters' section. Conditional statements are subsequently applied to filter the resources in the 'groupedTemplates'.","title":"Azure Resource Deployment"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/naming-conventions-and-structures/","text":"Pipeline Naming Conventions & Structures \u00b6 All pipelines on the Platform will have a specific set of naming conventions to ensure consistency, standardization and readability. This article describes the naming conventions of ADO Pipelines and associated YAML files. The conventions \u00b6 platform- <project> - <purpose> For the ADO Pipelines: ' Platform ' highlights the fact it is a Platform pipeline. 'Project' is always ADP for the Platform. 'Purpose' is typically either: core, bootstrap, product or service. For the YAML files, we try and maintain the same naming convention, except all files should be prefixed with: 'deploy' or 'ci' . Azure DevOps Pipelines \u00b6 Based on the types of pipelines and their purposes, the following naming conventions have been identified for ADO (UI Display Name): Core Infrastructure platform-adp-core platform-adp-bootstrap-serviceconnections Products platform-adp-products Services platform-adp-sevices <br> The ADO Pipelines will all be created in the existing DEFRA-FFC ADO project under the ADP Subfolder . Pipeline Matrix \u00b6 Folder Description Pipelines YAML Pipelines Bootstrap Contains pipelines used for bootstrapping e.g. setting up service connections platform-adp-bootstrap-serviceconnections deploy-platform-bootstrap-serviceconnections Core Contains pipelines used for install the ADP Platform Core resources e.g. AKS. These are all shared Platform components used by platform tenants. platform-adp-core deploy-platform-core Product Contains pipelines used for onboarding & deploying services onto the ADP Platform (i.e. their infrastructure components) platform-adp-products (not implemented yet) deploy-platform-products Services Contains Service pipelines for deploying into AKS Cluster with FluxCD (GitOps Pipelines) platform-adp-services (not implemented yet) deploy-platform-services Folder Structure - ADP Infrastructure \u00b6 Core infrastructure will reside within GitHub in ADP Infrastructure . There are planned to be other infrastructure and FluxCD repos, of which the design is currently in progress. It is proposed that the infrastructure that is dedicated for our tenant services (products) will reside with ADO-Infrastructure-Services*. All infrastructure will be within the 'infra' folder. ' Core ' designates the Platform Core Shared Infrastructure that is used by all the platform projects & services (tenants). ' Services ' designates that the folder contains only infrastructure and configuration dedicated to that project/service (tenant). Each Module instantiation file will be within it's own folder, broken down with the following conventions below (as per the Bicep Module registry convention): The following folder structure for Modules: Infra / <module-name> / <module-name> . <extension> The file extension will either be: . bicep or . biceparams for infrastructure modules. For PowerShell and PowerShell modules the extension will be: . ps1 or psm1 For .yaml for YAML Files (i.e. ADO Pipelines) Infrastructure repository diagram for ADP-Infra-Core and ADP-Infra-Services \u00b6","title":"Naming Conventions"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/naming-conventions-and-structures/#pipeline-naming-conventions-structures","text":"All pipelines on the Platform will have a specific set of naming conventions to ensure consistency, standardization and readability. This article describes the naming conventions of ADO Pipelines and associated YAML files.","title":"Pipeline Naming Conventions &amp; Structures"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/naming-conventions-and-structures/#the-conventions","text":"platform- <project> - <purpose> For the ADO Pipelines: ' Platform ' highlights the fact it is a Platform pipeline. 'Project' is always ADP for the Platform. 'Purpose' is typically either: core, bootstrap, product or service. For the YAML files, we try and maintain the same naming convention, except all files should be prefixed with: 'deploy' or 'ci' .","title":"The conventions"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/naming-conventions-and-structures/#azure-devops-pipelines","text":"Based on the types of pipelines and their purposes, the following naming conventions have been identified for ADO (UI Display Name): Core Infrastructure platform-adp-core platform-adp-bootstrap-serviceconnections Products platform-adp-products Services platform-adp-sevices <br> The ADO Pipelines will all be created in the existing DEFRA-FFC ADO project under the ADP Subfolder .","title":"Azure DevOps Pipelines"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/naming-conventions-and-structures/#pipeline-matrix","text":"Folder Description Pipelines YAML Pipelines Bootstrap Contains pipelines used for bootstrapping e.g. setting up service connections platform-adp-bootstrap-serviceconnections deploy-platform-bootstrap-serviceconnections Core Contains pipelines used for install the ADP Platform Core resources e.g. AKS. These are all shared Platform components used by platform tenants. platform-adp-core deploy-platform-core Product Contains pipelines used for onboarding & deploying services onto the ADP Platform (i.e. their infrastructure components) platform-adp-products (not implemented yet) deploy-platform-products Services Contains Service pipelines for deploying into AKS Cluster with FluxCD (GitOps Pipelines) platform-adp-services (not implemented yet) deploy-platform-services","title":"Pipeline Matrix"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/naming-conventions-and-structures/#folder-structure-adp-infrastructure","text":"Core infrastructure will reside within GitHub in ADP Infrastructure . There are planned to be other infrastructure and FluxCD repos, of which the design is currently in progress. It is proposed that the infrastructure that is dedicated for our tenant services (products) will reside with ADO-Infrastructure-Services*. All infrastructure will be within the 'infra' folder. ' Core ' designates the Platform Core Shared Infrastructure that is used by all the platform projects & services (tenants). ' Services ' designates that the folder contains only infrastructure and configuration dedicated to that project/service (tenant). Each Module instantiation file will be within it's own folder, broken down with the following conventions below (as per the Bicep Module registry convention): The following folder structure for Modules: Infra / <module-name> / <module-name> . <extension> The file extension will either be: . bicep or . biceparams for infrastructure modules. For PowerShell and PowerShell modules the extension will be: . ps1 or psm1 For .yaml for YAML Files (i.e. ADO Pipelines)","title":"Folder Structure - ADP Infrastructure"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/naming-conventions-and-structures/#infrastructure-repository-diagram-for-adp-infra-core-and-adp-infra-services","text":"","title":"Infrastructure repository diagram for ADP-Infra-Core and ADP-Infra-Services"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/application-deployments/","text":"Application Deployments \u00b6 Design Principles - Application repositories will contain the application source code, docker files and helm charts to deploy the application. 1 2 3 4 5 6 7 8 9 10 Application-repo \u251c\u2500 Dockerfile \u251c\u2500 src/ \u2502 \u2514\u2500 .... \u2514\u2500 manifests/ \u2514\u2500 helm/ \u251c\u2500 Chart.yaml \u251c\u2500 values.yaml \u2514\u2500 templates/ \u2514\u2500 .... - All application deployments are managed with HelmRelease . - All GitOps environments should use the main branch. Approaches such as a branch per environment have downsides and should be avoided. - Each environment will have an environment-specific Azure Container Registry that the CI pipeline will push the docker images and artifacts (helm charts etc) to. Promoting Changes across Environments \u00b6 CI/CD pipelines should support both continuous deployment and continuous delivery.","title":"Application Deployments"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/application-deployments/#application-deployments","text":"Design Principles - Application repositories will contain the application source code, docker files and helm charts to deploy the application. 1 2 3 4 5 6 7 8 9 10 Application-repo \u251c\u2500 Dockerfile \u251c\u2500 src/ \u2502 \u2514\u2500 .... \u2514\u2500 manifests/ \u2514\u2500 helm/ \u251c\u2500 Chart.yaml \u251c\u2500 values.yaml \u2514\u2500 templates/ \u2514\u2500 .... - All application deployments are managed with HelmRelease . - All GitOps environments should use the main branch. Approaches such as a branch per environment have downsides and should be avoided. - Each environment will have an environment-specific Azure Container Registry that the CI pipeline will push the docker images and artifacts (helm charts etc) to.","title":"Application Deployments"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/application-deployments/#promoting-changes-across-environments","text":"CI/CD pipelines should support both continuous deployment and continuous delivery.","title":"Promoting Changes across Environments"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-configuration/","text":"Flux Configuration \u00b6 Flux Build Deployment Completion Trigger \u00b6 Context : These are the findings from the 'Build Deployment completion Trigger' Spike. The goal of the findings is to be able to Execute Post deployment tests when Flux CD has completed deploying a new application\u200b. Example of how to configure Event Hub as an external source and send notifications to it: \u00b6 We can utilize the flux notification controller to dispatch events to external systems (Azure Function, Azure Event Hub, Slack, Teams) https://fluxcd.io/flux/components/notification/ Reference: https://github.com/fluxcd/notification-controller/blob/main/docs/spec/v1beta2/providers.md#sas-based-auth Using SAS based authentication: Create a secret containing the shared access key: kubectl create secret generic webhook-url --from-literal=address=\"Endpoint=sb://events-hub-adp-poc.servicebus.windows.net/;SharedAccessKeyName=flux-notifications;SharedAccessKey=an1ZOt9v90oycqy67rbcnEoXaIGecBLAH+AEhD/vy1g=;EntityPath=flux-events\" Create a Provider resource for Event Hub: 1 2 3 4 5 6 7 8 9 apiVersion: notification.toolkit.fluxcd.io/v1beta2 kind: Provider metadata: name: azureeventhub namespace: flux-config spec: type: azureeventhub secretRef: name: webhook-url Create an Alert resource for the type of Alerts we want to monitor: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: notification.toolkit.fluxcd.io/v1beta2 kind: Alert metadata: name: azureeventhub-alert namespace: flux-config spec: providerRef: name: azureeventhub eventSeverity: info eventSources: - kind: HelmRelease name: '*' inclusionList: - \".*succeeded.*\" Example ADO Pipeline Callback task \u00b6 To make ADO pipeline wait for Flux deployment completion we can utilize the AzureFunction@1 task. This will allow us to call an azure function asynchronously. The function can then poll a database ( or queue? ) and when a HelmRelease completion entry appears for the service, it will call back to the pipeline to continue: Example AzureFunction@1 yaml: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 stages: - stage: TestStage jobs: - job: TestJob pool: server timeoutInMinutes: 10 steps: - task: AzureFunction@1 inputs: function: 'https://adopipelineasyncfuncpoc.azurewebsites.net/api/AdoCallBack' key: '$(callBackKey)' method: 'POST' body: | '{ \"helmReleaseName\": \"ffc-demo-web-infra-post-deploy\", \"helmReleaseVersion\": \"4.32.30\" }' waitForCompletion: 'true' Example of Function App that uses the callback completion mode Configuring Flux V2 on AKS \u00b6 Enable Flux Extension on the AKS Cluster \u00b6 GitOps with Flux v2 is enabled as a cluster extension in Azure Kubernetes Service (AKS) clusters. The microsoft.flux cluster extension is installed using a bicep template aks-cluster.bicep and an ADP Infra Pipeline platform-adp-core . Below is a snippet of the code for enabling Flux on AKS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fluxExtension: { autoUpgradeMinorVersion: true releaseTrain: 'Stable' configurationSettings: { 'helm-controller.enabled': 'true' 'source-controller.enabled': 'true' 'kustomize-controller.enabled': 'true' 'notification-controller.enabled': 'true' 'image-automation-controller.enabled': 'false' 'image-reflector-controller.enabled': 'false' } configurations: [ ... ] Add Flux Customizations \u00b6 After the microsoft.flux cluster extension has been installed, create a fluxConfiguration resource that syncs the Git repository source adp-flux-core to the cluster and reconcile the cluster to the desired state. With GitOps, the Git repository is the source of truth for cluster configuration and application deployment. The Flux configuration links Flux to the ADP Flux Git repository and defines: The git repository that Flux should use The branch you want to use e.g. main The root Kustomization objects to run, which will then be used to deploy the rest of workloads (core services and business applications). Refer to the documentation for the Flux repositories structure for details of the two Flux repositories ( adp-flux-core and adp-flux-services ) and folder structures. The Flux Configuration has three Kustomizations Kustomization Path Purpose cluster ./clusters/ /01 Cluster level configurations e.g. Flux Controllers, CRDs infra ./infra/ /01 Core Services e.g. Nginx Plus Depends on the cluster Kustomization services ./services/ /01 Business applications Depends on the services Kustomization The Kustomizations have been configured with dependencies to ensure the Flux deployments are done in the correct sequence, starting with the Cluster Kustomization, followed by Infra and lastly Services . Below is a snippet of the Kustomizations configuration from aks-cluster.bicep and aks-cluster.parameters.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 kustomizations: { cluster: { path: fluxConfig.clusterCore.kustomizations.clusterPath dependsOn: [] timeoutInSeconds: fluxConfig.clusterCore.kustomizations.timeoutInSeconds syncIntervalInSeconds: fluxConfig.clusterCore.kustomizations.syncIntervalInSeconds validation: 'none' prune: true } infra: { path: fluxConfig.clusterCore.kustomizations.infraPath timeoutInSeconds: fluxConfig.clusterCore.kustomizations.timeoutInSeconds syncIntervalInSeconds: fluxConfig.clusterCore.kustomizations.syncIntervalInSeconds dependsOn: [ 'cluster' ] validation: 'none' prune: true } services: { path: fluxConfig.services.kustomizations.servicesPath timeoutInSeconds: fluxConfig.services.kustomizations.timeoutInSeconds syncIntervalInSeconds: fluxConfig.services.kustomizations.syncIntervalInSeconds retryIntervalInSeconds: fluxConfig.services.kustomizations.retryIntervalInSeconds dependsOn: [ 'infra' ] prune: true } } Although we have two flux Git repositories, we are using a single Flux customisation because we cannot set dependencies at the Flux Customisation level. Instead, we have a single Flux Customisation with three Kustomizations that will be deployed in sequential order Cluster > Infra > Services . The Services Flux configuration contains a GitRepository and a Kustomization file that points to the Services Flux git repostory adp-flux-services using the path ./services/environments/<environment>/01","title":"Flux Configuration"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-configuration/#flux-configuration","text":"","title":"Flux Configuration"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-configuration/#flux-build-deployment-completion-trigger","text":"Context : These are the findings from the 'Build Deployment completion Trigger' Spike. The goal of the findings is to be able to Execute Post deployment tests when Flux CD has completed deploying a new application\u200b.","title":"Flux Build Deployment Completion Trigger"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-configuration/#example-of-how-to-configure-event-hub-as-an-external-source-and-send-notifications-to-it","text":"We can utilize the flux notification controller to dispatch events to external systems (Azure Function, Azure Event Hub, Slack, Teams) https://fluxcd.io/flux/components/notification/ Reference: https://github.com/fluxcd/notification-controller/blob/main/docs/spec/v1beta2/providers.md#sas-based-auth Using SAS based authentication: Create a secret containing the shared access key: kubectl create secret generic webhook-url --from-literal=address=\"Endpoint=sb://events-hub-adp-poc.servicebus.windows.net/;SharedAccessKeyName=flux-notifications;SharedAccessKey=an1ZOt9v90oycqy67rbcnEoXaIGecBLAH+AEhD/vy1g=;EntityPath=flux-events\" Create a Provider resource for Event Hub: 1 2 3 4 5 6 7 8 9 apiVersion: notification.toolkit.fluxcd.io/v1beta2 kind: Provider metadata: name: azureeventhub namespace: flux-config spec: type: azureeventhub secretRef: name: webhook-url Create an Alert resource for the type of Alerts we want to monitor: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: notification.toolkit.fluxcd.io/v1beta2 kind: Alert metadata: name: azureeventhub-alert namespace: flux-config spec: providerRef: name: azureeventhub eventSeverity: info eventSources: - kind: HelmRelease name: '*' inclusionList: - \".*succeeded.*\"","title":"Example of how to configure Event Hub as an external source and send notifications to it:"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-configuration/#example-ado-pipeline-callback-task","text":"To make ADO pipeline wait for Flux deployment completion we can utilize the AzureFunction@1 task. This will allow us to call an azure function asynchronously. The function can then poll a database ( or queue? ) and when a HelmRelease completion entry appears for the service, it will call back to the pipeline to continue: Example AzureFunction@1 yaml: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 stages: - stage: TestStage jobs: - job: TestJob pool: server timeoutInMinutes: 10 steps: - task: AzureFunction@1 inputs: function: 'https://adopipelineasyncfuncpoc.azurewebsites.net/api/AdoCallBack' key: '$(callBackKey)' method: 'POST' body: | '{ \"helmReleaseName\": \"ffc-demo-web-infra-post-deploy\", \"helmReleaseVersion\": \"4.32.30\" }' waitForCompletion: 'true' Example of Function App that uses the callback completion mode","title":"Example ADO Pipeline Callback task"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-configuration/#configuring-flux-v2-on-aks","text":"","title":"Configuring Flux V2 on AKS"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-configuration/#enable-flux-extension-on-the-aks-cluster","text":"GitOps with Flux v2 is enabled as a cluster extension in Azure Kubernetes Service (AKS) clusters. The microsoft.flux cluster extension is installed using a bicep template aks-cluster.bicep and an ADP Infra Pipeline platform-adp-core . Below is a snippet of the code for enabling Flux on AKS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fluxExtension: { autoUpgradeMinorVersion: true releaseTrain: 'Stable' configurationSettings: { 'helm-controller.enabled': 'true' 'source-controller.enabled': 'true' 'kustomize-controller.enabled': 'true' 'notification-controller.enabled': 'true' 'image-automation-controller.enabled': 'false' 'image-reflector-controller.enabled': 'false' } configurations: [ ... ]","title":"Enable Flux Extension on the AKS Cluster"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-configuration/#add-flux-customizations","text":"After the microsoft.flux cluster extension has been installed, create a fluxConfiguration resource that syncs the Git repository source adp-flux-core to the cluster and reconcile the cluster to the desired state. With GitOps, the Git repository is the source of truth for cluster configuration and application deployment. The Flux configuration links Flux to the ADP Flux Git repository and defines: The git repository that Flux should use The branch you want to use e.g. main The root Kustomization objects to run, which will then be used to deploy the rest of workloads (core services and business applications). Refer to the documentation for the Flux repositories structure for details of the two Flux repositories ( adp-flux-core and adp-flux-services ) and folder structures. The Flux Configuration has three Kustomizations Kustomization Path Purpose cluster ./clusters/ /01 Cluster level configurations e.g. Flux Controllers, CRDs infra ./infra/ /01 Core Services e.g. Nginx Plus Depends on the cluster Kustomization services ./services/ /01 Business applications Depends on the services Kustomization The Kustomizations have been configured with dependencies to ensure the Flux deployments are done in the correct sequence, starting with the Cluster Kustomization, followed by Infra and lastly Services . Below is a snippet of the Kustomizations configuration from aks-cluster.bicep and aks-cluster.parameters.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 kustomizations: { cluster: { path: fluxConfig.clusterCore.kustomizations.clusterPath dependsOn: [] timeoutInSeconds: fluxConfig.clusterCore.kustomizations.timeoutInSeconds syncIntervalInSeconds: fluxConfig.clusterCore.kustomizations.syncIntervalInSeconds validation: 'none' prune: true } infra: { path: fluxConfig.clusterCore.kustomizations.infraPath timeoutInSeconds: fluxConfig.clusterCore.kustomizations.timeoutInSeconds syncIntervalInSeconds: fluxConfig.clusterCore.kustomizations.syncIntervalInSeconds dependsOn: [ 'cluster' ] validation: 'none' prune: true } services: { path: fluxConfig.services.kustomizations.servicesPath timeoutInSeconds: fluxConfig.services.kustomizations.timeoutInSeconds syncIntervalInSeconds: fluxConfig.services.kustomizations.syncIntervalInSeconds retryIntervalInSeconds: fluxConfig.services.kustomizations.retryIntervalInSeconds dependsOn: [ 'infra' ] prune: true } } Although we have two flux Git repositories, we are using a single Flux customisation because we cannot set dependencies at the Flux Customisation level. Instead, we have a single Flux Customisation with three Kustomizations that will be deployed in sequential order Cluster > Infra > Services . The Services Flux configuration contains a GitRepository and a Kustomization file that points to the Services Flux git repostory adp-flux-services using the path ./services/environments/<environment>/01","title":"Add Flux Customizations"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-manifests/","text":"Flux Manifests \u00b6 Flux manifests are YAML files that define the desired state of your Kubernetes resources. They are essential for managing and automating the deployment of applications and infrastructure in a Kubernetes cluster. Why We Need Flux Manifests \u00b6 Declarative Configuration : Flux manifests enable you to declare the desired state of your resources, making it easier to manage and maintain configurations. Version Control : By storing manifests in a Git repository, you can track changes, roll back to previous states, and collaborate with others. Automation : Flux continuously monitors the Git repository and applies changes to the cluster, ensuring that the cluster state matches the desired state defined in the manifests. Types of Flux Manifests \u00b6 Kustomization: Defines how to customize Kubernetes resources using Kustomize. HelmRelease: Manages Helm charts and releases. GitRepository: Specifies the source Git repository containing the manifests. HelmRepository: Specifies the source Helm repository containing the manifests. ImagePolicy: Specifies policies for automated image updates. ImageRepository: Defines the source container image repository. Microsoft Flux Extension for AKS \u00b6 The Microsoft Flux extension for Azure Kubernetes Service (AKS) integrates GitOps capabilities directly into AKS clusters. So rather than managing the set up of Flux, this extension is enabled and configured on the cluster. It simplifies the management of Kubernetes clusters by leveraging GitOps principles, making it easier to maintain, scale, and secure your deployments Flux Operator \u00b6 The Flux Operator is a Kubernetes controller that manages the lifecycle of Flux CD. It automates the installation, configuration, and upgrade of Flux controllers based on a declarative API Generating the Flux Manifests \u00b6 Flux manifests are used by the Flux GitOps Operator to deploy the services into the AKS cluster. The flux manifests are generated either when scaffolding a new service using the ADP Portal or when the CD pipeline is deploying a service to a new environment (for which the flux manifests don\u2019t exist or require updating). A Delivery Project is created following the process described in the Onboarding a delivery project section . The portal will make an API POST request to the backend API to create the Team specific AD Groups to be used for RBAC. The Backend API will populate the details of the teams in the Teams config repository. On creation of the project, the teams config will be updated (team specific AD groups). The Backend API will then sync the group changes and create the AD Groups in Microsoft Entra ID. This occurs on creation and update on the delivery project information in the ADP Portal. Once a Delivery Project has been created, the development teams can then scafford one or more services following the documentation in the guide How to create a service guide When a user submits the service template form that the engineer has completed, the portal will make API requests to the backend service. The Backend service will start by writing the information to the Teams config. The API will then create/update the Flux manifests for the Sandpit environments in the Flux Services repository. For upstream environments (dev, tst, pre and prd), the CI pipeline will add/update the flux manifests on demand during the deployment of the services to that environment. Flux will reconcile and deploy the application. Process for making changes to the Flux Templates \u00b6 The flux generation process is split into two environments, Sandpit and Production. The sandpit environment will be used the core ADP Platform Team to test new enhancements and issues pertaining to scaffolder whilst the Production environment will be used for all environments used by the Development Teams to development, testing and deploying live services. Each Flux Environment comprises of the following repositories. \u2022 adp-teams-config: This GitHub repository contains the configuration used to define and generate the flux manifests. The configuration is defined at tenant, environment, and service level. This GitHub repository\u2019s visibility has been set to internal. \u2022 adp-flux-core: This GitHub repository contains the flux manifests for deploying the cluster and platform level (core) resources e.g., Nginx Ingress controller, Azure Service Operator, Cert Manager, and other services. This GitHub repository\u2019s visibility has been set to public. \u2022 adp-flux-services: This GitHub repository contains the flux manifests for deploy service level resources. Each deployed microservice that is running in the cluster will have a set of flux manifests that define the configuration Flux requires to deploy the service. No manual process is required for adding the manifests. This has been automated using an API. \u2022 ADP Portal: Users of the Portal can use a set of defined Software Backend and Frontend Templates to scaffold a new service. This leverages an API that scaffolds updates the configuration in the repository adp-teams-config and then scaffolds the flux manifests in the repository adp-flux-services. Github Organisations used for Flux Manifests Scaffolding Sandpit - the defra adp sandpit org is used by the platform team. Production - main DEFRA Github Org Each of these will contain the following repositories adp-teams-config adp-flux-core adp-flux-services","title":"Generating Flux Manifests"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-manifests/#flux-manifests","text":"Flux manifests are YAML files that define the desired state of your Kubernetes resources. They are essential for managing and automating the deployment of applications and infrastructure in a Kubernetes cluster.","title":"Flux Manifests"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-manifests/#why-we-need-flux-manifests","text":"Declarative Configuration : Flux manifests enable you to declare the desired state of your resources, making it easier to manage and maintain configurations. Version Control : By storing manifests in a Git repository, you can track changes, roll back to previous states, and collaborate with others. Automation : Flux continuously monitors the Git repository and applies changes to the cluster, ensuring that the cluster state matches the desired state defined in the manifests.","title":"Why We Need Flux Manifests"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-manifests/#types-of-flux-manifests","text":"Kustomization: Defines how to customize Kubernetes resources using Kustomize. HelmRelease: Manages Helm charts and releases. GitRepository: Specifies the source Git repository containing the manifests. HelmRepository: Specifies the source Helm repository containing the manifests. ImagePolicy: Specifies policies for automated image updates. ImageRepository: Defines the source container image repository.","title":"Types of Flux Manifests"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-manifests/#microsoft-flux-extension-for-aks","text":"The Microsoft Flux extension for Azure Kubernetes Service (AKS) integrates GitOps capabilities directly into AKS clusters. So rather than managing the set up of Flux, this extension is enabled and configured on the cluster. It simplifies the management of Kubernetes clusters by leveraging GitOps principles, making it easier to maintain, scale, and secure your deployments","title":"Microsoft Flux Extension for AKS"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-manifests/#flux-operator","text":"The Flux Operator is a Kubernetes controller that manages the lifecycle of Flux CD. It automates the installation, configuration, and upgrade of Flux controllers based on a declarative API","title":"Flux Operator"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-manifests/#generating-the-flux-manifests","text":"Flux manifests are used by the Flux GitOps Operator to deploy the services into the AKS cluster. The flux manifests are generated either when scaffolding a new service using the ADP Portal or when the CD pipeline is deploying a service to a new environment (for which the flux manifests don\u2019t exist or require updating). A Delivery Project is created following the process described in the Onboarding a delivery project section . The portal will make an API POST request to the backend API to create the Team specific AD Groups to be used for RBAC. The Backend API will populate the details of the teams in the Teams config repository. On creation of the project, the teams config will be updated (team specific AD groups). The Backend API will then sync the group changes and create the AD Groups in Microsoft Entra ID. This occurs on creation and update on the delivery project information in the ADP Portal. Once a Delivery Project has been created, the development teams can then scafford one or more services following the documentation in the guide How to create a service guide When a user submits the service template form that the engineer has completed, the portal will make API requests to the backend service. The Backend service will start by writing the information to the Teams config. The API will then create/update the Flux manifests for the Sandpit environments in the Flux Services repository. For upstream environments (dev, tst, pre and prd), the CI pipeline will add/update the flux manifests on demand during the deployment of the services to that environment. Flux will reconcile and deploy the application.","title":"Generating the Flux Manifests"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/flux-manifests/#process-for-making-changes-to-the-flux-templates","text":"The flux generation process is split into two environments, Sandpit and Production. The sandpit environment will be used the core ADP Platform Team to test new enhancements and issues pertaining to scaffolder whilst the Production environment will be used for all environments used by the Development Teams to development, testing and deploying live services. Each Flux Environment comprises of the following repositories. \u2022 adp-teams-config: This GitHub repository contains the configuration used to define and generate the flux manifests. The configuration is defined at tenant, environment, and service level. This GitHub repository\u2019s visibility has been set to internal. \u2022 adp-flux-core: This GitHub repository contains the flux manifests for deploying the cluster and platform level (core) resources e.g., Nginx Ingress controller, Azure Service Operator, Cert Manager, and other services. This GitHub repository\u2019s visibility has been set to public. \u2022 adp-flux-services: This GitHub repository contains the flux manifests for deploy service level resources. Each deployed microservice that is running in the cluster will have a set of flux manifests that define the configuration Flux requires to deploy the service. No manual process is required for adding the manifests. This has been automated using an API. \u2022 ADP Portal: Users of the Portal can use a set of defined Software Backend and Frontend Templates to scaffold a new service. This leverages an API that scaffolds updates the configuration in the repository adp-teams-config and then scaffolds the flux manifests in the repository adp-flux-services. Github Organisations used for Flux Manifests Scaffolding Sandpit - the defra adp sandpit org is used by the platform team. Production - main DEFRA Github Org Each of these will contain the following repositories adp-teams-config adp-flux-core adp-flux-services","title":"Process for making changes to the Flux Templates"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/overview/","text":"GitOps for AKS \u00b6 GitOps Principles \u00b6 GitOps is a set of principles for operating and managing a software system. Below are the principles of GitOps According to GitOps principles , the desired state of a GitOps-managed system must be: Declarative : A system that GitOps manages must have its desired state expressed declaratively. The declaration is typically stored in a Git repository. Versioned and immutable : The desired state is stored in a way that enforces immutability and versioning, and retains a complete version history. Pulled automatically : Software agents automatically pull the desired state declarations from the source. Continuously reconciled : Software agents continuously observe actual system state and attempt to apply the desired state. Flux v2 \u00b6 You can use either Flux and Argo CD as GitOps operators for AKS. Both are Cloud Native Computing Foundation (CNCF) projects that are widely used. Refer to the Microsoft documentation GitOps for Azure Kubernetes Service . Flux V2 is the GitOps operator that will be implemented for the ADP Platform. Draft - Further discussion required on step 4 The CI pipeline uses either ADO Yaml Pipelines or Jenkins for build. For deployment, it uses Flux as the GitOps operator to pull and sync the app. The data flows through the scenario as follows: The app code is developed by using an IDE such as Visual Studio Code. A Pull request is raised to merge the changes into the main branch. CI pipeline will deploy the changes to DEV and run automation tests as part of the PR review process. PR approvers review changes The app code is committed to a GitHub repository if CI Build is successful, the automation tests have passed and PR has been approved. Updating the versions of the application in the Helm chart can be done automatically by the CI or manually by the developer before raising the PR. The CI pipeline builds a container image from the app code and pushes the container image to the DEV environment Azure Container Registry. The Flux operator detects configuration drift in the Git repository and pulls the configuration changes. An image policy is used by Flux to scan the ACR repository for new images. Flux uses manifest files to deploy the app to the AKS cluster. Promoting Changes across Environments \u00b6 Refer to the Wiki page Application Deployments Core/Shared Services \u00b6 Flux V2 will be used to bootstrap the baseline configuration of each cluster. The baseline configuration will comprise of the core services e.g. Nginx Plus Azure AD workload identity","title":"Overview"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/overview/#gitops-for-aks","text":"","title":"GitOps for AKS"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/overview/#gitops-principles","text":"GitOps is a set of principles for operating and managing a software system. Below are the principles of GitOps According to GitOps principles , the desired state of a GitOps-managed system must be: Declarative : A system that GitOps manages must have its desired state expressed declaratively. The declaration is typically stored in a Git repository. Versioned and immutable : The desired state is stored in a way that enforces immutability and versioning, and retains a complete version history. Pulled automatically : Software agents automatically pull the desired state declarations from the source. Continuously reconciled : Software agents continuously observe actual system state and attempt to apply the desired state.","title":"GitOps Principles"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/overview/#flux-v2","text":"You can use either Flux and Argo CD as GitOps operators for AKS. Both are Cloud Native Computing Foundation (CNCF) projects that are widely used. Refer to the Microsoft documentation GitOps for Azure Kubernetes Service . Flux V2 is the GitOps operator that will be implemented for the ADP Platform. Draft - Further discussion required on step 4 The CI pipeline uses either ADO Yaml Pipelines or Jenkins for build. For deployment, it uses Flux as the GitOps operator to pull and sync the app. The data flows through the scenario as follows: The app code is developed by using an IDE such as Visual Studio Code. A Pull request is raised to merge the changes into the main branch. CI pipeline will deploy the changes to DEV and run automation tests as part of the PR review process. PR approvers review changes The app code is committed to a GitHub repository if CI Build is successful, the automation tests have passed and PR has been approved. Updating the versions of the application in the Helm chart can be done automatically by the CI or manually by the developer before raising the PR. The CI pipeline builds a container image from the app code and pushes the container image to the DEV environment Azure Container Registry. The Flux operator detects configuration drift in the Git repository and pulls the configuration changes. An image policy is used by Flux to scan the ACR repository for new images. Flux uses manifest files to deploy the app to the AKS cluster.","title":"Flux v2"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/overview/#promoting-changes-across-environments","text":"Refer to the Wiki page Application Deployments","title":"Promoting Changes across Environments"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/overview/#coreshared-services","text":"Flux V2 will be used to bootstrap the baseline configuration of each cluster. The baseline configuration will comprise of the core services e.g. Nginx Plus Azure AD workload identity","title":"Core/Shared Services"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/repository-setup/","text":"Repository Setup \u00b6 The section below describes how the repositories have been set to handle multiple environments and teams with Flux V2. There are different ways of structuring repositories as described in the Flux documentation. Key considerations \u00b6 Teams will deliver new features and bug fixes using trunk-based development A new application repository will be created for each service/application The repository structure should support multi-tenancy Repositories \u00b6 Two repositories were created to ensure the separation of core infrastructure and application deployments and manifests. Repository Purpose adp-flux-core This will be the main repository used for configuring GitOps.`` It will contain the manifests to deploy the core services to the AKS Cluster e.g. for secrets management adp-flux-services This is a secondary repository that will be referenced by GitRepositories from the main repository adp-flux-core adp-flux-core structure \u00b6 The adp-flux-core contains the following top directories clusters directory contains the Flux configuration per cluster. infra directory contains the environment subfolders each containing base and overlays folders. Overlays are used to minimise duplication. Kustomizations are used to deploy the core services, such as Nginx Plus . The base folder for each environment will contain a list of the core services defined in the core folder that should be deployed for a specific environment. Folders 01/02 represent cluster instances within an environment. These overlay folders 01/02 contain environment specific Kustomize patches that contain the environment specific settings. Core folder contains the Flux configurations ( HelmRepository and HelmRelease ) CRD manifests used for installing the core services Services folder contains a GitRepository and Kustomization per environment that points to a path in the adp-flux-services repository. For example, the Kustomization for snd/01 will to point to the path services/environments/snd/01 in the adp-flux-services repository. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 . \u251c\u2500\u2500 clusters # Flux configuration (cluster level) / \u2502 \u251c\u2500\u2500 snd/ \u2502 \u2502 \u2514\u2500\u2500 01/02/03/04 \u2502 \u251c\u2500\u2500 dev/ \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u251c\u2500\u2500 tst/ \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u251c\u2500\u2500 pre/ \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u2514\u2500\u2500 prod/ \u2502 \u2514\u2500\u2500 01/02 \u251c\u2500\u2500 core # Core services installed in the Cluster/ \u2502 \u251c\u2500\u2500 azure-service-operator \u2502 \u251c\u2500\u2500 calico \u2502 \u251c\u2500\u2500 cert-manager \u2502 \u251c\u2500\u2500 grafana \u2502 \u251c\u2500\u2500 nginx-ingress \u2502 \u2514\u2500\u2500 /reloader \u251c\u2500\u2500 infra # Flux configuration for the core services e.g. Nginx Plus/ \u2502 \u251c\u2500\u2500 snd # Contains infra configuration for SND core services / \u2502 \u2502 \u251c\u2500\u2500 base # References 1 or more core services to be deployed \u2502 \u2502 \u2514\u2500\u2500 01/02 # Overlay containing the patches, references base \u2502 \u251c\u2500\u2500 dev/ \u2502 \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u251c\u2500\u2500 pre/ \u2502 \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u2514\u2500\u2500 prod/ \u2502 \u251c\u2500\u2500 base \u2502 \u2514\u2500\u2500 01/02 \u251c\u2500\u2500 core # Contains the manifests for the core service (HelmRelease, HelmRepository)/ \u2502 \u251c\u2500\u2500 nginxplus / \u2502 \u2502 \u251c\u2500\u2500 nginx-ingress-chart.yaml \u2502 \u2502 \u2514\u2500\u2500 nginx-ingress-release.yaml \u2502 \u2514\u2500\u2500 certmanager \u2514\u2500\u2500 services # Flux configuration for business applications/ \u251c\u2500\u2500 gitrepository.yaml # GitRepository resource for adp-flux-services \u251c\u2500\u2500 snd # environment/ \u2502 \u2514\u2500\u2500 01 # environment instance number / \u2502 \u251c\u2500\u2500 Kustomization.yaml # References GitRepository, services.yaml \u2502 \u2514\u2500\u2500 services.yaml # This is a Kustomization file that references gitrepository.yaml and specifies the path \u251c\u2500\u2500 dev/ \u2502 \u2514\u2500\u2500 01 / \u2502 \u251c\u2500\u2500 Kustomization.yaml \u2502 \u2514\u2500\u2500 services.yaml \u251c\u2500\u2500 pre/ \u2502 \u2514\u2500\u2500 01 / \u2502 \u251c\u2500\u2500 Kustomization.yaml \u2502 \u2514\u2500\u2500 services.yaml \u2514\u2500\u2500 prod/ \u2514\u2500\u2500 01 / \u251c\u2500\u2500 Kustomization.yaml \u2514\u2500\u2500 services.yaml You can use the markdown generator tool to update the above folder structure adp-flux-services structure \u00b6 The adp-flux-core contains the following top directories services directory contains the Flux configurations used for deploying the business applications. Tenants \u00b6 Tenants refers to the application teams that are responsible for the development of one or more business applications. A unique namespace will be created for each tenant All applications owned by a specific tenant on the ADP platform will be deployed to the same tenant namespace Below is a description of the subfolders inside the services folder. Subfolder Purpose base the base folder contains manifests that are common to each tenant e.g. namespace, ResourceQuota and ServiceAccount. These manifests are generic, in that they have variables that can be specified at the time of onboarding. environments This contains the environment subfolders each containing base and overlays folders. Overlays are used to minimise duplication. Kustomizations are used to deploy the business services tenants These are the application teams 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 . \u2514\u2500\u2500 services/ \u251c\u2500\u2500 base/ \u2502 \u2514\u2500\u2500 namespace.yaml \u251c\u2500\u2500 environments/ \u2502 \u251c\u2500\u2500 snd/ \u2502 \u2502 \u2514\u2500\u2500 01/ \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml # references business services \u2502 \u251c\u2500\u2500 dev/ \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u251c\u2500\u2500 pre/ \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u2514\u2500\u2500 prod/ \u2502 \u2514\u2500\u2500 01/02 \u2514\u2500\u2500 tenant-a # Team level configuration e.g. namespace, notification/ \u251c\u2500\u2500 kustomization.yaml # References common manifests defined in apps/base e.g. namespace.yaml \u251c\u2500\u2500 tenant-a-service1/ \u2502 \u251c\u2500\u2500 helmrelease.yaml # tenant a owns 1 or more services \u2502 \u251c\u2500\u2500 helmrepository.yaml \u2502 \u251c\u2500\u2500 snd.yaml # environment specific patch for snd environment \u2502 \u251c\u2500\u2500 dev.yaml # environment specific patch for dev environment \u2502 \u251c\u2500\u2500 pre.yaml # environment specific patch for pre environment \u2502 \u2514\u2500\u2500 prod.yaml # environment specific patch for prod environment \u2514\u2500\u2500 tenant-b-service1/ \u251c\u2500\u2500 helmrelease.yaml \u251c\u2500\u2500 helmrepository.yaml \u251c\u2500\u2500 dev.yaml \u251c\u2500\u2500 snd.yaml \u251c\u2500\u2500 pre.yaml \u2514\u2500\u2500 prod.yaml You can use the markdown generator tool to update the above folder structure","title":"Repository Setup"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/repository-setup/#repository-setup","text":"The section below describes how the repositories have been set to handle multiple environments and teams with Flux V2. There are different ways of structuring repositories as described in the Flux documentation.","title":"Repository Setup"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/repository-setup/#key-considerations","text":"Teams will deliver new features and bug fixes using trunk-based development A new application repository will be created for each service/application The repository structure should support multi-tenancy","title":"Key considerations"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/repository-setup/#repositories","text":"Two repositories were created to ensure the separation of core infrastructure and application deployments and manifests. Repository Purpose adp-flux-core This will be the main repository used for configuring GitOps.`` It will contain the manifests to deploy the core services to the AKS Cluster e.g. for secrets management adp-flux-services This is a secondary repository that will be referenced by GitRepositories from the main repository adp-flux-core","title":"Repositories"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/repository-setup/#adp-flux-core-structure","text":"The adp-flux-core contains the following top directories clusters directory contains the Flux configuration per cluster. infra directory contains the environment subfolders each containing base and overlays folders. Overlays are used to minimise duplication. Kustomizations are used to deploy the core services, such as Nginx Plus . The base folder for each environment will contain a list of the core services defined in the core folder that should be deployed for a specific environment. Folders 01/02 represent cluster instances within an environment. These overlay folders 01/02 contain environment specific Kustomize patches that contain the environment specific settings. Core folder contains the Flux configurations ( HelmRepository and HelmRelease ) CRD manifests used for installing the core services Services folder contains a GitRepository and Kustomization per environment that points to a path in the adp-flux-services repository. For example, the Kustomization for snd/01 will to point to the path services/environments/snd/01 in the adp-flux-services repository. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 . \u251c\u2500\u2500 clusters # Flux configuration (cluster level) / \u2502 \u251c\u2500\u2500 snd/ \u2502 \u2502 \u2514\u2500\u2500 01/02/03/04 \u2502 \u251c\u2500\u2500 dev/ \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u251c\u2500\u2500 tst/ \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u251c\u2500\u2500 pre/ \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u2514\u2500\u2500 prod/ \u2502 \u2514\u2500\u2500 01/02 \u251c\u2500\u2500 core # Core services installed in the Cluster/ \u2502 \u251c\u2500\u2500 azure-service-operator \u2502 \u251c\u2500\u2500 calico \u2502 \u251c\u2500\u2500 cert-manager \u2502 \u251c\u2500\u2500 grafana \u2502 \u251c\u2500\u2500 nginx-ingress \u2502 \u2514\u2500\u2500 /reloader \u251c\u2500\u2500 infra # Flux configuration for the core services e.g. Nginx Plus/ \u2502 \u251c\u2500\u2500 snd # Contains infra configuration for SND core services / \u2502 \u2502 \u251c\u2500\u2500 base # References 1 or more core services to be deployed \u2502 \u2502 \u2514\u2500\u2500 01/02 # Overlay containing the patches, references base \u2502 \u251c\u2500\u2500 dev/ \u2502 \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u251c\u2500\u2500 pre/ \u2502 \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u2514\u2500\u2500 prod/ \u2502 \u251c\u2500\u2500 base \u2502 \u2514\u2500\u2500 01/02 \u251c\u2500\u2500 core # Contains the manifests for the core service (HelmRelease, HelmRepository)/ \u2502 \u251c\u2500\u2500 nginxplus / \u2502 \u2502 \u251c\u2500\u2500 nginx-ingress-chart.yaml \u2502 \u2502 \u2514\u2500\u2500 nginx-ingress-release.yaml \u2502 \u2514\u2500\u2500 certmanager \u2514\u2500\u2500 services # Flux configuration for business applications/ \u251c\u2500\u2500 gitrepository.yaml # GitRepository resource for adp-flux-services \u251c\u2500\u2500 snd # environment/ \u2502 \u2514\u2500\u2500 01 # environment instance number / \u2502 \u251c\u2500\u2500 Kustomization.yaml # References GitRepository, services.yaml \u2502 \u2514\u2500\u2500 services.yaml # This is a Kustomization file that references gitrepository.yaml and specifies the path \u251c\u2500\u2500 dev/ \u2502 \u2514\u2500\u2500 01 / \u2502 \u251c\u2500\u2500 Kustomization.yaml \u2502 \u2514\u2500\u2500 services.yaml \u251c\u2500\u2500 pre/ \u2502 \u2514\u2500\u2500 01 / \u2502 \u251c\u2500\u2500 Kustomization.yaml \u2502 \u2514\u2500\u2500 services.yaml \u2514\u2500\u2500 prod/ \u2514\u2500\u2500 01 / \u251c\u2500\u2500 Kustomization.yaml \u2514\u2500\u2500 services.yaml You can use the markdown generator tool to update the above folder structure","title":"adp-flux-core structure"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/repository-setup/#adp-flux-services-structure","text":"The adp-flux-core contains the following top directories services directory contains the Flux configurations used for deploying the business applications.","title":"adp-flux-services structure"},{"location":"Platform-Architecture/architectural-components/ci-cd-and-automation/gitops-for-aks/repository-setup/#tenants","text":"Tenants refers to the application teams that are responsible for the development of one or more business applications. A unique namespace will be created for each tenant All applications owned by a specific tenant on the ADP platform will be deployed to the same tenant namespace Below is a description of the subfolders inside the services folder. Subfolder Purpose base the base folder contains manifests that are common to each tenant e.g. namespace, ResourceQuota and ServiceAccount. These manifests are generic, in that they have variables that can be specified at the time of onboarding. environments This contains the environment subfolders each containing base and overlays folders. Overlays are used to minimise duplication. Kustomizations are used to deploy the business services tenants These are the application teams 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 . \u2514\u2500\u2500 services/ \u251c\u2500\u2500 base/ \u2502 \u2514\u2500\u2500 namespace.yaml \u251c\u2500\u2500 environments/ \u2502 \u251c\u2500\u2500 snd/ \u2502 \u2502 \u2514\u2500\u2500 01/ \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml # references business services \u2502 \u251c\u2500\u2500 dev/ \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u251c\u2500\u2500 pre/ \u2502 \u2502 \u2514\u2500\u2500 01/02 \u2502 \u2514\u2500\u2500 prod/ \u2502 \u2514\u2500\u2500 01/02 \u2514\u2500\u2500 tenant-a # Team level configuration e.g. namespace, notification/ \u251c\u2500\u2500 kustomization.yaml # References common manifests defined in apps/base e.g. namespace.yaml \u251c\u2500\u2500 tenant-a-service1/ \u2502 \u251c\u2500\u2500 helmrelease.yaml # tenant a owns 1 or more services \u2502 \u251c\u2500\u2500 helmrepository.yaml \u2502 \u251c\u2500\u2500 snd.yaml # environment specific patch for snd environment \u2502 \u251c\u2500\u2500 dev.yaml # environment specific patch for dev environment \u2502 \u251c\u2500\u2500 pre.yaml # environment specific patch for pre environment \u2502 \u2514\u2500\u2500 prod.yaml # environment specific patch for prod environment \u2514\u2500\u2500 tenant-b-service1/ \u251c\u2500\u2500 helmrelease.yaml \u251c\u2500\u2500 helmrepository.yaml \u251c\u2500\u2500 dev.yaml \u251c\u2500\u2500 snd.yaml \u251c\u2500\u2500 pre.yaml \u2514\u2500\u2500 prod.yaml You can use the markdown generator tool to update the above folder structure","title":"Tenants"},{"location":"Platform-Architecture/architectural-components/monitoring/alerts/","text":"Alerts in ADP \u00b6 Azure Monitor for containers now includes recommended alerts . These preconfigured metrics alerts enable monitoring the system resources when they are running on peak capacity or hitting failure rates. Metric Alert Rules \u00b6 Metric alerts in Azure Monitor proactively identify issues related to system resources of your Azure resources, including monitored Kubernetes clusters. Container insights provides preconfigured alert rules so that we will use those as starting point... Container insights in Azure Monitor now supports alerts based on Prometheus metrics , and metric rules will be retired on March 14, 2026 . If you already use alerts based on custom metrics, you should migrate to Prometheus alerts and disable the equivalent custom metric alerts. As of August 15, 2023, you will no longer be able to configure new custom metric recommended alerts using the portal . Metric alert rules in Container insights (preview) Prometheus rules \u00b6 Prometheus alert rules use metric data from your Kubernetes cluster sent to Azure Monitor managed service for Prometheus. Enable Prometheus Alert Rules by deploying the community and recommended alerts using the Bicep template . Follow the README.md file in the same folder for how to deploy. https://github.com/Azure/prometheus-collector/blob/main/AddonBicepTemplate/AzureMonitorAlertsProfile.bicep Configure alertable metrics in ConfigMaps \u00b6 The tutorial below specifies how you can configure the alertable metrics in ConfigMaps . Additional Documentation \u00b6 Metric alert rules in Container insights (preview) Flux Alerts \u00b6 Flux Alerts are configured to notify teams about the status of their GitOps pipelines. The Flux controllers emit Kubernetes events whenever a resource status changes. You can use the notification-controller to forward these events to Slack, Microsoft Teams, Discord and others. The notification controller is part of the default Flux installation The following alerts will be configured for the following scenarios: Reconciliation failures in the cluster A new version of an app was deployed and if the deployment is healthy Slack Integration \u00b6","title":"Alerts"},{"location":"Platform-Architecture/architectural-components/monitoring/alerts/#alerts-in-adp","text":"Azure Monitor for containers now includes recommended alerts . These preconfigured metrics alerts enable monitoring the system resources when they are running on peak capacity or hitting failure rates.","title":"Alerts in ADP"},{"location":"Platform-Architecture/architectural-components/monitoring/alerts/#metric-alert-rules","text":"Metric alerts in Azure Monitor proactively identify issues related to system resources of your Azure resources, including monitored Kubernetes clusters. Container insights provides preconfigured alert rules so that we will use those as starting point... Container insights in Azure Monitor now supports alerts based on Prometheus metrics , and metric rules will be retired on March 14, 2026 . If you already use alerts based on custom metrics, you should migrate to Prometheus alerts and disable the equivalent custom metric alerts. As of August 15, 2023, you will no longer be able to configure new custom metric recommended alerts using the portal . Metric alert rules in Container insights (preview)","title":"Metric Alert Rules"},{"location":"Platform-Architecture/architectural-components/monitoring/alerts/#prometheus-rules","text":"Prometheus alert rules use metric data from your Kubernetes cluster sent to Azure Monitor managed service for Prometheus. Enable Prometheus Alert Rules by deploying the community and recommended alerts using the Bicep template . Follow the README.md file in the same folder for how to deploy. https://github.com/Azure/prometheus-collector/blob/main/AddonBicepTemplate/AzureMonitorAlertsProfile.bicep","title":"Prometheus rules"},{"location":"Platform-Architecture/architectural-components/monitoring/alerts/#configure-alertable-metrics-in-configmaps","text":"The tutorial below specifies how you can configure the alertable metrics in ConfigMaps .","title":"Configure alertable metrics in ConfigMaps"},{"location":"Platform-Architecture/architectural-components/monitoring/alerts/#additional-documentation","text":"Metric alert rules in Container insights (preview)","title":"Additional Documentation"},{"location":"Platform-Architecture/architectural-components/monitoring/alerts/#flux-alerts","text":"Flux Alerts are configured to notify teams about the status of their GitOps pipelines. The Flux controllers emit Kubernetes events whenever a resource status changes. You can use the notification-controller to forward these events to Slack, Microsoft Teams, Discord and others. The notification controller is part of the default Flux installation The following alerts will be configured for the following scenarios: Reconciliation failures in the cluster A new version of an app was deployed and if the deployment is healthy","title":"Flux Alerts"},{"location":"Platform-Architecture/architectural-components/monitoring/alerts/#slack-integration","text":"","title":"Slack Integration"},{"location":"Platform-Architecture/architectural-components/monitoring/automated-monitoring-implementation/","text":"Automated Monitoring Implementation - Enable Prometheus Logs \u00b6 This section details how the AKS Prometheus logs were enabled via Automation. The following documents were referenced: https://learn.microsoft.com/en-us/azure/azure-monitor/containers/prometheus-metrics-enable?tabs=bicep https://github.com/slavizh/BicepTemplates/blob/main/monitor-prometheus/aks-resources.bicep These are the steps that were carried out: The 'Monitoring Data Reader' role was given to the Grafana system assigned identity on the Azure Monitor Workspace, so Grafana can query metrics. Bicep Template A Data Collection Rule Association was created between the AKS Cluster and the Azure Monitor Workspace. Bicep Template The default metrics prometheusRuleGroups provided by Microsoft were added to the automation in order to populate the Dashboards in Grafana. Bicep Template The azureMonitorProfile metrics were enabled in the AKS Bicep Module Bicep Template Prometheus Log Retention Managed Prometheus includes 18 months of data retention. This is included as part of the service and there is no additional charge for storage and retention. https://azure.microsoft.com/en-gb/updates/general-availability-azure-monitor-managed-service-for-prometheus/ (Opens in new window or tab) https://techcommunity.microsoft.com/t5/azure-observability-blog/introducing-azure-monitor-managed-service-for-prometheus/ba-p/3600185 (Opens in new window or tab) Managed Prometheus Dashboard example: Automate creation of Flux Dashboards \u00b6 This section details how the Flux Dashboard creation and population was automated. The following document was referenced: https://learn.microsoft.com/en-us/azure/azure-arc/kubernetes/monitor-gitops-flux-2 These are the steps that were carried out: The 'Grafana Admin' permission was granted to the ADO SSV3 (ADO-DefraGovUK-AAD-ADP-SSV3) service principal on the Azure Managed Grafana instance. This is required to allow the pipeline to create the Dashboards in Grafana A PowerShell script was created to check if the 'Flux' folder and the new dashboards exist. If they don't exist the script will create them. PowerShell Script The Dashboard json templates were taken from: GitOps Flux - Application Deployments Dashboard Flux Control Plane Flux Cluster Stats The 'Reader' permission was granted to the Grafana system assigned identity on the environment subscription. e.g. AZD-ADP-SND1 Configure Azure Monitor Agent to scrape the Azure Managed Flux metrics by creating a configmap. This change was made in the adp-flux-core repository. SND1 Grafana Instance Flux Dashboard Example:","title":"Automated Monitoring Implementation"},{"location":"Platform-Architecture/architectural-components/monitoring/automated-monitoring-implementation/#automated-monitoring-implementation-enable-prometheus-logs","text":"This section details how the AKS Prometheus logs were enabled via Automation. The following documents were referenced: https://learn.microsoft.com/en-us/azure/azure-monitor/containers/prometheus-metrics-enable?tabs=bicep https://github.com/slavizh/BicepTemplates/blob/main/monitor-prometheus/aks-resources.bicep These are the steps that were carried out: The 'Monitoring Data Reader' role was given to the Grafana system assigned identity on the Azure Monitor Workspace, so Grafana can query metrics. Bicep Template A Data Collection Rule Association was created between the AKS Cluster and the Azure Monitor Workspace. Bicep Template The default metrics prometheusRuleGroups provided by Microsoft were added to the automation in order to populate the Dashboards in Grafana. Bicep Template The azureMonitorProfile metrics were enabled in the AKS Bicep Module Bicep Template Prometheus Log Retention Managed Prometheus includes 18 months of data retention. This is included as part of the service and there is no additional charge for storage and retention. https://azure.microsoft.com/en-gb/updates/general-availability-azure-monitor-managed-service-for-prometheus/ (Opens in new window or tab) https://techcommunity.microsoft.com/t5/azure-observability-blog/introducing-azure-monitor-managed-service-for-prometheus/ba-p/3600185 (Opens in new window or tab) Managed Prometheus Dashboard example:","title":"Automated Monitoring Implementation - Enable Prometheus Logs"},{"location":"Platform-Architecture/architectural-components/monitoring/automated-monitoring-implementation/#automate-creation-of-flux-dashboards","text":"This section details how the Flux Dashboard creation and population was automated. The following document was referenced: https://learn.microsoft.com/en-us/azure/azure-arc/kubernetes/monitor-gitops-flux-2 These are the steps that were carried out: The 'Grafana Admin' permission was granted to the ADO SSV3 (ADO-DefraGovUK-AAD-ADP-SSV3) service principal on the Azure Managed Grafana instance. This is required to allow the pipeline to create the Dashboards in Grafana A PowerShell script was created to check if the 'Flux' folder and the new dashboards exist. If they don't exist the script will create them. PowerShell Script The Dashboard json templates were taken from: GitOps Flux - Application Deployments Dashboard Flux Control Plane Flux Cluster Stats The 'Reader' permission was granted to the Grafana system assigned identity on the environment subscription. e.g. AZD-ADP-SND1 Configure Azure Monitor Agent to scrape the Azure Managed Flux metrics by creating a configmap. This change was made in the adp-flux-core repository. SND1 Grafana Instance Flux Dashboard Example:","title":"Automate creation of Flux Dashboards"},{"location":"Platform-Architecture/architectural-components/monitoring/network-watcher/","text":"Network Watcher \u00b6 Azure Network Watcher provides a suite of tools to monitor, diagnose, view metrics, and enable or disable logs for the ADP Platform, specifically, the AKS Clusters. Network Watcher is enabled automatically in a virtual network's region when we create or update the virtual network in a subscription. The Network Watcher resource in each ADP Platform subscription and region is created in the NetworkWatcherRG resource group. Below is a screenshot for Network Watcher instances in the Sandpit environments. NSG Flow Logs \u00b6 Flow Logs are enabled for the NSGs associated with AKS Cluster subnets. Flow Logs are vital to monitor, manage, and know the ADP Platform virtual networks (one per environment) so that they can protected and optimised. They enable tracking and being able to monitor the following: - Current state of the network - Who is connecting, and where users are connecting from. - Which ports are open to the internet - What network behavior is expected, what network behavior is irregular, and when sudden rises in traffic happen. Network Security Group (NSG) Flow Logs Retention Period is set to 30 days. Retention is available only if you use general-purpose v2 storage accounts. An ADP Platform storage account has been created in each ADP subscription for the flow logs . VNET Flow Logs \u00b6 Network Watcher VNet flow logs capability overcomes some of the existing limitations of NSG flow logs . e.g. VNet flow logs avoid the need to enable multi-level flow logging such as in cases of NSG flow logs where network security groups are configured at both subnet & NIC. VNet flow logs is currently in PREVIEW. So will not be implemented until it is GA. The preview version is not the available in UK South and UK West regions.","title":"Network Watcher"},{"location":"Platform-Architecture/architectural-components/monitoring/network-watcher/#network-watcher","text":"Azure Network Watcher provides a suite of tools to monitor, diagnose, view metrics, and enable or disable logs for the ADP Platform, specifically, the AKS Clusters. Network Watcher is enabled automatically in a virtual network's region when we create or update the virtual network in a subscription. The Network Watcher resource in each ADP Platform subscription and region is created in the NetworkWatcherRG resource group. Below is a screenshot for Network Watcher instances in the Sandpit environments.","title":"Network Watcher"},{"location":"Platform-Architecture/architectural-components/monitoring/network-watcher/#nsg-flow-logs","text":"Flow Logs are enabled for the NSGs associated with AKS Cluster subnets. Flow Logs are vital to monitor, manage, and know the ADP Platform virtual networks (one per environment) so that they can protected and optimised. They enable tracking and being able to monitor the following: - Current state of the network - Who is connecting, and where users are connecting from. - Which ports are open to the internet - What network behavior is expected, what network behavior is irregular, and when sudden rises in traffic happen. Network Security Group (NSG) Flow Logs Retention Period is set to 30 days. Retention is available only if you use general-purpose v2 storage accounts. An ADP Platform storage account has been created in each ADP subscription for the flow logs .","title":"NSG Flow Logs"},{"location":"Platform-Architecture/architectural-components/monitoring/network-watcher/#vnet-flow-logs","text":"Network Watcher VNet flow logs capability overcomes some of the existing limitations of NSG flow logs . e.g. VNet flow logs avoid the need to enable multi-level flow logging such as in cases of NSG flow logs where network security groups are configured at both subnet & NIC. VNet flow logs is currently in PREVIEW. So will not be implemented until it is GA. The preview version is not the available in UK South and UK West regions.","title":"VNET Flow Logs"},{"location":"Platform-Architecture/architectural-components/monitoring/overview/","text":"Monitoring \u00b6 Azure Monitor will be used to monitor the health and performance of the Kubernetes clusters and the workloads running on them. The AKS Cluster generates metrics ( Platform and Prometheus Metrics ) and logs ( Activity and Resource Logs ), refer to Monitoring AKS data reference for detailed information. Custom metrics will be enabled automatically since the AKS cluster uses managed identity authentication . Source: https://learn.microsoft.com/en-us/azure/aks/monitor-aks The diagram below shows the different levels of monitoring. Source : https://learn.microsoft.com/en-us/azure/azure-monitor/containers/monitor-kubernetes Container Insights \u00b6 Azure Monitor now offers a unified cloud native offering for Kubernetes monitoring - Azure Monitoring Managed Service for Prometheus - Azure Monitor Container Insights - Azure Managed Grafana Container Insights stores its data in a Log Analytics workspace . Therefore an ADP Platform Log Analytics Workspace has been created to store the AKS metrics and logs. Enable Container insights for Azure Kubernetes Service (AKS) cluster After enabling Container Insights, you will be able to view the AKS Cluster in the list of monitored clusters. There are many reports available for Node Monitoring, Resource Monitoring, Billing, Networking and Security. The diagram below shows the insights on the Nodes. The other tabs when clicked would show insights for the Cluster, Controllers or Containers. Azure Managed Grafana \u00b6 There are many benefits to using the managed services, such as, automatic authentication and authorisation setup based on Azure AD identities and pre-built roles (Grafana Admin, Grafana Editor and Grafana Viewer). The managed Grafana service also comes with the capability to integrate with various Azure data sources through an Azure managed identity and RBAC permissions on your subscriptions. It also comes with default Grafana Dashboards as a base. The managed Grafana services has been installed as a shared resource in the SSV3 and SSV5 subscriptions, which are in the O365_DefraDEV and DEFRA Tenants respectively. SSV3 is used for the sandpit environments whilst SSV5 will be used for all environments in the DEFRA Tenant. These are DEV, DEMO, PRE and PROD. Implementation of the Managed Grafana Instance \u00b6 Create an Azure Managed Grafana instance as a Shared resource (in SSV3 or SSV5) a. Create and configure an environment specific Azure Monitor workspace b. Link the Azure Monitor workspace to the Grafana instance Enable Prometheus metrics collection by adding the AKS cluster to the monitored clusters a. Define alerts Create Prometheus rule groups: Bicep template Azure Monitoring Managed Service for Prometheus \u00b6 Azure Monitor managed service for Prometheus is a fully managed Prometheus-compatible service that supports industry standard features such as PromQL, Grafana dashboards, and Prometheus alerts. Azure Monitor managed service for Prometheus overview diagram This service requires configuring the metrics addon for the Azure Monitor agent, which sends data to Prometheus. Azure Monitor managed service for Prometheus GA 23 May 2023. General Availability: Azure Monitor managed service for Prometheus","title":"Overview"},{"location":"Platform-Architecture/architectural-components/monitoring/overview/#monitoring","text":"Azure Monitor will be used to monitor the health and performance of the Kubernetes clusters and the workloads running on them. The AKS Cluster generates metrics ( Platform and Prometheus Metrics ) and logs ( Activity and Resource Logs ), refer to Monitoring AKS data reference for detailed information. Custom metrics will be enabled automatically since the AKS cluster uses managed identity authentication . Source: https://learn.microsoft.com/en-us/azure/aks/monitor-aks The diagram below shows the different levels of monitoring. Source : https://learn.microsoft.com/en-us/azure/azure-monitor/containers/monitor-kubernetes","title":"Monitoring"},{"location":"Platform-Architecture/architectural-components/monitoring/overview/#container-insights","text":"Azure Monitor now offers a unified cloud native offering for Kubernetes monitoring - Azure Monitoring Managed Service for Prometheus - Azure Monitor Container Insights - Azure Managed Grafana Container Insights stores its data in a Log Analytics workspace . Therefore an ADP Platform Log Analytics Workspace has been created to store the AKS metrics and logs. Enable Container insights for Azure Kubernetes Service (AKS) cluster After enabling Container Insights, you will be able to view the AKS Cluster in the list of monitored clusters. There are many reports available for Node Monitoring, Resource Monitoring, Billing, Networking and Security. The diagram below shows the insights on the Nodes. The other tabs when clicked would show insights for the Cluster, Controllers or Containers.","title":"Container Insights"},{"location":"Platform-Architecture/architectural-components/monitoring/overview/#azure-managed-grafana","text":"There are many benefits to using the managed services, such as, automatic authentication and authorisation setup based on Azure AD identities and pre-built roles (Grafana Admin, Grafana Editor and Grafana Viewer). The managed Grafana service also comes with the capability to integrate with various Azure data sources through an Azure managed identity and RBAC permissions on your subscriptions. It also comes with default Grafana Dashboards as a base. The managed Grafana services has been installed as a shared resource in the SSV3 and SSV5 subscriptions, which are in the O365_DefraDEV and DEFRA Tenants respectively. SSV3 is used for the sandpit environments whilst SSV5 will be used for all environments in the DEFRA Tenant. These are DEV, DEMO, PRE and PROD.","title":"Azure Managed Grafana"},{"location":"Platform-Architecture/architectural-components/monitoring/overview/#implementation-of-the-managed-grafana-instance","text":"Create an Azure Managed Grafana instance as a Shared resource (in SSV3 or SSV5) a. Create and configure an environment specific Azure Monitor workspace b. Link the Azure Monitor workspace to the Grafana instance Enable Prometheus metrics collection by adding the AKS cluster to the monitored clusters a. Define alerts Create Prometheus rule groups: Bicep template","title":"Implementation of the Managed Grafana Instance"},{"location":"Platform-Architecture/architectural-components/monitoring/overview/#azure-monitoring-managed-service-for-prometheus","text":"Azure Monitor managed service for Prometheus is a fully managed Prometheus-compatible service that supports industry standard features such as PromQL, Grafana dashboards, and Prometheus alerts. Azure Monitor managed service for Prometheus overview diagram This service requires configuring the metrics addon for the Azure Monitor agent, which sends data to Prometheus. Azure Monitor managed service for Prometheus GA 23 May 2023. General Availability: Azure Monitor managed service for Prometheus","title":"Azure Monitoring Managed Service for Prometheus"},{"location":"Platform-Architecture/integration-patterns/dynamics-and-platform-platform/","text":"Dynamics 365 & Power Platform Integration Patterns \u00b6 A key feature for tenant teams is the integration with Microsoft Dynamics 365 and Power Platform. When integration with these components, Service Principals are used to generate Bearer tokens to authenticate to the instance. This document describes the best practices, networking routes and pros and cons of different solutions. Dynamics 365 \u00b6 Azure Tenancy Alignment \u00b6 The Platform broadly aligns with the Defra Azure tenancies that the Microsoft SaaS products are hosted in. Namely, Defra and DefraDev. This reduces/removes the cross-tenant identity requirements and enables the use of Azure-managed identities with Microsoft Platform-managed secrets. A key requirement was to align identity solutions and ensure optimal automation. Authentication to Dynamics \u00b6 To connect to Azure services, the best practice approach is to use Azure Managed Identities. This can be used to connect to almost all Azure PaaS Services, as well as SaaS products such as Dynamics 365. This uses the industry standard OAuth 2 protocols - https://auth0.com/docs/get-started/authentication-and-authorization-flow/client-credentials-flow ADP Recommend and promote only using Azure Managed Identities to connect to Dynamics 365 with OAuth 2. Why do we recommend this? Automation and security Managed Identities are strongly preferred/required over service principals (app regs) and directly solve a key issue in Defra around Secret and Credential management and renewals. No Client Secret is required and no separate components are deployed. If not using an Azure MI, App Registrations must be created in Entra ID, adding to the automation and integration logic required to support, run and maintain. This would still be using OAuth2. App Reg Secrets expire every 1 year - and require an automated (or manual at worst) process to ensure these are kept up to date. An automated background process must be in place for App Registrations. With an MI, this process is managed by Microsoft securely. Managed identities directly solve this issue entirely within Azure as the Microsoft Platform itself automatically manages and renews the credentials without any user or service impact. The process is transparent and happens 6-monthly. No credentials are required to be stored anywhere. You don't need to store additional secrets and credentials, enhancing security presence. ADP Processes Within ADP, a Workload Identity (Azure MI) is automatically created for each service out of the box on deployment. This same Identity can be used within Dynamics as an ' application user' to integrate with 0365. No additional deployment is required in Azure. This removes the requirement for additional components (App Reg's) to be created and the same identity can be used for other scenarios. A dedicated MI can also be created, when required. No credential management or storage is required with the ADP recommended approach. Development teams can use NodeJS or C# SDKs to develop their applications and use this functionality easily with support from Microsoft / community. Note: The application user (the MI/WI) must be added to Dynamics with the relevant security profile and permissions. Reference articles below detail this fully, including any relevant SDKs. Example reference articles: https://blog.yannickreekmans.be/secretless-applications-use-azure-identity-sdk-to-access-data-with-a-managed-identity/ and https://www.eugenevanstaden.com/blog/d365-managed-identity/ Using an MI to connect to Dataverse https://community.dynamics.com/blogs/post/?postid=09f639ba-5134-4bd1-8812-04e019b7b920 A deeper understanding of App Regs, OAuth and connetivity can be found here - reference articles: https://learn.microsoft.com/en-us/power-apps/developer/data-platform/authenticate-oauth#app-registration https://www.vodovnik.com/2023/01/12/interacting-with-dataverse-data-from-azure-c/ Networking \u00b6 When integrating with SaaS products, there are networking considerations to consider in terms of network security and ingress/egress charges. When working with Azure PaaS and SaaS Services, a number of options may be available to you depending on the pattern. Virtual network integration ** DRAFT ** Microsoft are introducing a number of enhancements to secure your applications running in Azure with SaaS products. One example is https://learn.microsoft.com/en-us/data-integration/vnet/data-gateway-power-platform-dataflows These tools allows you to securely connect Azure Services to products like Power Platform and Dynamics securely - all within your own VNet without any public internet exposure, at lower cost. ADP is building a future pattern around these scenarios and will be fully detailed here shortly. As the ADP Cluster is within an Azure VNET, VNET Integration is required for secure/none public connectivity. Alternatively, whitelisting via the ADP Front Door can be used to secure integrations. A pattern is being developed for this approach. Power Platform \u00b6 Testing and Quality Assurance ** DRAFT ** \u00b6 TBC -","title":"Dynamics & Plower Platform"},{"location":"Platform-Architecture/integration-patterns/dynamics-and-platform-platform/#dynamics-365-power-platform-integration-patterns","text":"A key feature for tenant teams is the integration with Microsoft Dynamics 365 and Power Platform. When integration with these components, Service Principals are used to generate Bearer tokens to authenticate to the instance. This document describes the best practices, networking routes and pros and cons of different solutions.","title":"Dynamics 365 &amp; Power Platform Integration Patterns"},{"location":"Platform-Architecture/integration-patterns/dynamics-and-platform-platform/#dynamics-365","text":"","title":"Dynamics 365"},{"location":"Platform-Architecture/integration-patterns/dynamics-and-platform-platform/#azure-tenancy-alignment","text":"The Platform broadly aligns with the Defra Azure tenancies that the Microsoft SaaS products are hosted in. Namely, Defra and DefraDev. This reduces/removes the cross-tenant identity requirements and enables the use of Azure-managed identities with Microsoft Platform-managed secrets. A key requirement was to align identity solutions and ensure optimal automation.","title":"Azure Tenancy Alignment"},{"location":"Platform-Architecture/integration-patterns/dynamics-and-platform-platform/#authentication-to-dynamics","text":"To connect to Azure services, the best practice approach is to use Azure Managed Identities. This can be used to connect to almost all Azure PaaS Services, as well as SaaS products such as Dynamics 365. This uses the industry standard OAuth 2 protocols - https://auth0.com/docs/get-started/authentication-and-authorization-flow/client-credentials-flow ADP Recommend and promote only using Azure Managed Identities to connect to Dynamics 365 with OAuth 2. Why do we recommend this? Automation and security Managed Identities are strongly preferred/required over service principals (app regs) and directly solve a key issue in Defra around Secret and Credential management and renewals. No Client Secret is required and no separate components are deployed. If not using an Azure MI, App Registrations must be created in Entra ID, adding to the automation and integration logic required to support, run and maintain. This would still be using OAuth2. App Reg Secrets expire every 1 year - and require an automated (or manual at worst) process to ensure these are kept up to date. An automated background process must be in place for App Registrations. With an MI, this process is managed by Microsoft securely. Managed identities directly solve this issue entirely within Azure as the Microsoft Platform itself automatically manages and renews the credentials without any user or service impact. The process is transparent and happens 6-monthly. No credentials are required to be stored anywhere. You don't need to store additional secrets and credentials, enhancing security presence. ADP Processes Within ADP, a Workload Identity (Azure MI) is automatically created for each service out of the box on deployment. This same Identity can be used within Dynamics as an ' application user' to integrate with 0365. No additional deployment is required in Azure. This removes the requirement for additional components (App Reg's) to be created and the same identity can be used for other scenarios. A dedicated MI can also be created, when required. No credential management or storage is required with the ADP recommended approach. Development teams can use NodeJS or C# SDKs to develop their applications and use this functionality easily with support from Microsoft / community. Note: The application user (the MI/WI) must be added to Dynamics with the relevant security profile and permissions. Reference articles below detail this fully, including any relevant SDKs. Example reference articles: https://blog.yannickreekmans.be/secretless-applications-use-azure-identity-sdk-to-access-data-with-a-managed-identity/ and https://www.eugenevanstaden.com/blog/d365-managed-identity/ Using an MI to connect to Dataverse https://community.dynamics.com/blogs/post/?postid=09f639ba-5134-4bd1-8812-04e019b7b920 A deeper understanding of App Regs, OAuth and connetivity can be found here - reference articles: https://learn.microsoft.com/en-us/power-apps/developer/data-platform/authenticate-oauth#app-registration https://www.vodovnik.com/2023/01/12/interacting-with-dataverse-data-from-azure-c/","title":"Authentication to Dynamics"},{"location":"Platform-Architecture/integration-patterns/dynamics-and-platform-platform/#networking","text":"When integrating with SaaS products, there are networking considerations to consider in terms of network security and ingress/egress charges. When working with Azure PaaS and SaaS Services, a number of options may be available to you depending on the pattern. Virtual network integration ** DRAFT ** Microsoft are introducing a number of enhancements to secure your applications running in Azure with SaaS products. One example is https://learn.microsoft.com/en-us/data-integration/vnet/data-gateway-power-platform-dataflows These tools allows you to securely connect Azure Services to products like Power Platform and Dynamics securely - all within your own VNet without any public internet exposure, at lower cost. ADP is building a future pattern around these scenarios and will be fully detailed here shortly. As the ADP Cluster is within an Azure VNET, VNET Integration is required for secure/none public connectivity. Alternatively, whitelisting via the ADP Front Door can be used to secure integrations. A pattern is being developed for this approach.","title":"Networking"},{"location":"Platform-Architecture/integration-patterns/dynamics-and-platform-platform/#power-platform","text":"","title":"Power Platform"},{"location":"Platform-Architecture/integration-patterns/dynamics-and-platform-platform/#testing-and-quality-assurance-draft","text":"TBC -","title":"Testing and Quality Assurance ** DRAFT **"},{"location":"Platform-Architecture/integration-patterns/overview/","text":"The integration patterns that can be utilized in ADP will be defined here. This may be internal and external patterns, including with Azure services, SaaS products, and third-party services. A number of services are in scope and/or have defined patterns, including: Azure Integration Services ( PaaS services, APIs, etc. ) Microsoft Dynamics 365 and Power Platform ( SaaS products ) On-premise / third party ( i.e. RPA, Crown Hosting, etc. ). External Integration Products (i.e.. Dell Boomi, MuleSoft etc._)","title":"Overview"},{"location":"Platform-Architecture/platform-azure-services/ai-services/","text":"TODO This page is a work in progress and will be updated in due course. Azure AI Services \u00b6 This article details the AI Services Architecture for the solution at a high level. AI Services supported by ADP: Azure Open AI Service Azure AI Search Warning Please ensure you fellow DEFRA's guidelines and policies when using AI services. This includes the use of data and the use of AI services in general in order to ensure your delivery project is using AI responsibly. Azure Open AI \u00b6 Azure OpenAI Service provides REST API access to OpenAI's powerful language models including the GPT-4, GPT-4 Turbo with Vision, GPT-3.5-Turbo, and Embeddings model series. In addition, the new GPT-4 and GPT-3.5-Turbo model series have now reached general availability. These models can be easily adapted to your specific task including but not limited to content generation, summarization, image understanding, semantic search, and natural language to code translation. Deployed & Supported Models \u00b6 The Azure Development Platform (ADP) supports a range of models within Azure OpenAI. However, the availability of these models is limited to those supported by Azure in the UK South region . The following table lists the supported models: Model Deployment Quota Description gpt-4 gpt-4 80k An improvement on GPT-3.5, capable of understanding and generating both natural language and code. gpt-35-turbo gpt-35-turbo 350k An improvement on GPT-3, capable of understanding and generating both natural language and code. text-embedding-ada-002 text-embedding-ada-002 350k Converts text into numerical vector form to facilitate text similarity comparisons. text-embedding-3-large text-embedding-3-large 350k The latest and most capable model for converting text into numerical vector form for text similarity comparisons. Please note that upgrading between different embedding models is not supported. Warning All Delivery Projects should be mindful of the quota restrictions for each model per subscription/region. These models are shared resources among all Delivery Projects. If more quota is needed, ADP can make a request. However, please note that any increase in quota is subject to Microsoft\u2019s approval. Architecture \u00b6 Within the ADP Platform, Azure Open AI Services are deployed with an Azure API Management (APIM) to provide a secure and scalable API endpoint for the AI services. The APIM is used to manage the API lifecycle, provide security, and monitor the API usage. The APIM is deployed in a Subnet (/29) and uses a private link to connect to the Azure OpenAI service. Additionally, a developer portal is deployed with the APIM, offering self-service API documentation for the AI services. Between the delivery projects service and the APIM, private link will be implemented and the APIM will use the services' managed identity with the role of Cognitive Services OpenAI User assigned. This will allow the APIM to access the AI services on behalf of the delivery project's service privately and securely. For any other Azure services that require access to the AI services, they will need to utilize the APIM endpoint and the managed identity. Iterative Deployment \u00b6 To meet the timelines and requirements of the delivery projects, our initial step will be to deploy the Azure OpenAI Service. We will provide the AKS cluster and Azure AI Search with direct access to this service over a private endpoint, assigning the role of Cognitive Services OpenAI User . This approach will enable the delivery projects to begin utilizing the AI services and provide valuable feedback. Once the APIM is deployed, we will transition the AI services to APIM. This will establish a secure and scalable API endpoint for the AI services. Note For local development purposes, the delivery projects can directly use the Azure Open AI services in the SND environment only, provided they are connected to the DEFRA VPN or using a DEFRA laptop. This setup will facilitate testing of the AI services and the provision of feedback. Developer Access \u00b6 For local development, developers will be able to access SND only via the DEFRA VPN or DEFRA laptop with the assigned role of Cognitive Services OpenAI User . Giving the developers the ability to test Azure Open AI services locally via APIM and view model deployments of the deployment Azure Open AI service. For Dev plus environments, developers will be able to access SND only via the DEFRA VPN or DEFRA laptop with the assigned role of Cognitive Services OpenAI User . This will currently allow them to: View the resource in Azure Portal View the resource endpoint under \u201cKeys and Endpoint\u201d but not the keys. View the resource and associated model deployments in Azure OpenAI Studio View available models for deployment in Azure OpenAI Studio Use the Chat, Completions, and DALL-E (preview) playground experiences with any models that have already been deployed to this Azure OpenAI resource. Note In Dev plus environments, APIM endpoints will not be exposed for local developer access. They will remain accessible only to ADP Azure services or authenticated Delivery project's services. Monitoring \u00b6 APIM is used to enhance the monitoring of OpenAI usage per service. You can utilise an OpenAI Emit Token Metric policy to send metrics to Application Insights about consumption of large language model tokens through Azure OpenAI Service APIs. Token count metrics include: Total Tokens, Prompt Tokens, and Completion Tokens with additional dimensions such as the User ID and API ID. By leveraging these metrics, ADP can monitor usage per service. This information can be utilized by both the project delivery teams and ADP to infer usage, potentially request additional quota if required, and for billing purposes. To monitor Azure Open AI directly we enable the Azure OpenAI Service Diagnostic Logs . This allows us to monitor the usage of the AI services directly. The log data is stored in Azure Monitor (Log Analytics Workspace), where both ADP and the delivery projects can gain insights such as: Azure OpenAI Requests: Total number of calls made to the Azure OpenAI API over a period of time. Generated Completion Tokens: Number of generated tokens (output) from an Azure OpenAI model. Processed Inference Tokens: Number of inference tokens processed by an Azure OpenAI model. Calculated as prompt tokens (input) + generated tokens. Quota & Token Management \u00b6 The Azure Open AI services are shared between delivery projects and have a quota limit per subscription/region. The quota limit is shared between the delivery projects and can be increased if required. The quota limit will be monitored by the ADP team and will be increased if required with the approval from Microsoft. Given the current needs and requirements of the ADP platform, we have opted for the pay-as-you-go pricing model for the Azure Open AI services. This allows the delivery projects to pay only for what they use, eliminating concerns about overcharging. However, this does mean that there is a Tokens-per-Minute limit per model for the Azure Open AI services. Delivery projects need to be aware of this limit when using the AI services. To manage this and ensure efficient use of Azure Open AI, APIM will provide policy enforcement to manage the quota limit and offer a better experience to the delivery projects when the quota limit is reached: Retry policy : When the quota limit is reached, the Azure OpenAI will return a 429 status code to the delivery project's service. APIM will implement a retry policy to wait for a certain amount of time before retrying the request to the Azure Open AI service. This will allow the delivery project's service to wait for the quota limit to be reset and then retry the request to the Azure Open AI service. Token limit policy : By relying on token usage metrics returned from the OpenAI endpoint, the policy can accurately monitor and enforce limits in real time. The policy also enables precalculation of prompt tokens by API Management, minimizing unnecessary requests to the OpenAI backend if the limit is already exceeded. Possible Future Enhancements \u00b6 Semantic caching for Azure OpenAI APIs in APIM \u00b6 Enable semantic caching of responses to Azure OpenAI API requests to reduce bandwidth and processing requirements imposed on the backend APIs and lower latency perceived by API consumers. With semantic caching, you can return cached responses for identical prompts and also for prompts that are similar in meaning, even if the text isn't the same. More Azure OpenAI Service Model Deployments \u00b6 Currently, the range of models that can be deployed is limited to those supported by Azure in the UK South region . In the future, we will look to provide a broader selection the models for delivery projects to utilise. This includes potential support for additional models such as Whisper, DALL-E, and GPT-4o models. Reserved Capacity for Azure OpenAI Service \u00b6 Reserved capacity, also known as Provisioned Throughput Units (PTU), is a feature of Azure OpenAI. The newer offering, PTU-M (Managed), abstracts away the backend compute, pooling resources. Beyond the default TPMs described above, this Azure OpenAI service feature, PTUs, defines the model processing capacity. It uses reserved resources for processing prompts and generating completions. PTUs are purchased as a monthly commitment with an auto-renewal option, which will reserve Azure OpenAI capacity within an Azure subscription, using a specific model, in a specific Azure region. TPM and PTU can be used together to provide scaling within a single region. Please note, PTU minimums are very expensive . This requires ADP to be at a certain scale to justify the cost across its Delivery Projects. Key Resources \u00b6 Azure Open AI Service GitHub: Microsoft - AzureOpenAI-with-APIM GitHub: aoai-apim Medium: Azure OpenAI Best Practice Managed Identity with APIM Role-based access control for Azure OpenAI Service MS Learn: Import Azure Open Open AI Service into APIM Outstanding Questions \u00b6 ??? Azure AI Search \u00b6 Azure AI Search (formerly known as \"Azure Cognitive Search\") provides secure information retrieval at scale over user-owned content in traditional and generative AI search applications. Information retrieval is a foundational aspect of any application that surfaces text and vectors. Common use cases include catalog or document search, data exploration, and increasingly, chat-style applications over proprietary grounding data. When creating a search service, you\u2019ll work with the following capabilities: A search engine for vector search and full text and hybrid search over a search index Rich indexing with integrated data chunking and vectorization (preview), lexical analysis for text, and optional applied AI for content extraction and transformation Rich query syntax for vector queries, text search, hybrid queries, fuzzy search, autocomplete, geo-search and others Azure scale, security, and reach Azure integration at the data layer, machine learning layer, Azure AI services and Azure OpenAI ADP provides a managed Azure AI Search service for delivery projects to use which is scalable and secure using best practice. The core components (indexes, datastores, etc) of the Azure AI Search will be dependently deployable and can be created by delivery projects as required on a self-service basis. Supported Features \u00b6 Warning Features that require external services will be limited to what has already been previously request by Delivery Projects. This normally effects supported data sources and skillsets. If a Delivery Project requires a new data source or skillset, they will need to request this from ADP and it will be reviewed and approved if it is required and does not affect the current delivery projects Supported features of Azure AI Search: Indexes : An index is a persistent store of documents that are searchable using Azure AI Search. An index is similar to a database table, but it contains a schema that describes the structure of the documents within the index. An index can contain one or more fields, each of which has a name and a data type. An index can also contain a scoring profile, which is used to rank search results based on relevance. An index can be created, updated, and deleted using the Azure AI Search REST API or the Azure Portal. Indexers : Indexers are used to extract content from an external data source and populate an index with it. Indexers can be scheduled to run at regular intervals to keep the index up-to-date. Data Sources : A data source is a connection to an external data store that contains the content you want to index. Data sources can be used to connect to a variety of data stores, including Azure Blob Storage, Azure Cosmos DB, Azure SQL Database, and more. Azure Blob Storage: Azure Blob Storage is a cloud-based storage service that allows you to store large amounts of unstructured data, such as text files, images, and videos. Azure Blob Storage can be used as a data source for Azure AI Search to index content from text files, images, and videos. Skillsets : A skillset is a collection of cognitive skills that can be used to enrich content during indexing. A skillset can contain one or more skills, each of which performs a specific task, such as extracting text from images or translating text to another language. A skillset can be used to extract information from unstructured data, such as images, videos, and documents, and make it searchable using Azure AI Search. Supported Skillsets include: Azure Open AI Embedding Skillset: This skillset uses the Azure Open AI Embedding model to convert text into numerical vector form to facilitate text similarity. This skillset can be used to enhance the search experience by providing more relevant search results based on the semantic meaning of the text. Architecture \u00b6 ADP has selected a Standard SKU for the Azure AI Search service as it provides a cost effective balance of storage and query capacity for the delivery projects. Azure AI Search is a shared service between the ADP Delivery Projects. Allowing up to 50 indexes and 50 indexers in total and 35 GB of storage per partition and 160 GB of storage with two replicas, requiring two search unit per environment. This will allow 99.9% availability for read operations. Note The ADP Team can increase the tier and the number of search units (replicas and partitions) as required. Under the current scope of the delivery projects, the standard SKU with two search units is sufficient, allowing for 99.9% availability for read operations. If a project requires 99.9% availability for read/write operations, additional search units can be added. Azure AI Search will not be reachable from the public internet and will only be accessible via a private link to DEFRA VPN, DEFRA laptops, or consuming Azure/ Delivery Project services via a private endpoint. Delivery Project services will be given the role of Search Index Data Contributor scoped to the indexes that the service requires . This will allow the Read-write access to the content of these indexes. Azure AI Search will need access to Azure Open AI embedding models to allow for semantic search in the search indexes and for use in its skill sets. No direct access to the Azure open AI services will be allowed and will only be accessible via the Azure API Management endpoint. To ensure that Azure AI Search has efficient access, the role of Cognitive Services OpenAI User will be assigned to the Azure AI Search service system assigned managed identity. This will allow the Azure AI Search service to access the Azure Open AI services via the APIM endpoint over a private link securely and efficiently. Developer Access \u00b6 For SND environments, developers will be able to access the Azure AI Search service via the Azure Portal with the assigned role of Reader across the whole of the AI Search service, as well as the role of Search Index Data Contributor scoped to the indexes that are created for their Delivery Project . This will allow the Read-write access to content of these indexes and also import, refresh, or query the documents collection of an index. It facilitates local development and testing of the Azure AI Search service. For Dev plus environments, developers will be able to access the Azure AI Search service via the Azure Portal with the assigned role of Reader . This role currently enables them to read across the entire service, including search metrics, content metrics (storage consumed, number of objects), and the object definitions of data plane resources (indexes, indexers, and so on). However, they won\u2019t have access to read API keys or content within indexes, thereby ensuring the security of the data control plane. Developers in all environments will be able to access Azure AI Search only via DEFRA VPN or DEFRA laptop with restrictions that are detailed above. Monitoring \u00b6 Note Azure AI Search doesn't monitor individual user access to content on the search service. If you require this level of monitoring, you need to implement it in your client application. The diagnostic logs for Azure AI Search are stored in Azure Monitor (Log Analytics Workspace). This allows ADP and the delivery projects to gain insights into aspects such as latency, errors, and usage of the search service. Deployment of Azure AI Search \u00b6 The Azure AI Search service will be deployed as a common service for use by all Delivery Projects. For self-service creation and updating of the Azure AI Search Components, developers will be able to use ADP PowerShell scripts and a JSON definition file. These components will be created within the X repository, ensuring that the components are consistently created across all projects and environments using Azure Pipelines. Token replacement \u00b6 ???? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"name\" : \"MyDataSource\" , \"description\" : \"Example of a data source that uses a connection string with a token\" , \"type\" : \"sharepoint\" , \"subtype\" : null , \"credentials\" : { \"connectionString\" : \"#{MyDataSourceConnectionString}#\" }, \"container\" : { \"name\" : \"useQuery\" , \"query\" : \"includeLibrary=....\" }, \"dataChangeDetectionPolicy\" : null , \"dataDeletionDetectionPolicy\" : null , \"encryptionKey\" : null , \"identity\" : null } Possible Future Enhancements \u00b6 TBC Key Resources \u00b6 Secure your Azure Cognitive Search indexes and queries with Azure AD Create a private endpoint for a secure connection to Azure AI Search Azure AI Search with skill sets Outstanding Questions \u00b6 Where is the best place for the deployment scripts for Azure AI Search components? Very unsure of how is best to make this self-service for the delivery projects. What data source to support? What skillsets to support? Custom skillsets?","title":"AI Services"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#azure-ai-services","text":"This article details the AI Services Architecture for the solution at a high level. AI Services supported by ADP: Azure Open AI Service Azure AI Search Warning Please ensure you fellow DEFRA's guidelines and policies when using AI services. This includes the use of data and the use of AI services in general in order to ensure your delivery project is using AI responsibly.","title":"Azure AI Services"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#azure-open-ai","text":"Azure OpenAI Service provides REST API access to OpenAI's powerful language models including the GPT-4, GPT-4 Turbo with Vision, GPT-3.5-Turbo, and Embeddings model series. In addition, the new GPT-4 and GPT-3.5-Turbo model series have now reached general availability. These models can be easily adapted to your specific task including but not limited to content generation, summarization, image understanding, semantic search, and natural language to code translation.","title":"Azure Open AI"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#deployed-supported-models","text":"The Azure Development Platform (ADP) supports a range of models within Azure OpenAI. However, the availability of these models is limited to those supported by Azure in the UK South region . The following table lists the supported models: Model Deployment Quota Description gpt-4 gpt-4 80k An improvement on GPT-3.5, capable of understanding and generating both natural language and code. gpt-35-turbo gpt-35-turbo 350k An improvement on GPT-3, capable of understanding and generating both natural language and code. text-embedding-ada-002 text-embedding-ada-002 350k Converts text into numerical vector form to facilitate text similarity comparisons. text-embedding-3-large text-embedding-3-large 350k The latest and most capable model for converting text into numerical vector form for text similarity comparisons. Please note that upgrading between different embedding models is not supported. Warning All Delivery Projects should be mindful of the quota restrictions for each model per subscription/region. These models are shared resources among all Delivery Projects. If more quota is needed, ADP can make a request. However, please note that any increase in quota is subject to Microsoft\u2019s approval.","title":"Deployed &amp; Supported Models"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#architecture","text":"Within the ADP Platform, Azure Open AI Services are deployed with an Azure API Management (APIM) to provide a secure and scalable API endpoint for the AI services. The APIM is used to manage the API lifecycle, provide security, and monitor the API usage. The APIM is deployed in a Subnet (/29) and uses a private link to connect to the Azure OpenAI service. Additionally, a developer portal is deployed with the APIM, offering self-service API documentation for the AI services. Between the delivery projects service and the APIM, private link will be implemented and the APIM will use the services' managed identity with the role of Cognitive Services OpenAI User assigned. This will allow the APIM to access the AI services on behalf of the delivery project's service privately and securely. For any other Azure services that require access to the AI services, they will need to utilize the APIM endpoint and the managed identity.","title":"Architecture"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#iterative-deployment","text":"To meet the timelines and requirements of the delivery projects, our initial step will be to deploy the Azure OpenAI Service. We will provide the AKS cluster and Azure AI Search with direct access to this service over a private endpoint, assigning the role of Cognitive Services OpenAI User . This approach will enable the delivery projects to begin utilizing the AI services and provide valuable feedback. Once the APIM is deployed, we will transition the AI services to APIM. This will establish a secure and scalable API endpoint for the AI services. Note For local development purposes, the delivery projects can directly use the Azure Open AI services in the SND environment only, provided they are connected to the DEFRA VPN or using a DEFRA laptop. This setup will facilitate testing of the AI services and the provision of feedback.","title":"Iterative Deployment"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#developer-access","text":"For local development, developers will be able to access SND only via the DEFRA VPN or DEFRA laptop with the assigned role of Cognitive Services OpenAI User . Giving the developers the ability to test Azure Open AI services locally via APIM and view model deployments of the deployment Azure Open AI service. For Dev plus environments, developers will be able to access SND only via the DEFRA VPN or DEFRA laptop with the assigned role of Cognitive Services OpenAI User . This will currently allow them to: View the resource in Azure Portal View the resource endpoint under \u201cKeys and Endpoint\u201d but not the keys. View the resource and associated model deployments in Azure OpenAI Studio View available models for deployment in Azure OpenAI Studio Use the Chat, Completions, and DALL-E (preview) playground experiences with any models that have already been deployed to this Azure OpenAI resource. Note In Dev plus environments, APIM endpoints will not be exposed for local developer access. They will remain accessible only to ADP Azure services or authenticated Delivery project's services.","title":"Developer Access"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#monitoring","text":"APIM is used to enhance the monitoring of OpenAI usage per service. You can utilise an OpenAI Emit Token Metric policy to send metrics to Application Insights about consumption of large language model tokens through Azure OpenAI Service APIs. Token count metrics include: Total Tokens, Prompt Tokens, and Completion Tokens with additional dimensions such as the User ID and API ID. By leveraging these metrics, ADP can monitor usage per service. This information can be utilized by both the project delivery teams and ADP to infer usage, potentially request additional quota if required, and for billing purposes. To monitor Azure Open AI directly we enable the Azure OpenAI Service Diagnostic Logs . This allows us to monitor the usage of the AI services directly. The log data is stored in Azure Monitor (Log Analytics Workspace), where both ADP and the delivery projects can gain insights such as: Azure OpenAI Requests: Total number of calls made to the Azure OpenAI API over a period of time. Generated Completion Tokens: Number of generated tokens (output) from an Azure OpenAI model. Processed Inference Tokens: Number of inference tokens processed by an Azure OpenAI model. Calculated as prompt tokens (input) + generated tokens.","title":"Monitoring"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#quota-token-management","text":"The Azure Open AI services are shared between delivery projects and have a quota limit per subscription/region. The quota limit is shared between the delivery projects and can be increased if required. The quota limit will be monitored by the ADP team and will be increased if required with the approval from Microsoft. Given the current needs and requirements of the ADP platform, we have opted for the pay-as-you-go pricing model for the Azure Open AI services. This allows the delivery projects to pay only for what they use, eliminating concerns about overcharging. However, this does mean that there is a Tokens-per-Minute limit per model for the Azure Open AI services. Delivery projects need to be aware of this limit when using the AI services. To manage this and ensure efficient use of Azure Open AI, APIM will provide policy enforcement to manage the quota limit and offer a better experience to the delivery projects when the quota limit is reached: Retry policy : When the quota limit is reached, the Azure OpenAI will return a 429 status code to the delivery project's service. APIM will implement a retry policy to wait for a certain amount of time before retrying the request to the Azure Open AI service. This will allow the delivery project's service to wait for the quota limit to be reset and then retry the request to the Azure Open AI service. Token limit policy : By relying on token usage metrics returned from the OpenAI endpoint, the policy can accurately monitor and enforce limits in real time. The policy also enables precalculation of prompt tokens by API Management, minimizing unnecessary requests to the OpenAI backend if the limit is already exceeded.","title":"Quota &amp; Token Management"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#possible-future-enhancements","text":"","title":"Possible Future Enhancements"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#semantic-caching-for-azure-openai-apis-in-apim","text":"Enable semantic caching of responses to Azure OpenAI API requests to reduce bandwidth and processing requirements imposed on the backend APIs and lower latency perceived by API consumers. With semantic caching, you can return cached responses for identical prompts and also for prompts that are similar in meaning, even if the text isn't the same.","title":"Semantic caching for Azure OpenAI APIs in APIM"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#more-azure-openai-service-model-deployments","text":"Currently, the range of models that can be deployed is limited to those supported by Azure in the UK South region . In the future, we will look to provide a broader selection the models for delivery projects to utilise. This includes potential support for additional models such as Whisper, DALL-E, and GPT-4o models.","title":"More Azure OpenAI Service Model Deployments"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#reserved-capacity-for-azure-openai-service","text":"Reserved capacity, also known as Provisioned Throughput Units (PTU), is a feature of Azure OpenAI. The newer offering, PTU-M (Managed), abstracts away the backend compute, pooling resources. Beyond the default TPMs described above, this Azure OpenAI service feature, PTUs, defines the model processing capacity. It uses reserved resources for processing prompts and generating completions. PTUs are purchased as a monthly commitment with an auto-renewal option, which will reserve Azure OpenAI capacity within an Azure subscription, using a specific model, in a specific Azure region. TPM and PTU can be used together to provide scaling within a single region. Please note, PTU minimums are very expensive . This requires ADP to be at a certain scale to justify the cost across its Delivery Projects.","title":"Reserved Capacity for Azure OpenAI Service"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#key-resources","text":"Azure Open AI Service GitHub: Microsoft - AzureOpenAI-with-APIM GitHub: aoai-apim Medium: Azure OpenAI Best Practice Managed Identity with APIM Role-based access control for Azure OpenAI Service MS Learn: Import Azure Open Open AI Service into APIM","title":"Key Resources"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#outstanding-questions","text":"???","title":"Outstanding Questions"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#azure-ai-search","text":"Azure AI Search (formerly known as \"Azure Cognitive Search\") provides secure information retrieval at scale over user-owned content in traditional and generative AI search applications. Information retrieval is a foundational aspect of any application that surfaces text and vectors. Common use cases include catalog or document search, data exploration, and increasingly, chat-style applications over proprietary grounding data. When creating a search service, you\u2019ll work with the following capabilities: A search engine for vector search and full text and hybrid search over a search index Rich indexing with integrated data chunking and vectorization (preview), lexical analysis for text, and optional applied AI for content extraction and transformation Rich query syntax for vector queries, text search, hybrid queries, fuzzy search, autocomplete, geo-search and others Azure scale, security, and reach Azure integration at the data layer, machine learning layer, Azure AI services and Azure OpenAI ADP provides a managed Azure AI Search service for delivery projects to use which is scalable and secure using best practice. The core components (indexes, datastores, etc) of the Azure AI Search will be dependently deployable and can be created by delivery projects as required on a self-service basis.","title":"Azure AI Search"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#supported-features","text":"Warning Features that require external services will be limited to what has already been previously request by Delivery Projects. This normally effects supported data sources and skillsets. If a Delivery Project requires a new data source or skillset, they will need to request this from ADP and it will be reviewed and approved if it is required and does not affect the current delivery projects Supported features of Azure AI Search: Indexes : An index is a persistent store of documents that are searchable using Azure AI Search. An index is similar to a database table, but it contains a schema that describes the structure of the documents within the index. An index can contain one or more fields, each of which has a name and a data type. An index can also contain a scoring profile, which is used to rank search results based on relevance. An index can be created, updated, and deleted using the Azure AI Search REST API or the Azure Portal. Indexers : Indexers are used to extract content from an external data source and populate an index with it. Indexers can be scheduled to run at regular intervals to keep the index up-to-date. Data Sources : A data source is a connection to an external data store that contains the content you want to index. Data sources can be used to connect to a variety of data stores, including Azure Blob Storage, Azure Cosmos DB, Azure SQL Database, and more. Azure Blob Storage: Azure Blob Storage is a cloud-based storage service that allows you to store large amounts of unstructured data, such as text files, images, and videos. Azure Blob Storage can be used as a data source for Azure AI Search to index content from text files, images, and videos. Skillsets : A skillset is a collection of cognitive skills that can be used to enrich content during indexing. A skillset can contain one or more skills, each of which performs a specific task, such as extracting text from images or translating text to another language. A skillset can be used to extract information from unstructured data, such as images, videos, and documents, and make it searchable using Azure AI Search. Supported Skillsets include: Azure Open AI Embedding Skillset: This skillset uses the Azure Open AI Embedding model to convert text into numerical vector form to facilitate text similarity. This skillset can be used to enhance the search experience by providing more relevant search results based on the semantic meaning of the text.","title":"Supported Features"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#architecture_1","text":"ADP has selected a Standard SKU for the Azure AI Search service as it provides a cost effective balance of storage and query capacity for the delivery projects. Azure AI Search is a shared service between the ADP Delivery Projects. Allowing up to 50 indexes and 50 indexers in total and 35 GB of storage per partition and 160 GB of storage with two replicas, requiring two search unit per environment. This will allow 99.9% availability for read operations. Note The ADP Team can increase the tier and the number of search units (replicas and partitions) as required. Under the current scope of the delivery projects, the standard SKU with two search units is sufficient, allowing for 99.9% availability for read operations. If a project requires 99.9% availability for read/write operations, additional search units can be added. Azure AI Search will not be reachable from the public internet and will only be accessible via a private link to DEFRA VPN, DEFRA laptops, or consuming Azure/ Delivery Project services via a private endpoint. Delivery Project services will be given the role of Search Index Data Contributor scoped to the indexes that the service requires . This will allow the Read-write access to the content of these indexes. Azure AI Search will need access to Azure Open AI embedding models to allow for semantic search in the search indexes and for use in its skill sets. No direct access to the Azure open AI services will be allowed and will only be accessible via the Azure API Management endpoint. To ensure that Azure AI Search has efficient access, the role of Cognitive Services OpenAI User will be assigned to the Azure AI Search service system assigned managed identity. This will allow the Azure AI Search service to access the Azure Open AI services via the APIM endpoint over a private link securely and efficiently.","title":"Architecture"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#developer-access_1","text":"For SND environments, developers will be able to access the Azure AI Search service via the Azure Portal with the assigned role of Reader across the whole of the AI Search service, as well as the role of Search Index Data Contributor scoped to the indexes that are created for their Delivery Project . This will allow the Read-write access to content of these indexes and also import, refresh, or query the documents collection of an index. It facilitates local development and testing of the Azure AI Search service. For Dev plus environments, developers will be able to access the Azure AI Search service via the Azure Portal with the assigned role of Reader . This role currently enables them to read across the entire service, including search metrics, content metrics (storage consumed, number of objects), and the object definitions of data plane resources (indexes, indexers, and so on). However, they won\u2019t have access to read API keys or content within indexes, thereby ensuring the security of the data control plane. Developers in all environments will be able to access Azure AI Search only via DEFRA VPN or DEFRA laptop with restrictions that are detailed above.","title":"Developer Access"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#monitoring_1","text":"Note Azure AI Search doesn't monitor individual user access to content on the search service. If you require this level of monitoring, you need to implement it in your client application. The diagnostic logs for Azure AI Search are stored in Azure Monitor (Log Analytics Workspace). This allows ADP and the delivery projects to gain insights into aspects such as latency, errors, and usage of the search service.","title":"Monitoring"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#deployment-of-azure-ai-search","text":"The Azure AI Search service will be deployed as a common service for use by all Delivery Projects. For self-service creation and updating of the Azure AI Search Components, developers will be able to use ADP PowerShell scripts and a JSON definition file. These components will be created within the X repository, ensuring that the components are consistently created across all projects and environments using Azure Pipelines.","title":"Deployment of Azure AI Search"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#token-replacement","text":"???? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"name\" : \"MyDataSource\" , \"description\" : \"Example of a data source that uses a connection string with a token\" , \"type\" : \"sharepoint\" , \"subtype\" : null , \"credentials\" : { \"connectionString\" : \"#{MyDataSourceConnectionString}#\" }, \"container\" : { \"name\" : \"useQuery\" , \"query\" : \"includeLibrary=....\" }, \"dataChangeDetectionPolicy\" : null , \"dataDeletionDetectionPolicy\" : null , \"encryptionKey\" : null , \"identity\" : null }","title":"Token replacement"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#possible-future-enhancements_1","text":"TBC","title":"Possible Future Enhancements"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#key-resources_1","text":"Secure your Azure Cognitive Search indexes and queries with Azure AD Create a private endpoint for a secure connection to Azure AI Search Azure AI Search with skill sets","title":"Key Resources"},{"location":"Platform-Architecture/platform-azure-services/ai-services/#outstanding-questions_1","text":"Where is the best place for the deployment scripts for Azure AI Search components? Very unsure of how is best to make this self-service for the delivery projects. What data source to support? What skillsets to support? Custom skillsets?","title":"Outstanding Questions"},{"location":"Platform-Architecture/platform-azure-services/data-services/","text":"TODO This page is a work in progress and will be updated in due course. Data Services \u00b6 This article details the Data Services Architecture for the solution at a high level. Data Services supported by ADP: Data Services Azure PostgreSQL Flexible Server Azure SQL Database Azure Cache for Redis Azure Cosmos DB Azure PostgreSQL Flexible Server \u00b6 Azure SQL Database \u00b6 Azure Cache for Redis \u00b6 Azure Cosmos DB \u00b6","title":"Data Services"},{"location":"Platform-Architecture/platform-azure-services/data-services/#data-services","text":"This article details the Data Services Architecture for the solution at a high level. Data Services supported by ADP: Data Services Azure PostgreSQL Flexible Server Azure SQL Database Azure Cache for Redis Azure Cosmos DB","title":"Data Services"},{"location":"Platform-Architecture/platform-azure-services/data-services/#azure-postgresql-flexible-server","text":"","title":"Azure PostgreSQL Flexible Server"},{"location":"Platform-Architecture/platform-azure-services/data-services/#azure-sql-database","text":"","title":"Azure SQL Database"},{"location":"Platform-Architecture/platform-azure-services/data-services/#azure-cache-for-redis","text":"","title":"Azure Cache for Redis"},{"location":"Platform-Architecture/platform-azure-services/data-services/#azure-cosmos-db","text":"","title":"Azure Cosmos DB"},{"location":"Platform-Architecture/platform-azure-services/integration-services/","text":"TODO This page is a work in progress and will be updated in due course. Data Services \u00b6 This article details the Integration Services Architecture for the solution at a high level. Integration Services supported by ADP: Data Services Azure Service Bus Azure API Management Azure Service Bus \u00b6 Azure API Management \u00b6","title":"Integration Services"},{"location":"Platform-Architecture/platform-azure-services/integration-services/#data-services","text":"This article details the Integration Services Architecture for the solution at a high level. Integration Services supported by ADP: Data Services Azure Service Bus Azure API Management","title":"Data Services"},{"location":"Platform-Architecture/platform-azure-services/integration-services/#azure-service-bus","text":"","title":"Azure Service Bus"},{"location":"Platform-Architecture/platform-azure-services/integration-services/#azure-api-management","text":"","title":"Azure API Management"},{"location":"Platform-Strategy/adp-platform-strategy/","text":"Azure Developer Platform Strategy \u00b6 Overall platform strategy for ADP. TODO This page is a work in progress and is being updated in due course. Vision Statement: Build Applications; not infrastructure. \u00b6 Mission Statement: To empower every product development team in Defra to increase the speed of delivery by providing a self-service and standardized Development Platform to host their business applications securely and efficiently. \u00b6 Our Values: \u00b6 Innovation and evergreen delivery Standardization and compliance Full automation with 'Everything-As-Code' Security, scalability and efficiency Self-service & developer centricity Fully open and transparency Observability and monitorability Our Motto: \u00b6 Move fast, with stable infrastructure and standardized delivery Why? By providing a stable infrastructure environment with common, tried and trusted delivery processes, we'll enable application teams to move faster and realize business value quicker. Our reach is Defra Azure wide, and will be able to be used by any product development team across the Digital delivery programme. We will focus on hosting and running digital transactional business applications. Enabling Delivery - What are our Goals and Objectives? \u00b6 To achieve our product vision of \"Build Apps - Not Infra \" , the following ( initial ) goals and objectives will need to be achieved as part of the Platform delivery. Our product goals \u00b6 \u201cProduct Teams are onboarded on day 1, in less than an hour, onto a Defra infrastructure/delivery Platform with the permissions configured based on their personas, so they can perform their role immediately.\u201d \u201cProduct Teams will be able to self-service their needs by being able to immediately build, test, deploy, and run software in a standardised way that expedites delivery and follows/promotes Defra\u2019s development standards.\u201d \"Engineers will have full observability and monitoring across the estate to be able to run and maintain their applications, securely and efficiently - without having to ask.\" \"The Platform will have pre-formed 'Exemplar Services', common code libraries, SDKs, etc. that are able to be used by, built upon, and expanded for by Product Teams that will demonstrate integration onto the Platform.\" \"The Platforms Build and Release processes will be on-demand, with all expected features and processes included out of the box for every development team.\" \"Everything will be in code, delivered fully by automation, tracked across its lifecycle, and versioned appropriately.\" \"The Platform will be continuously scalable, efficiently, to any number of product teams that are building transactional business services in Azure, without exponentially adding infrastructure and engineering resource costs.\" \"The Platform will provide the capability to view the entire estate in a single place. A business user or product team will be able to view a detailed catalogue of all services, documentation, libraries, etc. across all environments.\"","title":"ADP Platform Strategy"},{"location":"Platform-Strategy/adp-platform-strategy/#azure-developer-platform-strategy","text":"Overall platform strategy for ADP. TODO This page is a work in progress and is being updated in due course.","title":"Azure Developer Platform Strategy"},{"location":"Platform-Strategy/adp-platform-strategy/#vision-statement-build-applications-not-infrastructure","text":"","title":"Vision Statement: Build Applications; not infrastructure."},{"location":"Platform-Strategy/adp-platform-strategy/#mission-statement-to-empower-every-product-development-team-in-defra-to-increase-the-speed-of-delivery-by-providing-a-self-service-and-standardized-development-platform-to-host-their-business-applications-securely-and-efficiently","text":"","title":"Mission Statement:  To empower every product development team in Defra to increase the speed of delivery by providing a self-service and standardized Development Platform to host their business applications securely and efficiently."},{"location":"Platform-Strategy/adp-platform-strategy/#our-values","text":"Innovation and evergreen delivery Standardization and compliance Full automation with 'Everything-As-Code' Security, scalability and efficiency Self-service & developer centricity Fully open and transparency Observability and monitorability","title":"Our Values:"},{"location":"Platform-Strategy/adp-platform-strategy/#our-motto","text":"Move fast, with stable infrastructure and standardized delivery Why? By providing a stable infrastructure environment with common, tried and trusted delivery processes, we'll enable application teams to move faster and realize business value quicker. Our reach is Defra Azure wide, and will be able to be used by any product development team across the Digital delivery programme. We will focus on hosting and running digital transactional business applications.","title":"Our Motto:"},{"location":"Platform-Strategy/adp-platform-strategy/#enabling-delivery-what-are-our-goals-and-objectives","text":"To achieve our product vision of \"Build Apps - Not Infra \" , the following ( initial ) goals and objectives will need to be achieved as part of the Platform delivery.","title":"Enabling Delivery - What are our Goals and Objectives?"},{"location":"Platform-Strategy/adp-platform-strategy/#our-product-goals","text":"\u201cProduct Teams are onboarded on day 1, in less than an hour, onto a Defra infrastructure/delivery Platform with the permissions configured based on their personas, so they can perform their role immediately.\u201d \u201cProduct Teams will be able to self-service their needs by being able to immediately build, test, deploy, and run software in a standardised way that expedites delivery and follows/promotes Defra\u2019s development standards.\u201d \"Engineers will have full observability and monitoring across the estate to be able to run and maintain their applications, securely and efficiently - without having to ask.\" \"The Platform will have pre-formed 'Exemplar Services', common code libraries, SDKs, etc. that are able to be used by, built upon, and expanded for by Product Teams that will demonstrate integration onto the Platform.\" \"The Platforms Build and Release processes will be on-demand, with all expected features and processes included out of the box for every development team.\" \"Everything will be in code, delivered fully by automation, tracked across its lifecycle, and versioned appropriately.\" \"The Platform will be continuously scalable, efficiently, to any number of product teams that are building transactional business services in Azure, without exponentially adding infrastructure and engineering resource costs.\" \"The Platform will provide the capability to view the entire estate in a single place. A business user or product team will be able to view a detailed catalogue of all services, documentation, libraries, etc. across all environments.\"","title":"Our product goals"},{"location":"Platform-Strategy/documentation-approach/","text":"Documentation Approach \u00b6 For ADP there is going to be two data sources for documentation one that can be external (ADP Documentation) and another that will be internal (ADP Documentation Internal) both will be contributed to on GitHub via a GitHub Git repository. Our approach is to fellow GDS's Service standard of \"Make new source code open\" for our documentation making most of our documentation open to the public allowing it to be easily viewed by third parties. Making it available for reuse on an open licence while still keeping ownership of the intellectual property, enabling across government collaboration and ease of support for existing and future projects with Defra. For the minority of documentation which is classified as possible sensitive information will be available via ADP Internal Documentation. Diagram of our approach: Explanation: Tech User - normally a developer, Tech Lead, or Solution Architect will be able to access both internal and external documentation. Either from the ADP Portal or from a GitHub pages website. Tech users can view and extend the documentation going directly to a GitHub repository and submitting a pull request. Non-tech User - Project Manager, Business Analyst, etc will have access to internal and external documentation via the ADP Portal. For the external documentation they will still have access to this GitHub page but not the external documentation's GitHub page. As non-tech users will require a GitHub account that is in Defra's GitHub Organisation to access it. 3 rd party - Member of the public, possible Defra supplier, member of a non-Defra government department, etc will have access to External documentation via GitHub pages website. ADP Team - Will have access to all. Will be the main contributors and approvers of internal/ external documentation. ADP team will add additional documentation to the README.md of the ADP GitHub repositories. On commit to the main branch this will be copied over the internal/ external documentation repositories allowing ADP customers to reduce the amount of places they need to look for documentation increasing their productivity for learning and use when on the ADP. Automated deployments - On commit to main for both internal and external documentation repositories, deployment pipelines will be ran to deploy the documentation to GitHub pages and to ADP Portal's documentation store. ADP Documentation (External) Portal Link: https://portal.snd1.adp.defra.gov.uk/docs/default/component/adp-documentation GitHub Pages: https://defra.github.io/adp-documentation (public) GitHub Repository: https://github.com/DEFRA/adp-documentation What will be stored here: What is ADP? - Introduction to ADP, Pros, cons, limitations, Is your Defra project right for ADP? Getting Started - Guides to enables ADP customers (users, projects, programmes) to get started quickly on ADP. How to guides - Guides on how to do development functions when developing on ADP, focused on the tech user. Platform Strategy - details of the ADP Platforms Strategies (current and future). Migrate to ADP - Step by step instructions on how to migrate your existing Defra project & services over to ADP. Developer Reference - Detailed reference material required to enable tech users to work with ADP. ADP repositories README's will be copied here. Platform Architecture - Fairly high level details of ADP Architecture. Enough detail that another Platform team could not implement ADP else where. ADP **Documentation Internal** Portal Link: https://portal.snd1.adp.defra.gov.uk/docs/default/component/adp-documentation-internal GitHub Pages: https://defra.github.io/adp-documentation-internal/ GitHub Repository: https://github.com/DEFRA/adp-documentation-internal (private) What will be stored here: ADP Internal - Internal information that should not be accessible to the public. Internal Architecture Details - Architecture information that should not be accessible to the public. ADP Runbooks - step by step instructions that are required as part of the change management process in order to release to production.","title":"Documentation Approach"},{"location":"Platform-Strategy/documentation-approach/#documentation-approach","text":"For ADP there is going to be two data sources for documentation one that can be external (ADP Documentation) and another that will be internal (ADP Documentation Internal) both will be contributed to on GitHub via a GitHub Git repository. Our approach is to fellow GDS's Service standard of \"Make new source code open\" for our documentation making most of our documentation open to the public allowing it to be easily viewed by third parties. Making it available for reuse on an open licence while still keeping ownership of the intellectual property, enabling across government collaboration and ease of support for existing and future projects with Defra. For the minority of documentation which is classified as possible sensitive information will be available via ADP Internal Documentation. Diagram of our approach: Explanation: Tech User - normally a developer, Tech Lead, or Solution Architect will be able to access both internal and external documentation. Either from the ADP Portal or from a GitHub pages website. Tech users can view and extend the documentation going directly to a GitHub repository and submitting a pull request. Non-tech User - Project Manager, Business Analyst, etc will have access to internal and external documentation via the ADP Portal. For the external documentation they will still have access to this GitHub page but not the external documentation's GitHub page. As non-tech users will require a GitHub account that is in Defra's GitHub Organisation to access it. 3 rd party - Member of the public, possible Defra supplier, member of a non-Defra government department, etc will have access to External documentation via GitHub pages website. ADP Team - Will have access to all. Will be the main contributors and approvers of internal/ external documentation. ADP team will add additional documentation to the README.md of the ADP GitHub repositories. On commit to the main branch this will be copied over the internal/ external documentation repositories allowing ADP customers to reduce the amount of places they need to look for documentation increasing their productivity for learning and use when on the ADP. Automated deployments - On commit to main for both internal and external documentation repositories, deployment pipelines will be ran to deploy the documentation to GitHub pages and to ADP Portal's documentation store. ADP Documentation (External) Portal Link: https://portal.snd1.adp.defra.gov.uk/docs/default/component/adp-documentation GitHub Pages: https://defra.github.io/adp-documentation (public) GitHub Repository: https://github.com/DEFRA/adp-documentation What will be stored here: What is ADP? - Introduction to ADP, Pros, cons, limitations, Is your Defra project right for ADP? Getting Started - Guides to enables ADP customers (users, projects, programmes) to get started quickly on ADP. How to guides - Guides on how to do development functions when developing on ADP, focused on the tech user. Platform Strategy - details of the ADP Platforms Strategies (current and future). Migrate to ADP - Step by step instructions on how to migrate your existing Defra project & services over to ADP. Developer Reference - Detailed reference material required to enable tech users to work with ADP. ADP repositories README's will be copied here. Platform Architecture - Fairly high level details of ADP Architecture. Enough detail that another Platform team could not implement ADP else where. ADP **Documentation Internal** Portal Link: https://portal.snd1.adp.defra.gov.uk/docs/default/component/adp-documentation-internal GitHub Pages: https://defra.github.io/adp-documentation-internal/ GitHub Repository: https://github.com/DEFRA/adp-documentation-internal (private) What will be stored here: ADP Internal - Internal information that should not be accessible to the public. Internal Architecture Details - Architecture information that should not be accessible to the public. ADP Runbooks - step by step instructions that are required as part of the change management process in order to release to production.","title":"Documentation Approach"},{"location":"Platform-Strategy/service-deployment-strategy/","text":"Platform service Deployment Strategy \u00b6 Guidance and Context \u00b6 This article outlines the Platform service deployment strategies available. Development teams should read the Platform Versioning and Git strategy document before reading this. ADP\u2019s primary deployment strategy is Rolling Deployments on AKS with HELM and FluxCD . This provides Platform services with a zero-downtime deployment strategy. This allows applications to achieve high availability with low/no business impact to live service. This is important for services that need 24/7 availability and allows the capability to deploy to production multiple times a day. In the future, we will support other deployment strategies, such as Blue-Green and Canary deployments. Deployment Strategies - ADP Rolling Updates \u00b6 ADP uses AKS (Kubernetes) with HELM Charts and Flux to perform rolling deployments. The default strategy applied to all services is rolling deployments, unless otherwise specified in the deployment YAML. We recommend starting with this strategy. This strategy allows for applications to be incrementally updated without downtime. There are 3 core parts to a Service deployment/upgrade, which are done in the following order: App Configuration, including Secrets, Service Infrastructure, Database upgrade and Web Application The deployment process flow: A new deployment is triggered via the CI & CD Pipelines for the Service: New app Secrets are imported/updated/deleted* in the Key Vault and are mastered in the Azure DevOps (ADO) Secret Library Groups for the service. New App Configuration keys and values are imported/updated/deleted in the Service Config Maps & App Configuration Service from the Service\u2019s \u2018appConfig.yaml\u2019 files. Note: The sentinel key is not updated yet. The new images and artefact are pushed to the environment Container Registry (ACR) (via pipeline deployment) and Flux updates the Services repository with the new version to be deployed: This can be a higher version (new image & release) or lower version (existing/rollback). Flux reconciles the Cluster with the new Web App code and Infrastructure versions requested with a rolling update. Any infrastructure updates take precedence over Application (Infra > App). Application deployment: The deployment will incrementally add new Pods (web applications) onto the Nodes in the Cluster. This will automatically pick up the new App Config/Secret updates on startup. AKS deployment will wait for those new Pods (apps) to start successfully with the configured/default (5m) wait times and health check endpoints. Once the new pods are started and reporting healthy via the endpoint(s), traffic will then be directed to the new Pods (updated app) via the internal load balancer/NGINX gracefully. The old Pods (previous version) will be deleted incrementally if the new Pods have started successfully, and all traffic has drained gracefully. If the new App/Pod does not start successfully, the deployment will time out and fail after a set period of health check retries (5m), but the previous app version (Pods) will remain in place and accepting traffic. The previous version\u2019s App Config will remain as-is/unchanged on the none-upgraded Pods. 1. Unhealthy Pods will be removed if an upgrade fails. Infrastructure deployment: The new infrastructure will be deployed (created, updated, or deleted). This can be Queues, Topics, Datastores, Identities, etc. Once the infrastructure upgrade is successful, the App (and database If applicable) can be deployed/upgraded. Database deployment: If a new DB Schema is to be deployed (migration required), this will be done before the Web Application is deployed. Liquibase will perform the PostgreSQL migration using a Flux pre-deploy job. If database deployment/migration fails, the App will not be upgraded. If a user has requested the deployment of App Config/Secrets only via the Flag in the build.yaml , the App or Infra will not be deployed on this release: The App Config & Secrets will be updated via the Pipeline, including the Sentinel Key with the Build ID \u2013 which triggers the configuration update. The Reloader service will perform a rolling and zero-downtime upgrade (restart Pods) of the Service to consume the new App configuration (incremental Pod restarts). Note All releases / deployments are promoted via the Common CI and CD Pipelines using Azure DevOps as the orchestrator. We promote continuous delivery with automated checks and tests above/in preference to manual intervention and approvals. Approval gates can be added optionally to Azure Pipelines to gate the promotion of code.\u2003 Deployment and App Configuration Guidance / Context \u00b6 All services will have the following settings defaulted (changeable if required): maxSurage \u2013 maximum additional Pods created at one time (50%). maxUnavailable \u2013 max Pods not available (25%) podDisruptionBudget \u2013 allowed disruptions for a Pod (application) (25% or at least 1) min and max replicas \u2013 number of replicas of the application in the Cluster. Minimum of 3 for production for high availability. All deployments of business apps are on the User/Apps Node Pools. Platform/System apps are on the System Node Pool. Taints/tolerations applied to that effect. Autoscaling via HPA is enabled. All services will have their own dedicated AKS Namespaces for their own team. Constraints Infrastructure is always deployed first if changed; database Schema migrations are second and App code is last (associated Config & Secrets consumed at that point) Database updates, if using PostgreSQL, will require development teams to deploy non-breaking changes and/or manage their schema updates appropriately with their app deployment to prevent downtime. Shutter pages will be included in phase 2 / Post MVP if required. Development teams must set health endpoints correctly for an effective rolling update. App Config, Infrastructure and Application Code are tied (versioned) together as an immutable unit. They are versioned using semver strategy defined in the versioning article. App Secrets in Key Vault/ADO library group are not versioned with the App/Code or Infra, they are fully independent, and can be rotated periodically. All secret rotations must have an overlap in expiry periods to ensure zero-downtime upgrades. Secrets should not be tied to versions as they are rotatable as good practice. The Platform has defined minimum replicas/availability to meet Defra SLA\u2019s. The Platform Reloader Service will drain and replace the Pods in the Cluster with a rolling upgrade on detection of new App Config or New App Secrets automatically via the Sentinel Key update. All HELM Deployments are full CRUD operations \u2013 add, update, or delete. This includes Apps, Infra and Databases. Warning: You can delete your own infrastructure and configuration! All App Configuration updates are full CRUD operations \u2013 create, update, or delete. Secrets are add/update only for MVP. *Delete will be added post-MVP.","title":"Service Deployment Strategy"},{"location":"Platform-Strategy/service-deployment-strategy/#platform-service-deployment-strategy","text":"","title":"Platform service Deployment Strategy"},{"location":"Platform-Strategy/service-deployment-strategy/#guidance-and-context","text":"This article outlines the Platform service deployment strategies available. Development teams should read the Platform Versioning and Git strategy document before reading this. ADP\u2019s primary deployment strategy is Rolling Deployments on AKS with HELM and FluxCD . This provides Platform services with a zero-downtime deployment strategy. This allows applications to achieve high availability with low/no business impact to live service. This is important for services that need 24/7 availability and allows the capability to deploy to production multiple times a day. In the future, we will support other deployment strategies, such as Blue-Green and Canary deployments.","title":"Guidance and Context"},{"location":"Platform-Strategy/service-deployment-strategy/#deployment-strategies-adp-rolling-updates","text":"ADP uses AKS (Kubernetes) with HELM Charts and Flux to perform rolling deployments. The default strategy applied to all services is rolling deployments, unless otherwise specified in the deployment YAML. We recommend starting with this strategy. This strategy allows for applications to be incrementally updated without downtime. There are 3 core parts to a Service deployment/upgrade, which are done in the following order: App Configuration, including Secrets, Service Infrastructure, Database upgrade and Web Application The deployment process flow: A new deployment is triggered via the CI & CD Pipelines for the Service: New app Secrets are imported/updated/deleted* in the Key Vault and are mastered in the Azure DevOps (ADO) Secret Library Groups for the service. New App Configuration keys and values are imported/updated/deleted in the Service Config Maps & App Configuration Service from the Service\u2019s \u2018appConfig.yaml\u2019 files. Note: The sentinel key is not updated yet. The new images and artefact are pushed to the environment Container Registry (ACR) (via pipeline deployment) and Flux updates the Services repository with the new version to be deployed: This can be a higher version (new image & release) or lower version (existing/rollback). Flux reconciles the Cluster with the new Web App code and Infrastructure versions requested with a rolling update. Any infrastructure updates take precedence over Application (Infra > App). Application deployment: The deployment will incrementally add new Pods (web applications) onto the Nodes in the Cluster. This will automatically pick up the new App Config/Secret updates on startup. AKS deployment will wait for those new Pods (apps) to start successfully with the configured/default (5m) wait times and health check endpoints. Once the new pods are started and reporting healthy via the endpoint(s), traffic will then be directed to the new Pods (updated app) via the internal load balancer/NGINX gracefully. The old Pods (previous version) will be deleted incrementally if the new Pods have started successfully, and all traffic has drained gracefully. If the new App/Pod does not start successfully, the deployment will time out and fail after a set period of health check retries (5m), but the previous app version (Pods) will remain in place and accepting traffic. The previous version\u2019s App Config will remain as-is/unchanged on the none-upgraded Pods. 1. Unhealthy Pods will be removed if an upgrade fails. Infrastructure deployment: The new infrastructure will be deployed (created, updated, or deleted). This can be Queues, Topics, Datastores, Identities, etc. Once the infrastructure upgrade is successful, the App (and database If applicable) can be deployed/upgraded. Database deployment: If a new DB Schema is to be deployed (migration required), this will be done before the Web Application is deployed. Liquibase will perform the PostgreSQL migration using a Flux pre-deploy job. If database deployment/migration fails, the App will not be upgraded. If a user has requested the deployment of App Config/Secrets only via the Flag in the build.yaml , the App or Infra will not be deployed on this release: The App Config & Secrets will be updated via the Pipeline, including the Sentinel Key with the Build ID \u2013 which triggers the configuration update. The Reloader service will perform a rolling and zero-downtime upgrade (restart Pods) of the Service to consume the new App configuration (incremental Pod restarts). Note All releases / deployments are promoted via the Common CI and CD Pipelines using Azure DevOps as the orchestrator. We promote continuous delivery with automated checks and tests above/in preference to manual intervention and approvals. Approval gates can be added optionally to Azure Pipelines to gate the promotion of code.","title":"Deployment Strategies - ADP Rolling Updates"},{"location":"Platform-Strategy/service-deployment-strategy/#deployment-and-app-configuration-guidance-context","text":"All services will have the following settings defaulted (changeable if required): maxSurage \u2013 maximum additional Pods created at one time (50%). maxUnavailable \u2013 max Pods not available (25%) podDisruptionBudget \u2013 allowed disruptions for a Pod (application) (25% or at least 1) min and max replicas \u2013 number of replicas of the application in the Cluster. Minimum of 3 for production for high availability. All deployments of business apps are on the User/Apps Node Pools. Platform/System apps are on the System Node Pool. Taints/tolerations applied to that effect. Autoscaling via HPA is enabled. All services will have their own dedicated AKS Namespaces for their own team. Constraints Infrastructure is always deployed first if changed; database Schema migrations are second and App code is last (associated Config & Secrets consumed at that point) Database updates, if using PostgreSQL, will require development teams to deploy non-breaking changes and/or manage their schema updates appropriately with their app deployment to prevent downtime. Shutter pages will be included in phase 2 / Post MVP if required. Development teams must set health endpoints correctly for an effective rolling update. App Config, Infrastructure and Application Code are tied (versioned) together as an immutable unit. They are versioned using semver strategy defined in the versioning article. App Secrets in Key Vault/ADO library group are not versioned with the App/Code or Infra, they are fully independent, and can be rotated periodically. All secret rotations must have an overlap in expiry periods to ensure zero-downtime upgrades. Secrets should not be tied to versions as they are rotatable as good practice. The Platform has defined minimum replicas/availability to meet Defra SLA\u2019s. The Platform Reloader Service will drain and replace the Pods in the Cluster with a rolling upgrade on detection of new App Config or New App Secrets automatically via the Sentinel Key update. All HELM Deployments are full CRUD operations \u2013 add, update, or delete. This includes Apps, Infra and Databases. Warning: You can delete your own infrastructure and configuration! All App Configuration updates are full CRUD operations \u2013 create, update, or delete. Secrets are add/update only for MVP. *Delete will be added post-MVP.","title":"Deployment and App Configuration Guidance / Context"},{"location":"Platform-Strategy/service-versioning-strategy/","text":"Platform service versioning strategy. \u00b6 This article outlines a two-phase versioning strategy for services on ADP with the goal to support ephemeral environments by phase 2. The following Git and Versioning strategies are in place and mandated: A Sematic Versioning (SemVer) strategy for all Platform and business services (app code and infrastructure) The Trunk Based Development Git git strategy for application development (code and infrastructure). In Phase 1, before ephemeral environments, Feature branch builds fetch the version from the main branch\u2019s package.json file for Node and the .csproj file for C#. If the versions are the same, a validation error is thrown; if the feature branch version is higher, it's tagged with \u2018 -alpha *\u2019 and the pipeline build ID. When the main branch version is pushed to the ACR on deployment after merging into main, it will take precedence over all feature (alpha) candidates of the same major/minor/patch version. In Phase 2 with ephemeral environments, the process remains the same for Feature branches. For Pull Request (PR) builds, if the package.json/csproj is not updated, a validation error is thrown; if it is updated, the image/build is tagged with a release candidate (-RC) and the build ID. The main branch version takes precedence over all Feature (alpha & RC) candidates. With ephemeral environments, each feature deployment will deploy a unique pod (application & infrastructure). Phase 1 Strategy \u2013 versioning logic (before ephemeral environments)\u00b6 \u00b6 Feature branch build and deployments Retrieve the version from the Main branch package.json for the repository (e.g.: 4.2.30) if main and feature branches are the same version (M/M/P) then: throw validation error message: \"The increment is invalid. Users must increase the package.json version.\". Do not continue CI. if main and feature branch version are not same (i.e., a developer has increased Major, Minor or Patch) and Feature Branch > Main branch version, then: Tag the image and build with \u2018-alpha\u2019 and build ID which becomes: 4.2.31-alpha.511210 and respect the supplied major/minor/patch. Push this version to Container Registry (ACR) when a deploy is requested. Pull Request (PR) builds and deployments No change for Phase 1, including tagging and naming. Developers merge (feature branch) version must be always above main. Main branch build and deployments New version example is: 4.2.31 (patch+1). Tag release in GitHub. This version will be pushed to the ACR on deployment after merge into main. The main branch version is the primary version which takes precedence above all feature (alpha) candidates of the same major/minor/patch. Phase 2 versioning logic \u2013 (with ephemeral environments are in place) \u00b6 Feature branch builds and deployments Retrieve the version from Main branch package.json/csproj for the repository (e.g. 4.2.30) if main and feature branch are the same version (M/M/P) then: throw validation error message: \"The increment is invalid. Developers must increase the package.json version.\". Do not continue CI. if main and feature branch version are not same (i.e., a developer has increased Major, Minor or Patch) and Feature Branch > Main branch then: Tag the image and build with \u2018-alpha\u2019 and \u2018build ID\u2019 which becomes 4.2.31-alpha.511210 and respect users major/minor/patch. Push this version to ACR when a deploy is requested. Pull Request (PR) - builds and deployments If package.json/csproj is not updated in the repository then throw validation message: \"The increment is invalid '4.2.30' -> '4.2.30'. Please upgrade\". Do not continue CI. If package.json/csproj is updated (i.e., 4.2.31) then tag the image and build with the release candidate (-RC) and build ID which becomes: 4.2.31-rc.511211 Push this version to the Container Registry (ACR) when a deploy is requested. Main branch \u2013 build and deployments New version example is: 4.2. 31 (patch+1). Tag release in GitHub. This version will be pushed to the Container Registry (ACR) on deployment after merge into main. The main branch version is the primary version which takes precedence over and above all feature (alpha & RC) candidates of the same major/minor/patch. Guidance / Context \u00b6 The Build ID is unique and is the ADO Pipeline build ID. It automatically increases on every CI on every image you request to be deployed (feature deployment). Developers must increment Major, Minor or Patch at least once, on a Feature branch build or PR build, to merge into main successfully. The build ID is automatically increased for subsequent deployments of the same version. The Main version takes priority over Alpha and RC candidates of the same major/minor/patch version. Constraints \u00b6 Feature deployments into Sandpit/Dev will overwrite the existing deployment in terms of app code, infrastructure, and databases in Phase 1. This can cause conflicts and constraints. Once ephemeral environments are delivered, PR and Feature deployments into Sandpit/Dev will have its own dedicated infrastructure, including Application, Infra and Databases. SemVer and Trunk based development are mandated and designed into the Platform. All merges into \u2018main\u2019 are classed as releases and are tagged in GitHub as such with the application version supplied. Long-lived feature branches are not allowed and are discouraged. To deploy into a higher environment above Sandpit, you must merge into Main.","title":"Service Versioning Strategy"},{"location":"Platform-Strategy/service-versioning-strategy/#platform-service-versioning-strategy","text":"This article outlines a two-phase versioning strategy for services on ADP with the goal to support ephemeral environments by phase 2. The following Git and Versioning strategies are in place and mandated: A Sematic Versioning (SemVer) strategy for all Platform and business services (app code and infrastructure) The Trunk Based Development Git git strategy for application development (code and infrastructure). In Phase 1, before ephemeral environments, Feature branch builds fetch the version from the main branch\u2019s package.json file for Node and the .csproj file for C#. If the versions are the same, a validation error is thrown; if the feature branch version is higher, it's tagged with \u2018 -alpha *\u2019 and the pipeline build ID. When the main branch version is pushed to the ACR on deployment after merging into main, it will take precedence over all feature (alpha) candidates of the same major/minor/patch version. In Phase 2 with ephemeral environments, the process remains the same for Feature branches. For Pull Request (PR) builds, if the package.json/csproj is not updated, a validation error is thrown; if it is updated, the image/build is tagged with a release candidate (-RC) and the build ID. The main branch version takes precedence over all Feature (alpha & RC) candidates. With ephemeral environments, each feature deployment will deploy a unique pod (application & infrastructure).","title":"Platform service versioning strategy."},{"location":"Platform-Strategy/service-versioning-strategy/#phase-1-strategy-versioning-logic-before-ephemeral-environments","text":"Feature branch build and deployments Retrieve the version from the Main branch package.json for the repository (e.g.: 4.2.30) if main and feature branches are the same version (M/M/P) then: throw validation error message: \"The increment is invalid. Users must increase the package.json version.\". Do not continue CI. if main and feature branch version are not same (i.e., a developer has increased Major, Minor or Patch) and Feature Branch > Main branch version, then: Tag the image and build with \u2018-alpha\u2019 and build ID which becomes: 4.2.31-alpha.511210 and respect the supplied major/minor/patch. Push this version to Container Registry (ACR) when a deploy is requested. Pull Request (PR) builds and deployments No change for Phase 1, including tagging and naming. Developers merge (feature branch) version must be always above main. Main branch build and deployments New version example is: 4.2.31 (patch+1). Tag release in GitHub. This version will be pushed to the ACR on deployment after merge into main. The main branch version is the primary version which takes precedence above all feature (alpha) candidates of the same major/minor/patch.","title":"Phase 1 Strategy \u2013 versioning logic (before ephemeral environments)\u00b6"},{"location":"Platform-Strategy/service-versioning-strategy/#phase-2-versioning-logic-with-ephemeral-environments-are-in-place","text":"Feature branch builds and deployments Retrieve the version from Main branch package.json/csproj for the repository (e.g. 4.2.30) if main and feature branch are the same version (M/M/P) then: throw validation error message: \"The increment is invalid. Developers must increase the package.json version.\". Do not continue CI. if main and feature branch version are not same (i.e., a developer has increased Major, Minor or Patch) and Feature Branch > Main branch then: Tag the image and build with \u2018-alpha\u2019 and \u2018build ID\u2019 which becomes 4.2.31-alpha.511210 and respect users major/minor/patch. Push this version to ACR when a deploy is requested. Pull Request (PR) - builds and deployments If package.json/csproj is not updated in the repository then throw validation message: \"The increment is invalid '4.2.30' -> '4.2.30'. Please upgrade\". Do not continue CI. If package.json/csproj is updated (i.e., 4.2.31) then tag the image and build with the release candidate (-RC) and build ID which becomes: 4.2.31-rc.511211 Push this version to the Container Registry (ACR) when a deploy is requested. Main branch \u2013 build and deployments New version example is: 4.2. 31 (patch+1). Tag release in GitHub. This version will be pushed to the Container Registry (ACR) on deployment after merge into main. The main branch version is the primary version which takes precedence over and above all feature (alpha & RC) candidates of the same major/minor/patch.","title":"Phase 2 versioning logic \u2013 (with ephemeral environments are in place)"},{"location":"Platform-Strategy/service-versioning-strategy/#guidance-context","text":"The Build ID is unique and is the ADO Pipeline build ID. It automatically increases on every CI on every image you request to be deployed (feature deployment). Developers must increment Major, Minor or Patch at least once, on a Feature branch build or PR build, to merge into main successfully. The build ID is automatically increased for subsequent deployments of the same version. The Main version takes priority over Alpha and RC candidates of the same major/minor/patch version.","title":"Guidance / Context"},{"location":"Platform-Strategy/service-versioning-strategy/#constraints","text":"Feature deployments into Sandpit/Dev will overwrite the existing deployment in terms of app code, infrastructure, and databases in Phase 1. This can cause conflicts and constraints. Once ephemeral environments are delivered, PR and Feature deployments into Sandpit/Dev will have its own dedicated infrastructure, including Application, Infra and Databases. SemVer and Trunk based development are mandated and designed into the Platform. All merges into \u2018main\u2019 are classed as releases and are tagged in GitHub as such with the application version supplied. Long-lived feature branches are not allowed and are discouraged. To deploy into a higher environment above Sandpit, you must merge into Main.","title":"Constraints"}]}